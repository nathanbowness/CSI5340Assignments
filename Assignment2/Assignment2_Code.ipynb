{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load MNIST Dataset\n",
    "### MNIST Dataset was loaded from Tensorflow package from -- https://www.tensorflow.org/datasets/catalog/mnist\n",
    "Load the MNIST dataset is downloaded using the tensor flow package, with information in the link above. The link given in the assignment is now password protected, so I pulled it directly from the TensorFlow package instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "def get_mnist_dataset():\n",
    "    \"\"\"\n",
    "    Grabs the MNIST dataset from TensorFlow.Keras and returns it in a usable form.\n",
    "    :return: x_train, y_train, x_test, y_test\n",
    "    \"\"\"\n",
    "    mnist = tf.keras.datasets.mnist\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "    return x_train, y_train, x_test, y_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Soft-max Regression Classifier\n",
    "This section will define method to use the soft-max regression classifier to classify the MNIST dataset.\n",
    "The classifier will have optional parameters to use or not use dropout, and to use or not use batch normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxRegressionClassifier:\n",
    "  def __init__(self, x_train, y_train, dropout=False, batch_norm=False):\n",
    "    # Constant parameters for training\n",
    "    self.learning_rate = 0.01\n",
    "    self.batch_size = 100\n",
    "    self.training_iterations = 5\n",
    "    self.dropout_rate = 0.2\n",
    "    # Store training dataset\n",
    "    self.x_train = x_train\n",
    "    self.y_train = y_train\n",
    "    self.train_dataset = tf.data.Dataset.from_tensor_slices((self.x_train, self.y_train))\n",
    "    # Store training params\n",
    "    self.dropout = dropout\n",
    "    self.batch_norm = batch_norm\n",
    "    # Initialize weights, biases, and optimizer\n",
    "    num_images = len(self.x_train)\n",
    "    image_size = len(self.x_train[0][0])*len(self.x_train[0][1])\n",
    "    self.weights = tf.Variable(tf.random.normal([image_size, num_images]), name=\"weights\")\n",
    "    self.biases = tf.Variable(tf.random.normal([num_images]), name=\"biases\")\n",
    "\n",
    "  def calculate_raw_model_output(self, x_data):\n",
    "    \"\"\"\n",
    "    Calculates the raw output of the model, i.e. the logits or y_predicted. Also optionally applies batch normalization or dropout.\n",
    "    :return: raw_model_output (logits)\n",
    "    \"\"\"\n",
    "    # !!! Blows up here !!!\n",
    "    raw_model_output = tf.add(tf.matmul(x_data, self.weights) + self.biases)\n",
    "\n",
    "    # Optionally apply the batch normalization\n",
    "    if self.batch_norm:\n",
    "        raw_model_output = self.batch_norm(raw_model_output)\n",
    "    # Optionally apply dropout\n",
    "    if self.dropout:\n",
    "        raw_model_output = tf.nn.dropout(raw_model_output, rate=self.dropout_rate)\n",
    "    return raw_model_output\n",
    "\n",
    "  def train_model(self):\n",
    "    optimizer = tf.optimizers.Adam(learning_rate=self.learning_rate)\n",
    "    trainable_variables = [self.weights, self.biases]\n",
    "\n",
    "    # Iterate through the entire training dataset training_iterations times\n",
    "    for iterations in range(self.training_iterations):\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = self.calculate_raw_model_output(self.x_train)\n",
    "            # Calculate loss using softmax cross entropy, then reduce mean\n",
    "            loss = tf.nn.softmax_cross_entropy_with_logits(logits=raw_model_output, labels=trainable_variables)\n",
    "            loss = tf.reduce_mean(loss)\n",
    "\n",
    "        gradients = tape.gradient(loss, trainable_variables)\n",
    "        # Apply gradients\n",
    "        optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "\n",
    "    # Calculate and display training accuracy every iteration through the dataset\n",
    "    train_accuracy = self.calculate_accuracy(x_train, y_train)\n",
    "    print(f\"Epoch {epoch + 1}, Training Accuracy: {train_accuracy.numpy()}\")\n",
    "\n",
    "  def accuracy_external(self, x_test_data, y_test_data):\n",
    "    predictions = self.calculate_raw_model_output(x_test_data)\n",
    "    predictions = tf.nn.softmax(predictions)\n",
    "    correct_prediction = tf.equal(tf.argmax(predictions, 1), tf.argmax(y_test_data, 1))\n",
    "    return tf.reduce_mean(tf.cast(correct_prediction, tf.float32), axis=-1)\n",
    "\n",
    "  def accuracy_internal(self):\n",
    "    predictions = self.calculate_raw_model_output(self.x_train)\n",
    "    predictions = tf.nn.softmax(predictions)\n",
    "    correct_prediction = tf.equal(tf.argmax(predictions, 1), tf.argmax(self.y_train, 1))\n",
    "    return tf.reduce_mean(tf.cast(correct_prediction, tf.float32), axis=-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images: 60000\n",
      "Image size: 784\n",
      "test\n"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "Could not find device for node: {{node BatchMatMulV2}} = BatchMatMulV2[T=DT_UINT8, adj_x=false, adj_y=false]\nAll kernels registered for op BatchMatMulV2:\n  device='CPU'; T in [DT_HALF]\n  device='CPU'; T in [DT_BFLOAT16]\n  device='CPU'; T in [DT_FLOAT]\n  device='CPU'; T in [DT_DOUBLE]\n  device='CPU'; T in [DT_INT16]\n  device='CPU'; T in [DT_INT32]\n  device='CPU'; T in [DT_INT64]\n  device='CPU'; T in [DT_COMPLEX64]\n  device='CPU'; T in [DT_COMPLEX128]\n  device='XLA_CPU_JIT'; T in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_COMPLEX64, DT_INT64, DT_BFLOAT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]\n [Op:BatchMatMulV2] name: ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Nathan\\Desktop\\Masters\\DL and RL\\CSI5340Assignments\\Assignment2\\Assignment2_Code.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Nathan/Desktop/Masters/DL%20and%20RL/CSI5340Assignments/Assignment2/Assignment2_Code.ipynb#W4sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nathan/Desktop/Masters/DL%20and%20RL/CSI5340Assignments/Assignment2/Assignment2_Code.ipynb#W4sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m softmax \u001b[39m=\u001b[39m SoftmaxRegressionClassifier(x_train, y_train, dropout\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, batch_norm\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Nathan/Desktop/Masters/DL%20and%20RL/CSI5340Assignments/Assignment2/Assignment2_Code.ipynb#W4sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m softmax\u001b[39m.\u001b[39;49mtrain_model()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nathan/Desktop/Masters/DL%20and%20RL/CSI5340Assignments/Assignment2/Assignment2_Code.ipynb#W4sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m train_accuracy \u001b[39m=\u001b[39m softmax\u001b[39m.\u001b[39maccuracy_internal()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nathan/Desktop/Masters/DL%20and%20RL/CSI5340Assignments/Assignment2/Assignment2_Code.ipynb#W4sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m ext_accuracy \u001b[39m=\u001b[39m softmax\u001b[39m.\u001b[39maccuracy_external(x_test, y_test)\n",
      "\u001b[1;32mc:\\Users\\Nathan\\Desktop\\Masters\\DL and RL\\CSI5340Assignments\\Assignment2\\Assignment2_Code.ipynb Cell 5\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nathan/Desktop/Masters/DL%20and%20RL/CSI5340Assignments/Assignment2/Assignment2_Code.ipynb#W4sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m \u001b[39mfor\u001b[39;00m iterations \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_iterations):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nathan/Desktop/Masters/DL%20and%20RL/CSI5340Assignments/Assignment2/Assignment2_Code.ipynb#W4sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m     \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mGradientTape() \u001b[39mas\u001b[39;00m tape:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Nathan/Desktop/Masters/DL%20and%20RL/CSI5340Assignments/Assignment2/Assignment2_Code.ipynb#W4sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m         y_pred \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcalculate_raw_model_output(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mx_train)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nathan/Desktop/Masters/DL%20and%20RL/CSI5340Assignments/Assignment2/Assignment2_Code.ipynb#W4sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m         \u001b[39m# Calculate loss using softmax cross entropy, then reduce mean\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nathan/Desktop/Masters/DL%20and%20RL/CSI5340Assignments/Assignment2/Assignment2_Code.ipynb#W4sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m         loss \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39msoftmax_cross_entropy_with_logits(logits\u001b[39m=\u001b[39mraw_model_output, labels\u001b[39m=\u001b[39mtrainable_variables)\n",
      "\u001b[1;32mc:\\Users\\Nathan\\Desktop\\Masters\\DL and RL\\CSI5340Assignments\\Assignment2\\Assignment2_Code.ipynb Cell 5\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nathan/Desktop/Masters/DL%20and%20RL/CSI5340Assignments/Assignment2/Assignment2_Code.ipynb#W4sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcalculate_raw_model_output\u001b[39m(\u001b[39mself\u001b[39m, x_data):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nathan/Desktop/Masters/DL%20and%20RL/CSI5340Assignments/Assignment2/Assignment2_Code.ipynb#W4sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m   \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nathan/Desktop/Masters/DL%20and%20RL/CSI5340Assignments/Assignment2/Assignment2_Code.ipynb#W4sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39m  Calculates the raw output of the model, i.e. the logits or y_predicted. Also optionally applies batch normalization or dropout.\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nathan/Desktop/Masters/DL%20and%20RL/CSI5340Assignments/Assignment2/Assignment2_Code.ipynb#W4sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39m  :return: raw_model_output (logits)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nathan/Desktop/Masters/DL%20and%20RL/CSI5340Assignments/Assignment2/Assignment2_Code.ipynb#W4sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Nathan/Desktop/Masters/DL%20and%20RL/CSI5340Assignments/Assignment2/Assignment2_Code.ipynb#W4sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m   raw_model_output \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39madd(tf\u001b[39m.\u001b[39;49mmatmul(x_data, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweights) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbiases)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nathan/Desktop/Masters/DL%20and%20RL/CSI5340Assignments/Assignment2/Assignment2_Code.ipynb#W4sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m   \u001b[39m# Optionally apply the batch normalization\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nathan/Desktop/Masters/DL%20and%20RL/CSI5340Assignments/Assignment2/Assignment2_Code.ipynb#W4sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_norm:\n",
      "File \u001b[1;32mc:\\Users\\Nathan\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m    154\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\Nathan\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:6656\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[1;34m(e, name)\u001b[0m\n\u001b[0;32m   6654\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mraise_from_not_ok_status\u001b[39m(e, name):\n\u001b[0;32m   6655\u001b[0m   e\u001b[39m.\u001b[39mmessage \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39m name: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(name \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m-> 6656\u001b[0m   \u001b[39mraise\u001b[39;00m core\u001b[39m.\u001b[39m_status_to_exception(e) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n",
      "\u001b[1;31mNotFoundError\u001b[0m: Could not find device for node: {{node BatchMatMulV2}} = BatchMatMulV2[T=DT_UINT8, adj_x=false, adj_y=false]\nAll kernels registered for op BatchMatMulV2:\n  device='CPU'; T in [DT_HALF]\n  device='CPU'; T in [DT_BFLOAT16]\n  device='CPU'; T in [DT_FLOAT]\n  device='CPU'; T in [DT_DOUBLE]\n  device='CPU'; T in [DT_INT16]\n  device='CPU'; T in [DT_INT32]\n  device='CPU'; T in [DT_INT64]\n  device='CPU'; T in [DT_COMPLEX64]\n  device='CPU'; T in [DT_COMPLEX128]\n  device='XLA_CPU_JIT'; T in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_COMPLEX64, DT_INT64, DT_BFLOAT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]\n [Op:BatchMatMulV2] name: "
     ]
    }
   ],
   "source": [
    "x_train, y_train, x_test, y_test = get_mnist_dataset()\n",
    "\n",
    "\n",
    "num_images = len(x_train)\n",
    "image_size = len(x_train[0][0])*len(x_train[0][1])\n",
    "\n",
    "print(f\"Number of images: {num_images}\")\n",
    "print(f\"Image size: {image_size}\")\n",
    "print(\"test\")\n",
    "\n",
    "softmax = SoftmaxRegressionClassifier(x_train, y_train, dropout=False, batch_norm=False)\n",
    "softmax.train_model()\n",
    "train_accuracy = softmax.accuracy_internal()\n",
    "ext_accuracy = softmax.accuracy_external(x_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(train_dataset)\n",
    "# print(f\"Shape of training tensor: {train_dataset.shape}\")\n",
    "# print(f\"Datatype of training tensor: {train_dataset.dtype}\")\n",
    "# print(f\"Device training tensor is stored on: {train_dataset.device}\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
