{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient ∂L/∂A:\n",
      "[0.18968844463637405, 0.7065160304517885, 0.7981986829563948]\n",
      "[-0.07376755578300863, -0.27475559087349993, -0.31040987227125494]\n",
      "[-0.16144176584057704, -0.6013080858428822, -0.6793381911857007]\n",
      "Gradient ∂L/∂B:\n",
      "[1.1777185815402542, 4.386545837381888, 4.955775890728363]\n",
      "[0.7407669302710993, 2.759070074449518, 3.117107050213333]\n",
      "[-0.6987706032333354, -2.602650012441237, -2.9403887846658003]\n",
      "Gradient ∂L/∂C:\n",
      "[2.3534379551019136, 1.74062997411646, 3.3845686449055257]\n",
      "[5.551898613001112, 4.1062485280717125, 7.984396582252078]\n",
      "[-2.9387451206182766, -2.1735299340046343, -4.226321143946626]\n"
     ]
    }
   ],
   "source": [
    "# import random\n",
    "# import math\n",
    "\n",
    "# random.seed(0)\n",
    "# torch.manual_seed(0)\n",
    "\n",
    "# # Define the sigmoid function\n",
    "# def sigmoid(x):\n",
    "#     return 1 / (1 + math.exp(-x))\n",
    "\n",
    "# # Initialize random values for A, B, and C\n",
    "# K = 3  # Define the size of matrices\n",
    "# A = [[random.uniform(-1, 1) for _ in range(K)] for _ in range(K)]\n",
    "# B = [[random.uniform(-1, 1) for _ in range(K)] for _ in range(K)]\n",
    "# C = [[random.uniform(-1, 1) for _ in range(K)] for _ in range(K)]\n",
    "\n",
    "# # Input vector x\n",
    "# x = [random.uniform(-1, 1) for _ in range(K)]\n",
    "\n",
    "# # Forward propagation\n",
    "# y = [0] * K\n",
    "# for i in range(K):\n",
    "#     y[i] = sum(A[i][j] * x[j] for j in range(K))\n",
    "#     y[i] = sigmoid(y[i])\n",
    "\n",
    "# v = [0] * K\n",
    "# for i in range(K):\n",
    "#     v[i] = sum(B[i][j] * x[j] for j in range(K))\n",
    "\n",
    "# z = [u + v for u, v in zip(y, v)]\n",
    "\n",
    "# w = [0] * K\n",
    "# for i in range(K):\n",
    "#     w[i] = sum(C[i][j] * z[j] for j in range(K))\n",
    "\n",
    "# L = sum(val ** 2 for val in w)\n",
    "\n",
    "# # Backpropagation\n",
    "# # Compute gradients ∂L/∂A, ∂L/∂B, ∂L/∂C\n",
    "# dL_dw = [2 * val for val in w]\n",
    "# dL_dC = [[dL_dw[i] * z[j] for j in range(K)] for i in range(K)]\n",
    "\n",
    "# dL_dz = [0] * K\n",
    "# for i in range(K):\n",
    "#     dL_dz[i] = sum(dL_dw[j] * C[i][j] for j in range(K))\n",
    "\n",
    "# dL_dv = [0] * K\n",
    "# for i in range(K):\n",
    "#     dL_dv[i] = dL_dz[i]\n",
    "\n",
    "# dL_dB = [[dL_dv[i] * x[j] for j in range(K)] for i in range(K)]\n",
    "\n",
    "# dL_dy = [0] * K\n",
    "# for i in range(K):\n",
    "#     dL_dy[i] = sum(dL_dz[j] * B[i][j] for j in range(K))\n",
    "#     dL_dy[i] *= y[i] * (1 - y[i])\n",
    "\n",
    "# dL_dA = [[dL_dy[i] * x[j] for j in range(K)] for i in range(K)]\n",
    "\n",
    "# # Print the gradients\n",
    "# print(\"Gradient ∂L/∂A:\")\n",
    "# for row in dL_dA:\n",
    "#     print(row)\n",
    "\n",
    "# print(\"Gradient ∂L/∂B:\")\n",
    "# for row in dL_dB:\n",
    "#     print(row)\n",
    "\n",
    "# print(\"Gradient ∂L/∂C:\")\n",
    "# for row in dL_dC:\n",
    "#     print(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient ∂L/∂A:\n",
      "tensor([[0.1668, 0.6211, 0.7017],\n",
      "        [0.3561, 1.3264, 1.4986],\n",
      "        [0.0633, 0.2358, 0.2663]])\n",
      "Gradient ∂L/∂B:\n",
      "tensor([[0.6984, 2.6014, 2.9389],\n",
      "        [1.4498, 5.4000, 6.1007],\n",
      "        [0.2569, 0.9570, 1.0812]])\n",
      "Gradient ∂L/∂C:\n",
      "tensor([[ 2.3534,  1.7406,  3.3846],\n",
      "        [ 5.5519,  4.1062,  7.9844],\n",
      "        [-2.9387, -2.1735, -4.2263]])\n"
     ]
    }
   ],
   "source": [
    "############## Auto grad ##############\n",
    "# import torch\n",
    "\n",
    "# x_tensor = torch.tensor(x, requires_grad=True, dtype=torch.float)\n",
    "\n",
    "# # Convert A, B, and C to PyTorch tensors\n",
    "# A_tensor = torch.tensor(A, requires_grad=True, dtype=torch.float)\n",
    "# B_tensor = torch.tensor(B, requires_grad=True, dtype=torch.float)\n",
    "# C_tensor = torch.tensor(C, requires_grad=True, dtype=torch.float)\n",
    "\n",
    "# # Forward propagation\n",
    "# y = torch.sigmoid(torch.matmul(A_tensor, x_tensor))\n",
    "# v = torch.matmul(B_tensor, x_tensor)\n",
    "# z = y + v\n",
    "# w = torch.matmul(C_tensor, z)\n",
    "# L = torch.norm(w)**2\n",
    "\n",
    "# # Backpropagation\n",
    "# L.backward()  # Automatically computes gradients\n",
    "\n",
    "# # Access the gradients\n",
    "# dL_dA = A_tensor.grad\n",
    "# dL_dB = B_tensor.grad\n",
    "# dL_dC = C_tensor.grad\n",
    "\n",
    "# # Print the gradients\n",
    "# print(\"Gradient ∂L/∂A:\")\n",
    "# print(dL_dA)\n",
    "\n",
    "# print(\"Gradient ∂L/∂B:\")\n",
    "# print(dL_dB)\n",
    "\n",
    "# print(\"Gradient ∂L/∂C:\")\n",
    "# print(dL_dC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient ∂L/∂A:\n",
      "[0.18968844463637405, 0.7065160304517885, 0.7981986829563948]\n",
      "[-0.07376755578300863, -0.27475559087349993, -0.31040987227125494]\n",
      "[-0.16144176584057704, -0.6013080858428822, -0.6793381911857007]\n",
      "Gradient ∂L/∂B:\n",
      "[1.1777185815402542, 4.386545837381888, 4.955775890728363]\n",
      "[0.7407669302710993, 2.759070074449518, 3.117107050213333]\n",
      "[-0.6987706032333354, -2.602650012441237, -2.9403887846658003]\n",
      "Gradient ∂L/∂C:\n",
      "[2.3534379551019136, 1.74062997411646, 3.3845686449055257]\n",
      "[5.551898613001112, 4.1062485280717125, 7.984396582252078]\n",
      "[-2.9387451206182766, -2.1735299340046343, -4.226321143946626]\n"
     ]
    }
   ],
   "source": [
    "# # Forward propagation\n",
    "# y = [0] * K\n",
    "# for i in range(K):\n",
    "#     y[i] = sum(A[i][j] * x[j] for j in range(K))\n",
    "#     y[i] = sigmoid(y[i])\n",
    "\n",
    "# v = [0] * K\n",
    "# for i in range(K):\n",
    "#     v[i] = sum(B[i][j] * x[j] for j in range(K))\n",
    "\n",
    "# z = [u + v for u, v in zip(y, v)]\n",
    "\n",
    "# w = [0] * K\n",
    "# for i in range(K):\n",
    "#     w[i] = sum(C[i][j] * z[j] for j in range(K))\n",
    "\n",
    "# L = sum(val ** 2 for val in w)\n",
    "\n",
    "# # Backpropagation\n",
    "# # Compute gradients ∂L/∂A, ∂L/∂B, ∂L/∂C\n",
    "# dL_dw = [2 * val for val in w]\n",
    "# dL_dC = [[dL_dw[i] * z[j] for j in range(K)] for i in range(K)]\n",
    "\n",
    "# dL_dz = [0] * K\n",
    "# for i in range(K):\n",
    "#     dL_dz[i] = sum(dL_dw[j] * C[i][j] for j in range(K))\n",
    "\n",
    "# dL_dv = [0] * K\n",
    "# for i in range(K):\n",
    "#     dL_dv[i] = dL_dz[i]\n",
    "\n",
    "# dL_dB = [[dL_dv[i] * x[j] for j in range(K)] for i in range(K)]\n",
    "\n",
    "# dL_dy = [0] * K\n",
    "# for i in range(K):\n",
    "#     dL_dy[i] = sum(dL_dz[j] * B[i][j] for j in range(K))\n",
    "#     dL_dy[i] *= y[i] * (1 - y[i])\n",
    "\n",
    "# dL_dA = [[dL_dy[i] * x[j] for j in range(K)] for i in range(K)]\n",
    "\n",
    "# # Print the gradients\n",
    "# print(\"Gradient ∂L/∂A:\")\n",
    "# for row in dL_dA:\n",
    "#     print(row)\n",
    "\n",
    "# print(\"Gradient ∂L/∂B:\")\n",
    "# for row in dL_dB:\n",
    "#     print(row)\n",
    "\n",
    "# print(\"Gradient ∂L/∂C:\")\n",
    "# for row in dL_dC:\n",
    "#     print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Manual:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'float' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Nathan\\Desktop\\Masters\\DL and RL\\CSI5340Assignments\\Assignment2\\A2_Question1_Code.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Nathan/Desktop/Masters/DL%20and%20RL/CSI5340Assignments/Assignment2/A2_Question1_Code.ipynb#W3sZmlsZQ%3D%3D?line=136'>137</a>\u001b[0m x \u001b[39m=\u001b[39m [random\u001b[39m.\u001b[39muniform(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m) \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(K)]\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Nathan/Desktop/Masters/DL%20and%20RL/CSI5340Assignments/Assignment2/A2_Question1_Code.ipynb#W3sZmlsZQ%3D%3D?line=138'>139</a>\u001b[0m \u001b[39m# Manual Backpropagation\u001b[39;00m\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/Nathan/Desktop/Masters/DL%20and%20RL/CSI5340Assignments/Assignment2/A2_Question1_Code.ipynb#W3sZmlsZQ%3D%3D?line=139'>140</a>\u001b[0m dL_dA_manual, dL_dB_manual, dL_dC_manual \u001b[39m=\u001b[39m manual_backpropagation(x, A, B, C)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Nathan/Desktop/Masters/DL%20and%20RL/CSI5340Assignments/Assignment2/A2_Question1_Code.ipynb#W3sZmlsZQ%3D%3D?line=141'>142</a>\u001b[0m dL_dA_manual, dL_dB_manual, dL_dC_manual \u001b[39m=\u001b[39m manual_backpropagation_2(x, A, B, C)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Nathan/Desktop/Masters/DL%20and%20RL/CSI5340Assignments/Assignment2/A2_Question1_Code.ipynb#W3sZmlsZQ%3D%3D?line=143'>144</a>\u001b[0m \u001b[39m# Automatic Differentiation with PyTorch\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\Nathan\\Desktop\\Masters\\DL and RL\\CSI5340Assignments\\Assignment2\\A2_Question1_Code.ipynb Cell 4\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nathan/Desktop/Masters/DL%20and%20RL/CSI5340Assignments/Assignment2/A2_Question1_Code.ipynb#W3sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     L \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(val \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m \u001b[39m2\u001b[39m \u001b[39mfor\u001b[39;00m val \u001b[39min\u001b[39;00m w)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nathan/Desktop/Masters/DL%20and%20RL/CSI5340Assignments/Assignment2/A2_Question1_Code.ipynb#W3sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mLoss Manual:\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Nathan/Desktop/Masters/DL%20and%20RL/CSI5340Assignments/Assignment2/A2_Question1_Code.ipynb#W3sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39mfor\u001b[39;00m row \u001b[39min\u001b[39;00m L:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nathan/Desktop/Masters/DL%20and%20RL/CSI5340Assignments/Assignment2/A2_Question1_Code.ipynb#W3sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m     \u001b[39mprint\u001b[39m(row)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nathan/Desktop/Masters/DL%20and%20RL/CSI5340Assignments/Assignment2/A2_Question1_Code.ipynb#W3sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m \u001b[39m# Backpropagation\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nathan/Desktop/Masters/DL%20and%20RL/CSI5340Assignments/Assignment2/A2_Question1_Code.ipynb#W3sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39m# Compute gradients ∂L/∂A, ∂L/∂B, ∂L/∂C\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'float' object is not iterable"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import math\n",
    "import torch\n",
    "\n",
    "# Define the sigmoid function\n",
    "def sigmoid(value):\n",
    "    return 1 / (1 + math.exp(-value))\n",
    "\n",
    "# Manual Backpropagation\n",
    "def manual_backpropagation(x, A, B, C):\n",
    "    K = len(x)\n",
    "    \n",
    "    # Forward propagation\n",
    "    y = [0] * K\n",
    "    for i in range(K):\n",
    "        y[i] = sum(A[i][j] * x[j] for j in range(K))\n",
    "        y[i] = sigmoid(y[i])\n",
    "\n",
    "    v = [0] * K\n",
    "    for i in range(K):\n",
    "        v[i] = sum(B[i][j] * x[j] for j in range(K))\n",
    "\n",
    "    z = [u + v for u, v in zip(y, v)]\n",
    "\n",
    "    w = [0] * K\n",
    "    for i in range(K):\n",
    "        w[i] = sum(C[i][j] * z[j] for j in range(K))\n",
    "\n",
    "    for val in w:\n",
    "        L = sum(val ** 2 for val in w)\n",
    "\n",
    "    print(f\"Loss Manual2: {L}\")\n",
    "    \n",
    "    # Backpropagation\n",
    "    # Compute gradients ∂L/∂A, ∂L/∂B, ∂L/∂C\n",
    "    dL_dw = [2 * val for val in w]\n",
    "    dL_dC = [[dL_dw[i] * z[j] for j in range(K)] for i in range(K)]\n",
    "\n",
    "    dL_dz = [0] * K\n",
    "    for i in range(K):\n",
    "        dL_dz[i] = sum(dL_dw[j] * C[i][j] for j in range(K))\n",
    "\n",
    "    dL_dv = [0] * K\n",
    "    for i in range(K):\n",
    "        dL_dv[i] = dL_dz[i]\n",
    "\n",
    "    dL_dB = [[dL_dv[i] * x[j] for j in range(K)] for i in range(K)]\n",
    "\n",
    "    dL_dy = [0] * K\n",
    "    for i in range(K):\n",
    "        dL_dy[i] = sum(dL_dz[j] * B[i][j] for j in range(K))\n",
    "        dL_dy[i] *= y[i] * (1 - y[i])\n",
    "\n",
    "    dL_dA = [[dL_dy[i] * x[j] for j in range(K)] for i in range(K)]\n",
    "    return dL_dA, dL_dB, dL_dC\n",
    "\n",
    "\n",
    "def manual_backpropagation_2(x, A, B, C):\n",
    "    K = len(x)\n",
    "\n",
    "    x_tensor = torch.tensor(x, requires_grad=True, dtype=torch.float)\n",
    "    # Convert A, B, and C to PyTorch tensors\n",
    "    A_tensor = torch.tensor(A, requires_grad=True, dtype=torch.float)\n",
    "    B_tensor = torch.tensor(B, requires_grad=True, dtype=torch.float)\n",
    "    C_tensor = torch.tensor(C, requires_grad=True, dtype=torch.float)\n",
    "    \n",
    "    # Forward propagation\n",
    "    y = torch.sigmoid(torch.matmul(A_tensor, x_tensor))\n",
    "    v = torch.matmul(B_tensor, x_tensor)\n",
    "    z = y + v\n",
    "    w = torch.matmul(C_tensor, z)\n",
    "    L = torch.norm(w)**2\n",
    "\n",
    "    print(f\"Loss Manual2: {L}\")\n",
    "\n",
    "    # Compute gradients manually\n",
    "    dL_dA = torch.zeros_like(A_tensor)\n",
    "    dL_dB = torch.zeros_like(B_tensor)\n",
    "    dL_dC = torch.zeros_like(C_tensor)\n",
    "\n",
    "    for i in range(K):\n",
    "        for j in range(K):\n",
    "            for k in range(K):\n",
    "                dL_dA[i, j] += 2 * w[k] * C_tensor[k, i] * z[j] * y[i] * (1 - y[i]) * x_tensor[j]\n",
    "                dL_dB[i, j] += 2 * w[i] * C_tensor[i, k] * z[k] * x_tensor[j]\n",
    "                dL_dC[i, j] += 2 * w[i] * z[i] * y[j] * (1 - y[j])\n",
    "\n",
    "    return dL_dA, dL_dB, dL_dC\n",
    "\n",
    "# Automatic Differentiation with PyTorch\n",
    "def autograd_torch_backpropagation(x, A, B, C):\n",
    "    # Set a random seed for reproducibility\n",
    "    random.seed(0)\n",
    "    torch.manual_seed(0)\n",
    "    \n",
    "    K = len(x)\n",
    "\n",
    "    x_tensor = torch.tensor(x, requires_grad=True, dtype=torch.float)\n",
    "    # Convert A, B, and C to PyTorch tensors\n",
    "    A_tensor = torch.tensor(A, requires_grad=True, dtype=torch.float)\n",
    "    B_tensor = torch.tensor(B, requires_grad=True, dtype=torch.float)\n",
    "    C_tensor = torch.tensor(C, requires_grad=True, dtype=torch.float)\n",
    "\n",
    "    # Forward propagation\n",
    "    y = torch.sigmoid(torch.matmul(A_tensor, x_tensor))\n",
    "    v = torch.matmul(B_tensor, x_tensor)\n",
    "    z = y + v\n",
    "    w = torch.matmul(C_tensor, z)\n",
    "    L = torch.norm(w)**2\n",
    "\n",
    "    print(f\"Loss Automatic: {L}\")\n",
    "\n",
    "    # Backpropagation\n",
    "    L.backward()  # Automatically computes gradients\n",
    "\n",
    "    # Access the gradients\n",
    "    dL_dA = A_tensor.grad\n",
    "    dL_dB = B_tensor.grad\n",
    "    dL_dC = C_tensor.grad\n",
    "\n",
    "    return dL_dA, dL_dB, dL_dC\n",
    "\n",
    "# Define the input data\n",
    "K = 3\n",
    "A = [[random.uniform(-1, 1) for _ in range(K)] for _ in range(K)]\n",
    "B = [[random.uniform(-1, 1) for _ in range(K)] for _ in range(K)]\n",
    "C = [[random.uniform(-1, 1) for _ in range(K)] for _ in range(K)]\n",
    "\n",
    "# Input vector x\n",
    "x = [random.uniform(-1, 1) for _ in range(K)]\n",
    "\n",
    "# Manual Backpropagation\n",
    "dL_dA_manual, dL_dB_manual, dL_dC_manual = manual_backpropagation(x, A, B, C)\n",
    "\n",
    "dL_dA_manual, dL_dB_manual, dL_dC_manual = manual_backpropagation_2(x, A, B, C)\n",
    "\n",
    "# Automatic Differentiation with PyTorch\n",
    "dL_dA_torch, dL_dB_torch, dL_dC_torch = autograd_torch_backpropagation(x, A, B, C)\n",
    "\n",
    "# Print the gradients\n",
    "print(\"Manual Backpropagation Gradient ∂L/∂A:\")\n",
    "for row in dL_dA_manual:\n",
    "    print(row)\n",
    "\n",
    "print(\"Manual Backpropagation Gradient 2 ∂L/∂A:\")\n",
    "for row in dL_dA_manual:\n",
    "    print(row)\n",
    "\n",
    "print(\"Automatic Differentiation with PyTorch Gradient ∂L/∂A:\")\n",
    "print(dL_dA_torch)\n",
    "\n",
    "print(\"Manual Backpropagation Gradient ∂L/∂B:\")\n",
    "for row in dL_dB_manual:\n",
    "    print(row)\n",
    "\n",
    "print(\"Automatic Differentiation with PyTorch Gradient ∂L/∂B:\")\n",
    "print(dL_dB_torch)\n",
    "\n",
    "print(\"Manual Backpropagation Gradient ∂L/∂C:\")\n",
    "for row in dL_dC_manual:\n",
    "    print(row)\n",
    "\n",
    "print(\"Automatic Differentiation with PyTorch Gradient ∂L/∂C:\")\n",
    "print(dL_dC_torch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
