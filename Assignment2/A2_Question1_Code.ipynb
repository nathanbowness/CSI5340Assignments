{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 775,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "K = 3\n",
    "# Define the input data\n",
    "A = [[random.uniform(-1, 1) for _ in range(K)] for _ in range(K)]\n",
    "B = [[random.uniform(-1, 1) for _ in range(K)] for _ in range(K)]\n",
    "C = [[random.uniform(-1, 1) for _ in range(K)] for _ in range(K)]\n",
    "# Input vector x\n",
    "x = [random.uniform(-1, 1) for _ in range(K)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 776,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.2475494922680394, 0.24990341037668337, 0.24613131555999815]\n",
      "[2.6941037971865462, -0.25914906232662077, -1.3628943180419348]\n",
      "[0.6669240271109266, -0.0647622344713422, -0.33545097146890795]\n",
      "Manual Backpropagation Loss: 1.663755752483803\n",
      "Manual Backpropagation Gradient ∂L/∂A:\n",
      "[[-0.300422123654174, -0.2945285599976424, -0.4893708903715612], [0.029172750150796622, 0.028600450554619992, 0.047520783563584565], [0.15110700639637215, 0.14814264827815948, 0.24614488893866265]]\n",
      "Manual Backpropagation Gradient ∂L/∂B:\n",
      "[[-1.2135840833350835, -1.1897764657046253, -1.9768608123085325], [0.11673610258789216, 0.1144460194100996, 0.19015660287290892], [0.6139284066823165, 0.6018845994509281, 1.0000551468984498]]\n",
      "Manual Backpropagation Gradient ∂L/∂C:\n",
      "[[2.314151963169928, -0.026865090069508626, 0.591445414794839], [2.735048533881765, -0.031751296533941445, 0.6990171520066827], [-0.7215167400321055, 0.008376118991368682, -0.18440351989899695]]\n"
     ]
    }
   ],
   "source": [
    "# Define the sigmoid function\n",
    "def sigmoid(value):\n",
    "    return 1 / (1 + math.exp(-value))\n",
    "\n",
    "def sigmoid_derivative(value):\n",
    "    return sigmoid(value) * (1 - sigmoid(value))\n",
    "\n",
    "def matrix_vector_multiplication(matrix, vector):\n",
    "    if len(matrix) != len(matrix[0]) or len(matrix) != len(vector):\n",
    "        raise ValueError(\"Matrix and vector dimensions don't match for multiplication\")\n",
    "\n",
    "    result = [0] * len(vector)\n",
    "    for i in range(len(matrix)):\n",
    "        for j in range(len(vector)):\n",
    "            result[i] += matrix[i][j] * vector[j]\n",
    "\n",
    "    return result\n",
    "\n",
    "def outer_product_manual(arr1, arr2):\n",
    "    # Ensure arr1 and arr2 have the same length\n",
    "    if len(arr1) != len(arr2):\n",
    "        raise ValueError(\"Input arrays must have the same length.\")\n",
    "\n",
    "    outer_result = []\n",
    "    for i in range(len(arr1)):\n",
    "        row = [arr1[i] * val for val in arr2]\n",
    "        outer_result.append(row)\n",
    "\n",
    "    return outer_result\n",
    "\n",
    "def matrix_vector_dot_product_manual(matrix, vector):\n",
    "    # Ensure the matrix has the correct shape (k x k) and the vector has length k\n",
    "    k = len(vector)\n",
    "    if len(matrix) != k or len(matrix[0]) != k:\n",
    "        raise ValueError(\"Matrix dimensions must be k x k, and the vector must have length k.\")\n",
    "\n",
    "    result = [sum(matrix[i][j] * vector[i] for i in range(k)) for j in range(k)]\n",
    "    return result\n",
    "\n",
    "def vector_elementwise_multiply_manual(vector1, vector2):\n",
    "    # Ensure both vectors have the same length (k)\n",
    "    k = len(vector1)\n",
    "    if len(vector2) != k:\n",
    "        raise ValueError(\"Both vectors must have the same length (k).\")\n",
    "\n",
    "    result = [vector1[i] * vector2[i] for i in range(k)]\n",
    "    return result\n",
    "\n",
    "# Manual Backpropagation\n",
    "def manual_backpropagation(x, A, B, C):\n",
    "    K = len(x)\n",
    "    \n",
    "    # Initialze all other vectors to zero, of length K\n",
    "    y, u, v, w = [0] * K, [0] * K, [0] * K, [0] * K\n",
    "\n",
    "    # Forward propagation\n",
    "    y = matrix_vector_multiplication(A, x)\n",
    "    for i in range(K):\n",
    "        u[i] = sigmoid(y[i])\n",
    "    v = matrix_vector_multiplication(B, x)\n",
    "    z = [u + v for u, v in zip(u, v)]\n",
    "    w = matrix_vector_multiplication(C, z)\n",
    "\n",
    "    L = sum(val ** 2 for val in w)\n",
    "\n",
    "    #Backpropagation\n",
    "    #Compute gradients ∂L/∂A, ∂L/∂B, ∂L/∂C\n",
    "    dL_dw = [round(2 * val, 4) for val in w]\n",
    "    dL_dz = matrix_vector_dot_product_manual(C, dL_dw)\n",
    "\n",
    "    #Compute gradient ∂L/∂A\n",
    "    y_prime = []\n",
    "    for i in range(K):\n",
    "        y_prime.append(sigmoid_derivative(y[i]))\n",
    "\n",
    "    dL_dy = vector_elementwise_multiply_manual(dL_dz, y_prime)\n",
    "    dL_dA = outer_product_manual(dL_dy, x)\n",
    "\n",
    "    #Compute gradient ∂L/∂b\n",
    "    dL_dv = dL_dz # dz_dv = 1\n",
    "    dL_dB = outer_product_manual(dL_dv, x)\n",
    "\n",
    "    #Compute gradient ∂L/∂C\n",
    "    dL_dC = outer_product_manual(dL_dw, z)\n",
    "\n",
    "    return dL_dA, dL_dB, dL_dC, L\n",
    "\n",
    "\n",
    "# Manual Backpropagation\n",
    "dL_dA, dL_dB, dL_dC, loss = manual_backpropagation(x, A, B, C)\n",
    "\n",
    "print(f\"Manual Backpropagation Loss: {loss}\")\n",
    "print(\"Manual Backpropagation Gradient ∂L/∂A:\")\n",
    "print(dL_dA)\n",
    "print(\"Manual Backpropagation Gradient ∂L/∂B:\")\n",
    "print(dL_dB)\n",
    "print(\"Manual Backpropagation Gradient ∂L/∂C:\")\n",
    "print(dL_dC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verify the gradients using libraries - Torch, mxnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 777,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss (torch): 1.6637557744979858\n",
      "Gradients (torch) dL/dA:\n",
      "tensor([[-0.3004, -0.2945, -0.4894],\n",
      "        [ 0.0292,  0.0286,  0.0475],\n",
      "        [ 0.1511,  0.1481,  0.2462]])\n",
      "Gradients (torch) dL/dB:\n",
      "tensor([[-1.2136, -1.1898, -1.9769],\n",
      "        [ 0.1167,  0.1144,  0.1901],\n",
      "        [ 0.6139,  0.6019,  1.0001]])\n",
      "Gradients (torch) dL/dC:\n",
      "tensor([[ 2.3141, -0.0269,  0.5914],\n",
      "        [ 2.7351, -0.0318,  0.6990],\n",
      "        [-0.7215,  0.0084, -0.1844]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "A_torch = torch.tensor(A, requires_grad=True, dtype=torch.float32)\n",
    "B_torch = torch.tensor(B, requires_grad=True, dtype=torch.float32)\n",
    "C_torch = torch.tensor(C, requires_grad=True, dtype=torch.float32)\n",
    "x_torch = torch.tensor(x, dtype=torch.float32)\n",
    "\n",
    "# Define the sigmoid function\n",
    "def sigmoid(value):\n",
    "    return 1 / (1 + torch.exp(-value))\n",
    "\n",
    "def forwardpropagation_torch(x, A, B, C):\n",
    "    y = torch.matmul(A, x.unsqueeze(1)).squeeze()\n",
    "    u = sigmoid(y)\n",
    "    v = torch.matmul(B, x.unsqueeze(1)).squeeze()\n",
    "    z = u + v\n",
    "    w = torch.matmul(C, z.unsqueeze(1)).squeeze()\n",
    "    L = torch.norm(w, p=2)**2\n",
    "    return L\n",
    "\n",
    "def backwardpropagation_torch(L_torch):\n",
    "    L_torch.backward()\n",
    "    dA = A_torch.grad\n",
    "    dB = B_torch.grad\n",
    "    dC = C_torch.grad\n",
    "    return dA, dB, dC\n",
    "\n",
    "L_torch = forwardpropagation_torch(x_torch, A_torch, B_torch, C_torch)\n",
    "print(f\"Loss (torch): {L_torch}\")\n",
    "dA_torch, dB_torch, dC_torch = backwardpropagation_torch(L_torch)\n",
    "\n",
    "\n",
    "print(f\"Gradients (torch) dL/dA:\\n{dA_torch}\")\n",
    "print(f\"Gradients (torch) dL/dB:\\n{dB_torch}\")\n",
    "print(f\"Gradients (torch) dL/dC:\\n{dC_torch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 778,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss (mx): \n",
      "[1.6637558]\n",
      "<NDArray 1 @cpu(0)>\n",
      "Gradients (mx) dL/dA:\n",
      "\n",
      "[[-0.3004252  -0.29453158 -0.4893759 ]\n",
      " [ 0.02917137  0.02859909  0.04751853]\n",
      " [ 0.1511112   0.14814675  0.24615172]]\n",
      "<NDArray 3x3 @cpu(0)>\n",
      "Gradients (mx) dL/dB:\n",
      "\n",
      "[[-1.2135965  -1.1897885  -1.9768808 ]\n",
      " [ 0.11673056  0.11444059  0.19014758]\n",
      " [ 0.6139454   0.6019013   1.0000829 ]]\n",
      "<NDArray 3x3 @cpu(0)>\n",
      "Gradients (mx) dL/dC:\n",
      "\n",
      "[[ 2.3141418  -0.0268649   0.59144276]\n",
      " [ 2.7350845  -0.03175163  0.69902635]\n",
      " [-0.72155     0.00837648 -0.18441202]]\n",
      "<NDArray 3x3 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "import mxnet as mx\n",
    "from mxnet import autograd\n",
    "\n",
    "x_mx = mx.nd.array(x_torch.detach().numpy())\n",
    "A_mx = mx.nd.array(A_torch.detach().numpy())\n",
    "B_mx = mx.nd.array(B_torch.detach().numpy())\n",
    "C_mx = mx.nd.array(C_torch.detach().numpy())\n",
    "\n",
    "def sigmoid(value):\n",
    "    return 1 / (1 + mx.nd.exp(-value))\n",
    "\n",
    "def forwardpropagation_mx(x, A, B, C):\n",
    "    y = mx.nd.dot(A, x)\n",
    "    u = sigmoid(y)\n",
    "    v = mx.nd.dot(B, x)\n",
    "    z = u + v\n",
    "    w = mx.nd.dot(C, z)\n",
    "    L = mx.nd.norm(w) ** 2\n",
    "    return L\n",
    "\n",
    "def backwardpropagation_mx(L_mx):\n",
    "    L_mx.backward()\n",
    "    dA = A_mx.grad\n",
    "    dB = B_mx.grad\n",
    "    dC = C_mx.grad\n",
    "    return dA, dB, dC\n",
    "\n",
    "A_mx.attach_grad()\n",
    "B_mx.attach_grad()\n",
    "C_mx.attach_grad()\n",
    "\n",
    "with autograd.record():\n",
    "    L_mx = forwardpropagation_mx(x_mx, A_mx, B_mx, C_mx)\n",
    "    print(f\"Loss (mx): {L_mx}\")\n",
    "\n",
    "dA_mx, dB_mx, dC_mx = backwardpropagation_mx(L_mx)\n",
    "\n",
    "print(f\"Gradients (mx) dL/dA:\\n{dA_mx}\")\n",
    "print(f\"Gradients (mx) dL/dB:\\n{dB_mx}\")\n",
    "print(f\"Gradients (mx) dL/dC:\\n{dC_mx}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 779,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def manual_backpropagation2(x, A, B, C):\n",
    "    K = len(x)\n",
    "    \n",
    "    # Forward propagation\n",
    "    y = [0] * K\n",
    "    for i in range(K):\n",
    "        y[i] = sum(A[i][j] * x[j] for j in range(K))\n",
    "    \n",
    "    # Convert y to a NumPy array\n",
    "    y = np.array([sigmoid(y[i]) for i in range(K)])\n",
    "\n",
    "    v = [0] * K\n",
    "    for i in range(K):\n",
    "        v[i] = sum(B[i][j] * x[j] for j in range(K))\n",
    "\n",
    "    z = [u + v for u, v in zip(y, v)]\n",
    "\n",
    "    w = [0] * K\n",
    "    for i in range(K):\n",
    "        w[i] = sum(C[i][j] * z[j] for j in range(K))\n",
    "\n",
    "    L = sum(val ** 2 for val in w)\n",
    "\n",
    "    # Backpropagation\n",
    "    # Compute gradients ∂L/∂A, ∂L/∂B, ∂L/∂C\n",
    "    dL_dw = [round(2 * val, 4) for val in w]\n",
    "    dL_dC = np.outer(dL_dw, z)\n",
    "\n",
    "    dL_dz = np.dot(dL_dw, C)\n",
    "    dL_dv = dL_dz\n",
    "    dL_dB = np.outer(dL_dv, x)\n",
    "\n",
    "    dL_dy = np.multiply(dL_dz, np.multiply(y, 1 - y))\n",
    "    dL_dA = np.outer(dL_dy, x)\n",
    "\n",
    "    return dL_dA, dL_dB, dL_dC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 780,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.1, 0.4, 0.8999999999999999, 1.6]\n"
     ]
    }
   ],
   "source": [
    "def custom_elementwise_multiply(vector1, vector2):\n",
    "    # Ensure both vectors have the same length (k)\n",
    "    k = len(vector1)\n",
    "    if len(vector2) != k:\n",
    "        raise ValueError(\"Both vectors must have the same length (k).\")\n",
    "\n",
    "    result = [vector1[i] * vector2[i] for i in range(k)]\n",
    "    return result\n",
    "\n",
    "# Example usage:\n",
    "k = 4\n",
    "vector1 = [1, 2, 3, 4]\n",
    "vector2 = [0.1, 0.2, 0.3, 0.4]\n",
    "\n",
    "result = custom_elementwise_multiply(vector1, vector2)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
