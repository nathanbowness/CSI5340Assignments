{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "\n",
    "K = 3\n",
    "# Define the input data\n",
    "A = [[random.uniform(-1, 1) for _ in range(K)] for _ in range(K)]\n",
    "B = [[random.uniform(-1, 1) for _ in range(K)] for _ in range(K)]\n",
    "C = [[random.uniform(-1, 1) for _ in range(K)] for _ in range(K)]\n",
    "# Input vector x\n",
    "x = [random.uniform(-1, 1) for _ in range(K)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Manual Good: 2.5623731019213514\n",
      "Gradient dL/dA:\n",
      "[1.5652832703147892, 2.0823644836416766, 0.6996088004250455]\n",
      "[1.4729555301955382, 1.9595368712053904, 0.6583425959393906]\n",
      "[0.6015556063265368, 0.8002756135622635, 0.26886736995945326]\n",
      "Gradient dL/dA:\n",
      "[-0.0875055771261898, -0.11641247905978047, -0.03911092196841996]\n",
      "[-1.2326785886564526, -1.6398859947231954, -0.550949981434444]\n",
      "[0.15977480094479973, 0.2125553739557054, 0.07141190284660875]\n",
      "Loss Manual2: 2.5623731019213514\n",
      "--- Manual Backpropagation Gradient ∂L/∂A: ----\n",
      "[-0.0875055771261898, -0.11641247905978047, -0.03911092196841996]\n",
      "[-1.2326785886564526, -1.6398859947231954, -0.550949981434444]\n",
      "[0.15977480094479973, 0.2125553739557054, 0.07141190284660875]\n",
      "--- Manual Backpropagation Gradient Rounded ∂L/∂A: --- \n",
      "[0.2919, 0.3884, 0.1305]\n",
      "[0.3558, 0.4734, 0.159]\n",
      "[-0.1233, -0.164, -0.0551]\n",
      "--- Automatic Differentiation with PyTorch Gradient ∂L/∂A: ---\n",
      "[[0.8068272891978621, 1.0733574703833884, 0.3606142623835685], [0.7538935417432827, 1.002937401520513, 0.33695533989904397], [0.7949013362811232, 1.05749185598734, 0.3552839162588525]]\n"
     ]
    }
   ],
   "source": [
    "# Define the sigmoid function\n",
    "def sigmoid(value):\n",
    "    return 1 / (1 + math.exp(-value))\n",
    "\n",
    "def sigmoid_derivative(value):\n",
    "    return sigmoid(value) * (1 - sigmoid(value))\n",
    "\n",
    "def matrix_vector_multiplication(matrix, vector):\n",
    "    if len(matrix) != len(matrix[0]) or len(matrix) != len(vector):\n",
    "        raise ValueError(\"Matrix and vector dimensions don't match for multiplication\")\n",
    "\n",
    "    result = [0] * len(vector)\n",
    "    for i in range(len(matrix)):\n",
    "        for j in range(len(vector)):\n",
    "            result[i] += matrix[i][j] * vector[j]\n",
    "\n",
    "    return result\n",
    "\n",
    "# Manual Backpropagation\n",
    "def manual_backpropagation(x, A, B, C):\n",
    "    K = len(x)\n",
    "    \n",
    "    # Initialze all other vectors to zero, of length K\n",
    "    y, u, v, w = [0] * K, [0] * K, [0] * K, [0] * K\n",
    "\n",
    "    # Forward propagation\n",
    "    y = matrix_vector_multiplication(A, x)\n",
    "    for i in range(K):\n",
    "        u[i] = sigmoid(y[i])\n",
    "    v = matrix_vector_multiplication(B, x)\n",
    "    z = [u + v for u, v in zip(u, v)]\n",
    "    w = matrix_vector_multiplication(C, z)\n",
    "\n",
    "    L = sum(val ** 2 for val in w)\n",
    "\n",
    "    print(f\"Loss Manual Good: {L}\")\n",
    "\n",
    "    #Backpropagation\n",
    "    #Compute gradients ∂L/∂A, ∂L/∂B, ∂L/∂C\n",
    "    dL_dw = [2 * val for val in w]\n",
    "    dL_dz = [dL_dw[i] for i in range(K)]\n",
    "    dL_dy = [dL_dz[i] for i in range(K)]\n",
    "    dL_dv = [dL_dy[i] * sigmoid_derivative(y[i]) for i in range(K)]\n",
    "\n",
    "    dL_dC = [[dL_dw[i] * z[j] for j in range(K)] for i in range(K)]\n",
    "    dL_dB = [[dL_dv[i] * x[j] for j in range(K)] for i in range(K)]\n",
    "    dL_dA = [[dL_dy[i] * x[j] for j in range(K)] for i in range(K)]\n",
    "\n",
    "    print(\"Gradient dL/dA:\")\n",
    "    for row in dL_dA:\n",
    "        print(row)\n",
    "\n",
    "    dL_dw = [2 * val for val in w]\n",
    "    dL_dC = [[dL_dw[i] * z[j] for j in range(K)] for i in range(K)]\n",
    "\n",
    "    dL_dz = [0] * K\n",
    "    for i in range(K):\n",
    "        dL_dz[i] = sum(dL_dw[j] * C[i][j] for j in range(K))\n",
    "\n",
    "    dL_dv = [0] * K\n",
    "    for i in range(K):\n",
    "        dL_dv[i] = dL_dz[i]\n",
    "\n",
    "    dL_dB = [[dL_dv[i] * x[j] for j in range(K)] for i in range(K)]\n",
    "\n",
    "    dL_dy = [0] * K\n",
    "    for i in range(K):\n",
    "        dL_dy[i] = sum(dL_dz[j] * B[i][j] for j in range(K))\n",
    "        dL_dy[i] *= y[i] * (1 - y[i])\n",
    "\n",
    "    dL_dA = [[dL_dy[i] * x[j] for j in range(K)] for i in range(K)]\n",
    "    print(\"Gradient dL/dA:\")\n",
    "    for row in dL_dA:\n",
    "        print(row)\n",
    "\n",
    "\n",
    "    return dL_dA, dL_dB, dL_dC\n",
    "\n",
    "def manual_backpropagation_rounded(x, A, B, C):\n",
    "    K = len(x)\n",
    "    \n",
    "    # Forward propagation\n",
    "    y = [0] * K\n",
    "    for i in range(K):\n",
    "        y[i] = sum(A[i][j] * x[j] for j in range(K))\n",
    "        y[i] = sigmoid(y[i])\n",
    "\n",
    "    v = [0] * K\n",
    "    for i in range(K):\n",
    "        v[i] = sum(B[i][j] * x[j] for j in range(K))\n",
    "\n",
    "    z = [u + v for u, v in zip(y, v)]\n",
    "\n",
    "    w = [0] * K\n",
    "    for i in range(K):\n",
    "        w[i] = sum(C[i][j] * z[j] for j in range(K))\n",
    "\n",
    "    for val in w:\n",
    "        L = sum(val ** 2 for val in w)\n",
    "\n",
    "    print(f\"Loss Manual2: {L}\")\n",
    "    \n",
    "    # Backpropagation ---- !!! Something wrong with this back propogation algorithm !!!\n",
    "    # Compute gradients ∂L/∂A, ∂L/∂B, ∂L/∂C\n",
    "    dL_dw = [round(2 * val, 4) for val in w]\n",
    "    dL_dC = [[round(dL_dw[i] * z[j], 4) for j in range(K)] for i in range(K)]\n",
    "\n",
    "    dL_dz = [0] * K\n",
    "    for i in range(K):\n",
    "        dL_dz[i] = sum(round(dL_dw[j] * C[i][j], 4) for j in range(K))\n",
    "\n",
    "    dL_dv = [0] * K\n",
    "    for i in range(K):\n",
    "        dL_dv[i] = dL_dz[i]\n",
    "\n",
    "    dL_dB = [[round(dL_dv[i] * x[j], 4) for j in range(K)] for i in range(K)]\n",
    "\n",
    "    dL_dy = [0] * K\n",
    "    for i in range(K):\n",
    "        dL_dy[i] = sum(round(dL_dz[j] * B[i][j], 4) for j in range(K))\n",
    "        dL_dy[i] *= round(y[i] * (1 - y[i]), 4)\n",
    "\n",
    "    dL_dA = [[round(dL_dy[i] * x[j], 4) for j in range(K)] for i in range(K)]\n",
    "    return dL_dA, dL_dB, dL_dC\n",
    "\n",
    "\n",
    "def backwardpropagation(x, A, B, C):\n",
    "    K = len(x)\n",
    "    \n",
    "    # Initialize gradients\n",
    "    dA, dB, dC = [[0] * K for _ in range(K)], [[0] * K for _ in range(K)], [[0] * K for _ in range(K)]\n",
    "\n",
    "    # Forward propagation\n",
    "    y = matrix_vector_multiplication(A, x)\n",
    "    u = [sigmoid(val) for val in y]\n",
    "    v = matrix_vector_multiplication(B, x)\n",
    "    z = [u_i + v_i for u_i, v_i in zip(u, v)]\n",
    "    w = matrix_vector_multiplication(C, z)\n",
    "\n",
    "    # Compute the loss L\n",
    "    L = sum(val ** 2 for val in w)\n",
    "\n",
    "    # Backpropagation\n",
    "    # Gradients of L w.r.t. w\n",
    "    dL_dw = [2 * val for val in w]\n",
    "\n",
    "    # Gradients of L w.r.t. C\n",
    "    for i in range(K):\n",
    "        for j in range(K):\n",
    "            dC[i][j] = dL_dw[i] * z[j]\n",
    "\n",
    "    # Gradients of L w.r.t. z\n",
    "    dL_dz = sum(dL_dw[i] * z[i] for i in range(K))\n",
    "\n",
    "    # Gradients of L w.r.t. u and v\n",
    "    dL_du = [dL_dz * sigmoid(y_i) * (1 - sigmoid(y_i)) for y_i in y]\n",
    "    dL_dv = [dL_dz] * K  # Same gradient for all elements of v\n",
    "\n",
    "    # Gradients of L w.r.t. A\n",
    "    for i in range(K):\n",
    "        for j in range(K):\n",
    "            dA[i][j] = dL_du[i] * x[j]\n",
    "\n",
    "    # Gradients of L w.r.t. B\n",
    "    for i in range(K):\n",
    "        for j in range(K):\n",
    "            dB[i][j] = dL_dv[i] * x[j]\n",
    "\n",
    "    return dA, dB, dC, L\n",
    "\n",
    "\n",
    "# Manual Backpropagation\n",
    "dL_dA_manual, dL_dB_manual, dL_dC_manual = manual_backpropagation(x, A, B, C)\n",
    "\n",
    "dL_dA_manual_round, dL_dB_manual_round, dL_dC_manual_round = manual_backpropagation_rounded(x, A, B, C)\n",
    "\n",
    "dL_dA_final, dL_dB_final, dL_dC_final, L = backwardpropagation(x, A, B, C)\n",
    "\n",
    "#Print the gradients\n",
    "print(\"--- Manual Backpropagation Gradient ∂L/∂A: ----\")\n",
    "for row in dL_dA_manual:\n",
    "    print(row)\n",
    "\n",
    "print(\"--- Manual Backpropagation Gradient Rounded ∂L/∂A: --- \")\n",
    "for row in dL_dA_manual_round:\n",
    "    print(row)\n",
    "\n",
    "print(\"--- Automatic Differentiation with PyTorch Gradient ∂L/∂A: ---\")\n",
    "print(dL_dA_final)\n",
    "\n",
    "# print(\"Manual Backpropagation Gradient ∂L/∂B:\")\n",
    "# for row in dL_dB_manual:\n",
    "#     print(row)\n",
    "\n",
    "# print(\"Automatic Differentiation with PyTorch Gradient ∂L/∂B:\")\n",
    "# print(dL_dB_torch)\n",
    "\n",
    "# print(\"Manual Backpropagation Gradient ∂L/∂C:\")\n",
    "# for row in dL_dC_manual:\n",
    "#     print(row)\n",
    "\n",
    "# print(\"Automatic Differentiation with PyTorch Gradient ∂L/∂C:\")\n",
    "# print(dL_dC_torch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verify the gradients using libraries - Torch, mxnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients (torch) dL/dA:\n",
      "tensor([[-0.6609, -0.8792, -0.2954],\n",
      "        [-0.5241, -0.6972, -0.2342],\n",
      "        [ 0.5251,  0.6986,  0.2347]])\n",
      "Gradients (torch) dL/dB:\n",
      "tensor([[-2.6469, -3.5212, -1.1830],\n",
      "        [-2.2461, -2.9881, -1.0039],\n",
      "        [ 2.1346,  2.8398,  0.9541]])\n",
      "Gradients (torch) dL/dC:\n",
      "tensor([[-2.5464, -1.7239, -1.2134],\n",
      "        [-2.3962, -1.6222, -1.1419],\n",
      "        [-0.9786, -0.6625, -0.4663]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "A_torch = torch.tensor(A, requires_grad=True, dtype=torch.float32)\n",
    "B_torch = torch.tensor(B, requires_grad=True, dtype=torch.float32)\n",
    "C_torch = torch.tensor(C, requires_grad=True, dtype=torch.float32)\n",
    "x_torch = torch.tensor(x, dtype=torch.float32)\n",
    "\n",
    "# Define the sigmoid function\n",
    "def sigmoid(value):\n",
    "    return 1 / (1 + torch.exp(-value))\n",
    "\n",
    "def forwardpropagation_torch(x, A, B, C):\n",
    "    y = torch.matmul(A, x.unsqueeze(1)).squeeze()\n",
    "    u = sigmoid(y)\n",
    "    v = torch.matmul(B, x.unsqueeze(1)).squeeze()\n",
    "    z = u + v\n",
    "    w = torch.matmul(C, z.unsqueeze(1)).squeeze()\n",
    "    L = torch.norm(w, p=2)**2\n",
    "    return L\n",
    "\n",
    "def backwardpropagation_torch(L_torch):\n",
    "    L_torch.backward()\n",
    "    dA = A_torch.grad\n",
    "    dB = B_torch.grad\n",
    "    dC = C_torch.grad\n",
    "    return dA, dB, dC\n",
    "\n",
    "L_torch = forwardpropagation_torch(x_torch, A_torch, B_torch, C_torch)\n",
    "dA_torch, dB_torch, dC_torch = backwardpropagation_torch(L_torch)\n",
    "\n",
    "\n",
    "print(f\"Gradients (torch) dL/dA:\\n{dA_torch}\")\n",
    "print(f\"Gradients (torch) dL/dB:\\n{dB_torch}\")\n",
    "print(f\"Gradients (torch) dL/dC:\\n{dC_torch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients (mx) dL/dA:\n",
      "\n",
      "[[-0.66090703 -0.8792334  -0.29539472]\n",
      " [-0.5240515  -0.6971685  -0.23422666]\n",
      " [ 0.5251271   0.69859946  0.23470742]]\n",
      "<NDArray 3x3 @cpu(0)>\n",
      "Gradients (mx) dL/dB:\n",
      "\n",
      "[[-2.6468637  -3.5212379  -1.183025  ]\n",
      " [-2.246134   -2.9881299  -1.0039175 ]\n",
      " [ 2.1346316   2.8397932   0.95408106]]\n",
      "<NDArray 3x3 @cpu(0)>\n",
      "Gradients (mx) dL/dC:\n",
      "\n",
      "[[-2.5463698  -1.7238632  -1.213438  ]\n",
      " [-2.3961732  -1.6221819  -1.1418638 ]\n",
      " [-0.97859794 -0.6624996  -0.46633756]]\n",
      "<NDArray 3x3 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "import mxnet as mx\n",
    "from mxnet import autograd\n",
    "\n",
    "x_mx = mx.nd.array(x_torch.detach().numpy())\n",
    "A_mx = mx.nd.array(A_torch.detach().numpy())\n",
    "B_mx = mx.nd.array(B_torch.detach().numpy())\n",
    "C_mx = mx.nd.array(C_torch.detach().numpy())\n",
    "\n",
    "def sigmoid(value):\n",
    "    return 1 / (1 + mx.nd.exp(-value))\n",
    "\n",
    "def forwardpropagation_mx(x, A, B, C):\n",
    "    y = mx.nd.dot(A, x)\n",
    "    u = sigmoid(y)\n",
    "    v = mx.nd.dot(B, x)\n",
    "    z = u + v\n",
    "    w = mx.nd.dot(C, z)\n",
    "    L = mx.nd.norm(w) ** 2\n",
    "    return L\n",
    "\n",
    "def backwardpropagation_mx(L_mx):\n",
    "    L_mx.backward()\n",
    "    dA = A_mx.grad\n",
    "    dB = B_mx.grad\n",
    "    dC = C_mx.grad\n",
    "    return dA, dB, dC\n",
    "\n",
    "A_mx.attach_grad()\n",
    "B_mx.attach_grad()\n",
    "C_mx.attach_grad()\n",
    "\n",
    "with autograd.record():\n",
    "    L_mx = forwardpropagation_mx(x_mx, A_mx, B_mx, C_mx)\n",
    "\n",
    "dA_mx, dB_mx, dC_mx = backwardpropagation_mx(L_mx)\n",
    "\n",
    "print(f\"Gradients (mx) dL/dA:\\n{dA_mx}\")\n",
    "print(f\"Gradients (mx) dL/dB:\\n{dB_mx}\")\n",
    "print(f\"Gradients (mx) dL/dC:\\n{dC_mx}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
