{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "\n",
    "K = 3\n",
    "# Define the input data\n",
    "A = [[random.uniform(-1, 1) for _ in range(K)] for _ in range(K)]\n",
    "B = [[random.uniform(-1, 1) for _ in range(K)] for _ in range(K)]\n",
    "C = [[random.uniform(-1, 1) for _ in range(K)] for _ in range(K)]\n",
    "# Input vector x\n",
    "x = [random.uniform(-1, 1) for _ in range(K)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Manual Good: 0.6441052860636061\n",
      "Gradient dL/dA:\n",
      "[-0.9634435530157177, 0.3338766098092709, -0.49543344505552134]\n",
      "[0.5473762111111067, -0.18969052528709854, 0.2814783296471856]\n",
      "[-0.882387547274748, 0.30578705093810965, -0.4537518581675414]\n",
      "Gradient dL/dA:\n",
      "[0.004567639815161595, -0.001582893041883398, 0.002348826272504369]\n",
      "[-0.08610838055021142, 0.029840434433632178, -0.04427967937572271]\n",
      "[-1.8231132345920278, 0.6317908964761841, -0.9375030511263464]\n",
      "Loss Manual2: 0.6441052860636061\n",
      "Loss Automatic: 0.6441052556037903\n",
      "--- Manual Backpropagation Gradient ∂L/∂A: ----\n",
      "[0.004567639815161595, -0.001582893041883398, 0.002348826272504369]\n",
      "[-0.08610838055021142, 0.029840434433632178, -0.04427967937572271]\n",
      "[-1.8231132345920278, 0.6317908964761841, -0.9375030511263464]\n",
      "--- Automatic Differentiation with PyTorch Gradient ∂L/∂A: ---\n",
      "tensor([[-0.3001,  0.1040, -0.1543],\n",
      "        [ 0.0129, -0.0045,  0.0066],\n",
      "        [-0.0737,  0.0255, -0.0379]])\n",
      "--- Automatic Differentiation with PyTorch Gradient ∂L/∂A: ---\n",
      "[[-0.18799577837712306, 0.0651490094531619, -0.09667343337939474], [-0.16568836675538806, 0.057418486017126937, -0.08520224987784566], [-0.13198704280784393, 0.04573945848046494, -0.06787195276391443]]\n"
     ]
    }
   ],
   "source": [
    "# Define the sigmoid function\n",
    "def sigmoid(value):\n",
    "    return 1 / (1 + math.exp(-value))\n",
    "\n",
    "def sigmoid_derivative(value):\n",
    "    return sigmoid(value) * (1 - sigmoid(value))\n",
    "\n",
    "def matrix_vector_multiplication(matrix, vector):\n",
    "    if len(matrix) != len(matrix[0]) or len(matrix) != len(vector):\n",
    "        raise ValueError(\"Matrix and vector dimensions don't match for multiplication\")\n",
    "\n",
    "    result = [0] * len(vector)\n",
    "    for i in range(len(matrix)):\n",
    "        for j in range(len(vector)):\n",
    "            result[i] += matrix[i][j] * vector[j]\n",
    "\n",
    "    return result\n",
    "\n",
    "# Manual Backpropagation\n",
    "def manual_backpropagation(x, A, B, C):\n",
    "    K = len(x)\n",
    "    \n",
    "    # Initialze all other vectors to zero, of length K\n",
    "    y, u, v, w = [0] * K, [0] * K, [0] * K, [0] * K\n",
    "\n",
    "    # Forward propagation\n",
    "    y = matrix_vector_multiplication(A, x)\n",
    "    for i in range(K):\n",
    "        u[i] = sigmoid(y[i])\n",
    "    v = matrix_vector_multiplication(B, x)\n",
    "    z = [u + v for u, v in zip(u, v)]\n",
    "    w = matrix_vector_multiplication(C, z)\n",
    "\n",
    "    L = sum(val ** 2 for val in w)\n",
    "\n",
    "    print(f\"Loss Manual Good: {L}\")\n",
    "\n",
    "    #Backpropagation\n",
    "    #Compute gradients ∂L/∂A, ∂L/∂B, ∂L/∂C\n",
    "    dL_dw = [2 * val for val in w]\n",
    "    dL_dz = [dL_dw[i] for i in range(K)]\n",
    "    dL_dy = [dL_dz[i] for i in range(K)]\n",
    "    dL_dv = [dL_dy[i] * sigmoid_derivative(y[i]) for i in range(K)]\n",
    "\n",
    "    dL_dC = [[dL_dw[i] * z[j] for j in range(K)] for i in range(K)]\n",
    "    dL_dB = [[dL_dv[i] * x[j] for j in range(K)] for i in range(K)]\n",
    "    dL_dA = [[dL_dy[i] * x[j] for j in range(K)] for i in range(K)]\n",
    "\n",
    "    print(\"Gradient dL/dA:\")\n",
    "    for row in dL_dA:\n",
    "        print(row)\n",
    "\n",
    "    dL_dw = [2 * val for val in w]\n",
    "    dL_dC = [[dL_dw[i] * z[j] for j in range(K)] for i in range(K)]\n",
    "\n",
    "    dL_dz = [0] * K\n",
    "    for i in range(K):\n",
    "        dL_dz[i] = sum(dL_dw[j] * C[i][j] for j in range(K))\n",
    "\n",
    "    dL_dv = [0] * K\n",
    "    for i in range(K):\n",
    "        dL_dv[i] = dL_dz[i]\n",
    "\n",
    "    dL_dB = [[dL_dv[i] * x[j] for j in range(K)] for i in range(K)]\n",
    "\n",
    "    dL_dy = [0] * K\n",
    "    for i in range(K):\n",
    "        dL_dy[i] = sum(dL_dz[j] * B[i][j] for j in range(K))\n",
    "        dL_dy[i] *= y[i] * (1 - y[i])\n",
    "\n",
    "    dL_dA = [[dL_dy[i] * x[j] for j in range(K)] for i in range(K)]\n",
    "    print(\"Gradient dL/dA:\")\n",
    "    for row in dL_dA:\n",
    "        print(row)\n",
    "\n",
    "\n",
    "    return dL_dA, dL_dB, dL_dC\n",
    "\n",
    "def manual_backpropagation_rounded(x, A, B, C):\n",
    "    K = len(x)\n",
    "    \n",
    "    # Forward propagation\n",
    "    y = [0] * K\n",
    "    for i in range(K):\n",
    "        y[i] = sum(A[i][j] * x[j] for j in range(K))\n",
    "        y[i] = sigmoid(y[i])\n",
    "\n",
    "    v = [0] * K\n",
    "    for i in range(K):\n",
    "        v[i] = sum(B[i][j] * x[j] for j in range(K))\n",
    "\n",
    "    z = [u + v for u, v in zip(y, v)]\n",
    "\n",
    "    w = [0] * K\n",
    "    for i in range(K):\n",
    "        w[i] = sum(C[i][j] * z[j] for j in range(K))\n",
    "\n",
    "    for val in w:\n",
    "        L = sum(val ** 2 for val in w)\n",
    "\n",
    "    print(f\"Loss Manual2: {L}\")\n",
    "    \n",
    "    # Backpropagation ---- !!! Something wrong with this back propogation algorithm !!!\n",
    "    # Compute gradients ∂L/∂A, ∂L/∂B, ∂L/∂C\n",
    "    dL_dw = [round(2 * val, 4) for val in w]\n",
    "    dL_dC = [[round(dL_dw[i] * z[j], 4) for j in range(K)] for i in range(K)]\n",
    "\n",
    "    dL_dz = [0] * K\n",
    "    for i in range(K):\n",
    "        dL_dz[i] = sum(round(dL_dw[j] * C[i][j], 4) for j in range(K))\n",
    "\n",
    "    dL_dv = [0] * K\n",
    "    for i in range(K):\n",
    "        dL_dv[i] = dL_dz[i]\n",
    "\n",
    "    dL_dB = [[round(dL_dv[i] * x[j], 4) for j in range(K)] for i in range(K)]\n",
    "\n",
    "    dL_dy = [0] * K\n",
    "    for i in range(K):\n",
    "        dL_dy[i] = sum(round(dL_dz[j] * B[i][j], 4) for j in range(K))\n",
    "        dL_dy[i] *= round(y[i] * (1 - y[i]), 4)\n",
    "\n",
    "    dL_dA = [[round(dL_dy[i] * x[j], 4) for j in range(K)] for i in range(K)]\n",
    "    return dL_dA, dL_dB, dL_dC\n",
    "\n",
    "\n",
    "def backwardpropagation(x, A, B, C):\n",
    "    K = len(x)\n",
    "    \n",
    "    # Initialize gradients\n",
    "    dA, dB, dC = [[0] * K for _ in range(K)], [[0] * K for _ in range(K)], [[0] * K for _ in range(K)]\n",
    "\n",
    "    # Forward propagation\n",
    "    y = matrix_vector_multiplication(A, x)\n",
    "    u = [sigmoid(val) for val in y]\n",
    "    v = matrix_vector_multiplication(B, x)\n",
    "    z = [u_i + v_i for u_i, v_i in zip(u, v)]\n",
    "    w = matrix_vector_multiplication(C, z)\n",
    "\n",
    "    # Compute the loss L\n",
    "    L = sum(val ** 2 for val in w)\n",
    "\n",
    "    # Backpropagation\n",
    "    # Gradients of L w.r.t. w\n",
    "    dL_dw = [2 * val for val in w]\n",
    "\n",
    "    # Gradients of L w.r.t. C\n",
    "    for i in range(K):\n",
    "        for j in range(K):\n",
    "            dC[i][j] = dL_dw[i] * z[j]\n",
    "\n",
    "    # Gradients of L w.r.t. z\n",
    "    dL_dz = sum(dL_dw[i] * z[i] for i in range(K))\n",
    "\n",
    "    # Gradients of L w.r.t. u and v\n",
    "    dL_du = [dL_dz * sigmoid(y_i) * (1 - sigmoid(y_i)) for y_i in y]\n",
    "    dL_dv = [dL_dz] * K  # Same gradient for all elements of v\n",
    "\n",
    "    # Gradients of L w.r.t. A\n",
    "    for i in range(K):\n",
    "        for j in range(K):\n",
    "            dA[i][j] = dL_du[i] * x[j]\n",
    "\n",
    "    # Gradients of L w.r.t. B\n",
    "    for i in range(K):\n",
    "        for j in range(K):\n",
    "            dB[i][j] = dL_dv[i] * x[j]\n",
    "\n",
    "    return dA, dB, dC, L\n",
    "\n",
    "\n",
    "# Manual Backpropagation\n",
    "dL_dA_manual, dL_dB_manual, dL_dC_manual = manual_backpropagation(x, A, B, C)\n",
    "\n",
    "dL_dA_manual_round, dL_dB_manual_round, dL_dC_manual_round = manual_backpropagation_rounded(x, A, B, C)\n",
    "\n",
    "dL_dA_final, dL_dB_final, dL_dC_final, L = backwardpropagation(x, A, B, C)\n",
    "\n",
    "#Print the gradients\n",
    "print(\"--- Manual Backpropagation Gradient ∂L/∂A: ----\")\n",
    "for row in dL_dA_manual:\n",
    "    print(row)\n",
    "\n",
    "print(\"--- Manual Backpropagation Gradient Rounded ∂L/∂A: --- \")\n",
    "for row in dL_dA_manual_round:\n",
    "    print(row)\n",
    "\n",
    "print(\"--- Automatic Differentiation with PyTorch Gradient ∂L/∂A: ---\")\n",
    "print(dL_dA_final)\n",
    "\n",
    "# print(\"Manual Backpropagation Gradient ∂L/∂B:\")\n",
    "# for row in dL_dB_manual:\n",
    "#     print(row)\n",
    "\n",
    "# print(\"Automatic Differentiation with PyTorch Gradient ∂L/∂B:\")\n",
    "# print(dL_dB_torch)\n",
    "\n",
    "# print(\"Manual Backpropagation Gradient ∂L/∂C:\")\n",
    "# for row in dL_dC_manual:\n",
    "#     print(row)\n",
    "\n",
    "# print(\"Automatic Differentiation with PyTorch Gradient ∂L/∂C:\")\n",
    "# print(dL_dC_torch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verify the gradients using libraries - Torch, mxnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients (torch) dL/dA:\n",
      "tensor([[-0.3001,  0.1040, -0.1543],\n",
      "        [ 0.0129, -0.0045,  0.0066],\n",
      "        [-0.0737,  0.0255, -0.0379]])\n",
      "Gradients (torch) dL/dB:\n",
      "tensor([[-1.2072,  0.4183, -0.6208],\n",
      "        [ 0.0588, -0.0204,  0.0302],\n",
      "        [-0.4221,  0.1463, -0.2171]])\n",
      "Gradients (torch) dL/dC:\n",
      "tensor([[ 0.9729,  0.5914,  0.2404],\n",
      "        [-0.5527, -0.3360, -0.1366],\n",
      "        [ 0.8910,  0.5417,  0.2201]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "A_torch = torch.tensor(A, requires_grad=True, dtype=torch.float32)\n",
    "B_torch = torch.tensor(B, requires_grad=True, dtype=torch.float32)\n",
    "C_torch = torch.tensor(C, requires_grad=True, dtype=torch.float32)\n",
    "x_torch = torch.tensor(x, dtype=torch.float32)\n",
    "\n",
    "# Define the sigmoid function\n",
    "def sigmoid(value):\n",
    "    return 1 / (1 + torch.exp(-value))\n",
    "\n",
    "def forwardpropagation_torch(x, A, B, C):\n",
    "    y = torch.matmul(A, x.unsqueeze(1)).squeeze()\n",
    "    u = sigmoid(y)\n",
    "    v = torch.matmul(B, x.unsqueeze(1)).squeeze()\n",
    "    z = u + v\n",
    "    w = torch.matmul(C, z.unsqueeze(1)).squeeze()\n",
    "    L = torch.norm(w, p=2)**2\n",
    "    return L\n",
    "\n",
    "def backwardpropagation_torch(L_torch):\n",
    "    L_torch.backward()\n",
    "    dA = A_torch.grad\n",
    "    dB = B_torch.grad\n",
    "    dC = C_torch.grad\n",
    "    return dA, dB, dC\n",
    "\n",
    "L_torch = forwardpropagation_torch(x_torch, A_torch, B_torch, C_torch)\n",
    "dA_torch, dB_torch, dC_torch = backwardpropagation_torch(L_torch)\n",
    "\n",
    "\n",
    "print(f\"Gradients (torch) dL/dA:\\n{dA_torch}\")\n",
    "print(f\"Gradients (torch) dL/dB:\\n{dB_torch}\")\n",
    "print(f\"Gradients (torch) dL/dC:\\n{dC_torch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients (mx) dL/dA:\n",
      "\n",
      "[[-0.3000914   0.10399519 -0.15431656]\n",
      " [ 0.01288185 -0.00446414  0.00662426]\n",
      " [-0.07367561  0.02553192 -0.03788635]]\n",
      "<NDArray 3x3 @cpu(0)>\n",
      "Gradients (mx) dL/dB:\n",
      "\n",
      "[[-1.2071898   0.4183457  -0.6207756 ]\n",
      " [ 0.05879717 -0.02037587  0.03023538]\n",
      " [-0.42214587  0.14629258 -0.21708089]]\n",
      "<NDArray 3x3 @cpu(0)>\n",
      "Gradients (mx) dL/dC:\n",
      "\n",
      "[[ 0.97286165  0.5914468   0.24035469]\n",
      " [-0.55272716 -0.33602798 -0.13655648]\n",
      " [ 0.8910135   0.5416876   0.22013332]]\n",
      "<NDArray 3x3 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "import mxnet as mx\n",
    "from mxnet import autograd\n",
    "\n",
    "x_mx = mx.nd.array(x_torch.detach().numpy())\n",
    "A_mx = mx.nd.array(A_torch.detach().numpy())\n",
    "B_mx = mx.nd.array(B_torch.detach().numpy())\n",
    "C_mx = mx.nd.array(C_torch.detach().numpy())\n",
    "\n",
    "def sigmoid(value):\n",
    "    return 1 / (1 + mx.nd.exp(-value))\n",
    "\n",
    "def forwardpropagation_mx(x, A, B, C):\n",
    "    y = mx.nd.dot(A, x)\n",
    "    u = sigmoid(y)\n",
    "    v = mx.nd.dot(B, x)\n",
    "    z = u + v\n",
    "    w = mx.nd.dot(C, z)\n",
    "    L = mx.nd.norm(w) ** 2\n",
    "    return L\n",
    "\n",
    "def backwardpropagation_mx(L_mx):\n",
    "    L_mx.backward()\n",
    "    dA = A_mx.grad\n",
    "    dB = B_mx.grad\n",
    "    dC = C_mx.grad\n",
    "    return dA, dB, dC\n",
    "\n",
    "A_mx.attach_grad()\n",
    "B_mx.attach_grad()\n",
    "C_mx.attach_grad()\n",
    "\n",
    "with autograd.record():\n",
    "    L_mx = forwardpropagation_mx(x_mx, A_mx, B_mx, C_mx)\n",
    "\n",
    "dA_mx, dB_mx, dC_mx = backwardpropagation_mx(L_mx)\n",
    "\n",
    "print(f\"Gradients (mx) dL/dA:\\n{dA_mx}\")\n",
    "print(f\"Gradients (mx) dL/dB:\\n{dB_mx}\")\n",
    "print(f\"Gradients (mx) dL/dC:\\n{dC_mx}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
