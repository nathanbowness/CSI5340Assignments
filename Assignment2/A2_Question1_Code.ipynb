{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Manual2: 6.39822721545684\n",
      "Loss Manual2: 6.39822721545684\n",
      "Loss Manual2: 6.398228168487549\n",
      "Loss Automatic: 6.398228168487549\n",
      "--- Manual Backpropagation Gradient ∂L/∂A: ----\n",
      "[0.18968844463637405, 0.7065160304517885, 0.7981986829563948]\n",
      "[-0.07376755578300863, -0.27475559087349993, -0.31040987227125494]\n",
      "[-0.16144176584057704, -0.6013080858428822, -0.6793381911857007]\n",
      "--- Manual Backpropagation Gradient 2 ∂L/∂A: ---\n",
      "tensor([0.2211, 0.6091, 1.3381], grad_fn=<UnbindBackward0>)\n",
      "tensor([0.4722, 1.3009, 2.8577], grad_fn=<UnbindBackward0>)\n",
      "tensor([0.0839, 0.2312, 0.5079], grad_fn=<UnbindBackward0>)\n",
      "--- Manual Backpropagation Gradient Rounded ∂L/∂A: --- \n",
      "[0.1897, 0.7067, 0.7984]\n",
      "[-0.0738, -0.2747, -0.3104]\n",
      "[-0.1615, -0.6014, -0.6794]\n",
      "--- Automatic Differentiation with PyTorch Gradient ∂L/∂A: ---\n",
      "tensor([[0.1668, 0.6211, 0.7017],\n",
      "        [0.3561, 1.3264, 1.4986],\n",
      "        [0.0633, 0.2358, 0.2663]])\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import math\n",
    "import torch\n",
    "\n",
    "# Define the sigmoid function\n",
    "def sigmoid(value):\n",
    "    return 1 / (1 + math.exp(-value))\n",
    "\n",
    "# Manual Backpropagation\n",
    "def manual_backpropagation(x, A, B, C):\n",
    "    K = len(x)\n",
    "    \n",
    "    # Forward propagation\n",
    "    y = [0] * K\n",
    "    for i in range(K):\n",
    "        y[i] = sum(A[i][j] * x[j] for j in range(K))\n",
    "        y[i] = sigmoid(y[i])\n",
    "\n",
    "    v = [0] * K\n",
    "    for i in range(K):\n",
    "        v[i] = sum(B[i][j] * x[j] for j in range(K))\n",
    "\n",
    "    z = [u + v for u, v in zip(y, v)]\n",
    "\n",
    "    w = [0] * K\n",
    "    for i in range(K):\n",
    "        w[i] = sum(C[i][j] * z[j] for j in range(K))\n",
    "\n",
    "    for val in w:\n",
    "        L = sum(val ** 2 for val in w)\n",
    "\n",
    "    print(f\"Loss Manual2: {L}\")\n",
    "    \n",
    "    # Backpropagation\n",
    "    # Compute gradients ∂L/∂A, ∂L/∂B, ∂L/∂C\n",
    "    dL_dw = [2 * val for val in w]\n",
    "    dL_dC = [[dL_dw[i] * z[j] for j in range(K)] for i in range(K)]\n",
    "\n",
    "    dL_dz = [0] * K\n",
    "    for i in range(K):\n",
    "        dL_dz[i] = sum(dL_dw[j] * C[i][j] for j in range(K))\n",
    "\n",
    "    dL_dv = [0] * K\n",
    "    for i in range(K):\n",
    "        dL_dv[i] = dL_dz[i]\n",
    "\n",
    "    dL_dB = [[dL_dv[i] * x[j] for j in range(K)] for i in range(K)]\n",
    "\n",
    "    dL_dy = [0] * K\n",
    "    for i in range(K):\n",
    "        dL_dy[i] = sum(dL_dz[j] * B[i][j] for j in range(K))\n",
    "        dL_dy[i] *= y[i] * (1 - y[i])\n",
    "\n",
    "    dL_dA = [[dL_dy[i] * x[j] for j in range(K)] for i in range(K)]\n",
    "    return dL_dA, dL_dB, dL_dC\n",
    "\n",
    "def manual_backpropagation_rounded(x, A, B, C):\n",
    "    K = len(x)\n",
    "    \n",
    "    # Forward propagation\n",
    "    y = [0] * K\n",
    "    for i in range(K):\n",
    "        y[i] = sum(A[i][j] * x[j] for j in range(K))\n",
    "        y[i] = sigmoid(y[i])\n",
    "\n",
    "    v = [0] * K\n",
    "    for i in range(K):\n",
    "        v[i] = sum(B[i][j] * x[j] for j in range(K))\n",
    "\n",
    "    z = [u + v for u, v in zip(y, v)]\n",
    "\n",
    "    w = [0] * K\n",
    "    for i in range(K):\n",
    "        w[i] = sum(C[i][j] * z[j] for j in range(K))\n",
    "\n",
    "    for val in w:\n",
    "        L = sum(val ** 2 for val in w)\n",
    "\n",
    "    print(f\"Loss Manual2: {L}\")\n",
    "    \n",
    "    # Backpropagation ---- !!! Something wrong with this back propogation algorithm !!!\n",
    "    # Compute gradients ∂L/∂A, ∂L/∂B, ∂L/∂C\n",
    "    dL_dw = [round(2 * val, 4) for val in w]\n",
    "    dL_dC = [[round(dL_dw[i] * z[j], 4) for j in range(K)] for i in range(K)]\n",
    "\n",
    "    dL_dz = [0] * K\n",
    "    for i in range(K):\n",
    "        dL_dz[i] = sum(round(dL_dw[j] * C[i][j], 4) for j in range(K))\n",
    "\n",
    "    dL_dv = [0] * K\n",
    "    for i in range(K):\n",
    "        dL_dv[i] = dL_dz[i]\n",
    "\n",
    "    dL_dB = [[round(dL_dv[i] * x[j], 4) for j in range(K)] for i in range(K)]\n",
    "\n",
    "    dL_dy = [0] * K\n",
    "    for i in range(K):\n",
    "        dL_dy[i] = sum(round(dL_dz[j] * B[i][j], 4) for j in range(K))\n",
    "        dL_dy[i] *= round(y[i] * (1 - y[i]), 4)\n",
    "\n",
    "    dL_dA = [[round(dL_dy[i] * x[j], 4) for j in range(K)] for i in range(K)]\n",
    "    return dL_dA, dL_dB, dL_dC\n",
    "\n",
    "\n",
    "def manual_backpropagation_2(x, A, B, C):\n",
    "    K = len(x)\n",
    "\n",
    "    x_tensor = torch.tensor(x, requires_grad=True, dtype=torch.float)\n",
    "    # Convert A, B, and C to PyTorch tensors\n",
    "    A_tensor = torch.tensor(A, requires_grad=True, dtype=torch.float)\n",
    "    B_tensor = torch.tensor(B, requires_grad=True, dtype=torch.float)\n",
    "    C_tensor = torch.tensor(C, requires_grad=True, dtype=torch.float)\n",
    "    \n",
    "    # Forward propagation\n",
    "    y = torch.sigmoid(torch.matmul(A_tensor, x_tensor))\n",
    "    v = torch.matmul(B_tensor, x_tensor)\n",
    "    z = y + v\n",
    "    w = torch.matmul(C_tensor, z)\n",
    "    L = torch.norm(w)**2\n",
    "\n",
    "    print(f\"Loss Manual2: {L}\")\n",
    "\n",
    "    # Compute gradients manually\n",
    "    dL_dA = torch.zeros_like(A_tensor)\n",
    "    dL_dB = torch.zeros_like(B_tensor)\n",
    "    dL_dC = torch.zeros_like(C_tensor)\n",
    "\n",
    "    for i in range(K):\n",
    "        for j in range(K):\n",
    "            for k in range(K):\n",
    "                dL_dA[i, j] += 2 * w[k] * C_tensor[k, i] * z[j] * y[i] * (1 - y[i]) * x_tensor[j]\n",
    "                dL_dB[i, j] += 2 * w[i] * C_tensor[i, k] * z[k] * x_tensor[j]\n",
    "                dL_dC[i, j] += 2 * w[i] * z[i] * y[j] * (1 - y[j])\n",
    "\n",
    "    return dL_dA, dL_dB, dL_dC\n",
    "\n",
    "# Automatic Differentiation with PyTorch\n",
    "def autograd_torch_backpropagation(x, A, B, C):\n",
    "    # Set a random seed for reproducibility\n",
    "    random.seed(0)\n",
    "    torch.manual_seed(0)\n",
    "    \n",
    "    K = len(x)\n",
    "\n",
    "    x_tensor = torch.tensor(x, requires_grad=True, dtype=torch.float)\n",
    "    # Convert A, B, and C to PyTorch tensors\n",
    "    A_tensor = torch.tensor(A, requires_grad=True, dtype=torch.float)\n",
    "    B_tensor = torch.tensor(B, requires_grad=True, dtype=torch.float)\n",
    "    C_tensor = torch.tensor(C, requires_grad=True, dtype=torch.float)\n",
    "\n",
    "    # Forward propagation\n",
    "    y = torch.sigmoid(torch.matmul(A_tensor, x_tensor))\n",
    "    v = torch.matmul(B_tensor, x_tensor)\n",
    "    z = y + v\n",
    "    w = torch.matmul(C_tensor, z)\n",
    "    L = torch.norm(w)**2\n",
    "\n",
    "    print(f\"Loss Automatic: {L}\")\n",
    "\n",
    "    # Backpropagation\n",
    "    L.backward()  # Automatically computes gradients\n",
    "\n",
    "    # Access the gradients\n",
    "    dL_dA = A_tensor.grad\n",
    "    dL_dB = B_tensor.grad\n",
    "    dL_dC = C_tensor.grad\n",
    "\n",
    "    return dL_dA, dL_dB, dL_dC\n",
    "\n",
    "# Define the input data\n",
    "K = 3\n",
    "A = [[random.uniform(-1, 1) for _ in range(K)] for _ in range(K)]\n",
    "B = [[random.uniform(-1, 1) for _ in range(K)] for _ in range(K)]\n",
    "C = [[random.uniform(-1, 1) for _ in range(K)] for _ in range(K)]\n",
    "\n",
    "# Input vector x\n",
    "x = [random.uniform(-1, 1) for _ in range(K)]\n",
    "\n",
    "# Manual Backpropagation\n",
    "dL_dA_manual, dL_dB_manual, dL_dC_manual = manual_backpropagation(x, A, B, C)\n",
    "\n",
    "dL_dA_manual_round, dL_dB_manual_round, dL_dC_manual_round = manual_backpropagation_rounded(x, A, B, C)\n",
    "\n",
    "dL_dA_manual_2, dL_dB_manual_2, dL_dC_manual_2 = manual_backpropagation_2(x, A, B, C)\n",
    "\n",
    "# Automatic Differentiation with PyTorch\n",
    "dL_dA_torch, dL_dB_torch, dL_dC_torch = autograd_torch_backpropagation(x, A, B, C)\n",
    "\n",
    "# Print the gradients\n",
    "print(\"--- Manual Backpropagation Gradient ∂L/∂A: ----\")\n",
    "for row in dL_dA_manual:\n",
    "    print(row)\n",
    "\n",
    "print(\"--- Manual Backpropagation Gradient 2 ∂L/∂A: ---\")\n",
    "for row in dL_dA_manual_2:\n",
    "    print(row)\n",
    "\n",
    "print(\"--- Manual Backpropagation Gradient Rounded ∂L/∂A: --- \")\n",
    "for row in dL_dA_manual_round:\n",
    "    print(row)\n",
    "\n",
    "print(\"--- Automatic Differentiation with PyTorch Gradient ∂L/∂A: ---\")\n",
    "print(dL_dA_torch)\n",
    "\n",
    "# print(\"Manual Backpropagation Gradient ∂L/∂B:\")\n",
    "# for row in dL_dB_manual:\n",
    "#     print(row)\n",
    "\n",
    "# print(\"Automatic Differentiation with PyTorch Gradient ∂L/∂B:\")\n",
    "# print(dL_dB_torch)\n",
    "\n",
    "# print(\"Manual Backpropagation Gradient ∂L/∂C:\")\n",
    "# for row in dL_dC_manual:\n",
    "#     print(row)\n",
    "\n",
    "# print(\"Automatic Differentiation with PyTorch Gradient ∂L/∂C:\")\n",
    "# print(dL_dC_torch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
