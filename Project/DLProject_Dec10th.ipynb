{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vRjDB0seyHnj",
        "outputId": "8f3b1198-34d3-4df9-de0a-3367d471f858"
      },
      "outputs": [],
      "source": [
        "import torch \n",
        "import torchvision \n",
        "import plotly\n",
        "import logging\n",
        "import json\n",
        "import os\n",
        "import timeit\n",
        "import numpy as np\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, Subset, TensorDataset\n",
        "import random\n",
        "import concurrent.futures\n",
        "from functools import partial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "BATCH_SIZE=64\n",
        "\n",
        "# Make it repeatable, set seeds\n",
        "random.seed(0)\n",
        "torch.manual_seed(0)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "# load MNIST dataset\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "train_dataset, validate_dataset = torch.utils.data.random_split(train_dataset, [50000, 10000])\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "validate_loader = DataLoader(dataset=validate_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# # Create a random subset of the MNIST dataset and pin the memory for faster transfer to GPU\n",
        "subset_size = 500\n",
        "subset_indices = torch.randperm(len(train_dataset))[:subset_size]\n",
        "subset_dataset = Subset(train_dataset, subset_indices)\n",
        "subset_data_loader = DataLoader(subset_dataset, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "VPPueoaSFWmr"
      },
      "outputs": [],
      "source": [
        "def train(model, data_loader, optimizer, criterion, mask = None):\n",
        "    results = []\n",
        "    model.train()\n",
        "    for data in data_loader:\n",
        "        inputs, labels = data\n",
        "        optimizer.zero_grad() # zero the parameter gradients\n",
        "        predictions = model(inputs) # forward pass\n",
        "        loss = criterion(predictions, labels) # calculate loss\n",
        "        loss.backward() # backward pass\n",
        "        optimizer.step() # update parameters\n",
        "\n",
        "    return model\n",
        "\n",
        "def evaluate(model, data_loader, mask = None):\n",
        "      model.eval()\n",
        "      with torch.inference_mode():\n",
        "          correct = 0\n",
        "          total = 0\n",
        "          for inputs, labels in data_loader:\n",
        "              inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
        "              outputs = model(inputs, mask)\n",
        "              _, predicted = torch.max(outputs.data, 1)\n",
        "              total += labels.size(0)\n",
        "              correct += (predicted == labels).sum().item()\n",
        "\n",
        "      return correct / total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "KXDeZNPSzGpP"
      },
      "outputs": [],
      "source": [
        "DEBUG = bool(os.environ.get('DEBUG', 'True') == 'True')\n",
        "log_level = os.environ.get('LOG_LEVEL', 'DEBUG' if DEBUG else 'INFO')\n",
        "logging.basicConfig(level=logging.getLevelName(log_level), format=\"%(message)s\", force=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Crossover methods\n",
        "def basic_crossover(couple):\n",
        "    return couple[0] * couple[1]\n",
        "\n",
        "def sbx_crossover(couple, probability=0.9, eta=1):\n",
        "    # Initialize the offspring tensors\n",
        "    offspring1_tensor = torch.zeros_like(couple[0], device=DEVICE)\n",
        "    offspring2_tensor = torch.zeros_like(couple[1], device=DEVICE)\n",
        "\n",
        "    for i in range(couple[0].numel()):\n",
        "        if torch.rand(1).item() <= probability:\n",
        "            u = torch.rand(1).item()\n",
        "\n",
        "            if u <= 0.5:\n",
        "                beta = (2 * u) ** (1 / (eta + 1))\n",
        "            else:\n",
        "                beta = (1 / (2 * (1 - u))) ** (1 / (eta + 1))\n",
        "\n",
        "            offspring1_tensor.view(-1)[i] = 0.5 * (((1 + beta) * couple[0].view(-1)[i]) + ((1 - beta) * couple[1].view(-1)[i]))\n",
        "            offspring2_tensor.view(-1)[i] = 0.5 * (((1 - beta) * couple[0].view(-1)[i]) + ((1 + beta) * couple[1].view(-1)[i]))\n",
        "        else:\n",
        "            offspring1_tensor.view(-1)[i] = couple[0].view(-1)[i]\n",
        "            offspring2_tensor.view(-1)[i] = couple[1].view(-1)[i]\n",
        "    return offspring1_tensor, offspring2_tensor\n",
        "\n",
        "def uniform_crossover(couple, probability=0.5):\n",
        "    # Initialize the offspring tensors\n",
        "    offspring1_tensor = torch.zeros_like(couple[0], device=DEVICE)\n",
        "    offspring2_tensor = torch.zeros_like(couple[1], device=DEVICE)\n",
        "\n",
        "    for i in range(couple[0].numel()):\n",
        "        if torch.rand(1).item() <= probability:\n",
        "            offspring1_tensor.view(-1)[i] = couple[1].view(-1)[i]\n",
        "            offspring2_tensor.view(-1)[i] = couple[0].view(-1)[i]\n",
        "        else:\n",
        "            offspring1_tensor.view(-1)[i] = couple[0].view(-1)[i]\n",
        "            offspring2_tensor.view(-1)[i] = couple[1].view(-1)[i]\n",
        "    return offspring1_tensor, offspring2_tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "id": "eWOFMw5uIALJ",
        "outputId": "0b7ac7d2-2c6e-41a4-f76f-faa26e1d26d0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Masks Shape: torch.Size([14, 2, 500])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Accuracies: [0.074, 0.108, 0.064, 0.104, 0.076, 0.086, 0.106, 0.072, 0.098, 0.072, 0.088, 0.092, 0.094, 0.068]\n",
            "Accuracies: [0.098, 0.104, 0.108, 0.106, 0.06, 0.074, 0.096, 0.12, 0.124, 0.106, 0.106, 0.11, 0.072, 0.048]\n",
            "Accuracies: [0.108, 0.11, 0.12, 0.124, 0.124, 0.118, 0.1, 0.09, 0.088, 0.1, 0.132, 0.09, 0.088, 0.102]\n",
            "Accuracies: [0.12, 0.124, 0.124, 0.132, 0.072, 0.09, 0.14, 0.098, 0.11, 0.084, 0.1, 0.034, 0.074, 0.072]\n",
            "Accuracies: [0.124, 0.124, 0.132, 0.14, 0.118, 0.078, 0.112, 0.142, 0.072, 0.064, 0.066, 0.104, 0.084, 0.09]\n",
            "Accuracies: [0.124, 0.132, 0.14, 0.142, 0.062, 0.084, 0.104, 0.144, 0.116, 0.122, 0.13, 0.07, 0.082, 0.122]\n",
            "Accuracies: [0.132, 0.14, 0.142, 0.144, 0.092, 0.118, 0.154, 0.122, 0.116, 0.106, 0.076, 0.128, 0.084, 0.08]\n",
            "Accuracies: [0.14, 0.142, 0.144, 0.154, 0.096, 0.14, 0.13, 0.136, 0.122, 0.052, 0.092, 0.084, 0.054, 0.106]\n",
            "Accuracies: [0.14, 0.142, 0.144, 0.154, 0.132, 0.144, 0.144, 0.13, 0.108, 0.118, 0.088, 0.074, 0.106, 0.078]\n",
            "Accuracies: [0.144, 0.144, 0.144, 0.154, 0.138, 0.132, 0.142, 0.142, 0.082, 0.09, 0.056, 0.076, 0.07, 0.108]\n",
            "Accuracies: [0.144, 0.144, 0.144, 0.154, 0.158, 0.12, 0.14, 0.11, 0.1, 0.074, 0.082, 0.114, 0.104, 0.098]\n",
            "Accuracies: [0.144, 0.144, 0.158, 0.154, 0.118, 0.148, 0.144, 0.118, 0.06, 0.096, 0.088, 0.058, 0.08, 0.08]\n",
            "Accuracies: [0.144, 0.148, 0.158, 0.154, 0.124, 0.12, 0.152, 0.134, 0.078, 0.086, 0.054, 0.092, 0.082, 0.066]\n",
            "Accuracies: [0.148, 0.152, 0.158, 0.154, 0.148, 0.156, 0.164, 0.15, 0.084, 0.102, 0.086, 0.116, 0.098, 0.154]\n",
            "Accuracies: [0.154, 0.156, 0.158, 0.164, 0.124, 0.166, 0.15, 0.142, 0.066, 0.088, 0.07, 0.088, 0.068, 0.09]\n",
            "Accuracies: [0.156, 0.166, 0.158, 0.164, 0.146, 0.128, 0.152, 0.14, 0.11, 0.084, 0.076, 0.102, 0.106, 0.112]\n",
            "Accuracies: [0.156, 0.158, 0.164, 0.166, 0.14, 0.17, 0.13, 0.162, 0.084, 0.092, 0.104, 0.098, 0.064, 0.118]\n",
            "Accuracies: [0.162, 0.164, 0.166, 0.17, 0.134, 0.114, 0.126, 0.116, 0.09, 0.112, 0.1, 0.126, 0.116, 0.086]\n",
            "Accuracies: [0.162, 0.164, 0.166, 0.17, 0.164, 0.146, 0.146, 0.14, 0.064, 0.1, 0.064, 0.082, 0.106, 0.112]\n",
            "Accuracies: [0.164, 0.17, 0.166, 0.164, 0.146, 0.136, 0.116, 0.14, 0.068, 0.1, 0.13, 0.068, 0.072, 0.1]\n",
            "Accuracies: [0.164, 0.164, 0.166, 0.17, 0.114, 0.162, 0.136, 0.112, 0.066, 0.126, 0.08, 0.1, 0.114, 0.074]\n",
            "Accuracies: [0.164, 0.164, 0.166, 0.17, 0.138, 0.156, 0.146, 0.128, 0.07, 0.072, 0.08, 0.088, 0.096, 0.108]\n",
            "Accuracies: [0.164, 0.164, 0.166, 0.17, 0.094, 0.128, 0.142, 0.126, 0.086, 0.102, 0.074, 0.048, 0.078, 0.088]\n",
            "Accuracies: [0.164, 0.164, 0.166, 0.17, 0.124, 0.126, 0.138, 0.126, 0.098, 0.102, 0.1, 0.094, 0.11, 0.106]\n",
            "Accuracies: [0.164, 0.164, 0.166, 0.17, 0.112, 0.166, 0.146, 0.142, 0.08, 0.078, 0.102, 0.122, 0.098, 0.11]\n",
            "Accuracies: [0.164, 0.166, 0.166, 0.17, 0.176, 0.144, 0.164, 0.156, 0.116, 0.074, 0.104, 0.1, 0.066, 0.074]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[14], line 181\u001b[0m\n\u001b[0;32m    177\u001b[0m     train_masks(mask_model, subset_data_loader, KEEP_BEST, num_hidden_layers, hidden_size, masks, cx, mt, ms)\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 181\u001b[0m     \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mSHOW_PLOTS\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[1;32mIn[14], line 177\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(SHOW_PLOTS)\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;66;03m# Convert mask of floats to a binary mask\u001b[39;00m\n\u001b[0;32m    175\u001b[0m ms \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m a: convert_to_binary_mask(a, NUM_ELEMENTS_TO_KEEP)\n\u001b[1;32m--> 177\u001b[0m \u001b[43mtrain_masks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubset_data_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mKEEP_BEST\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_hidden_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmasks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mms\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[1;32mIn[14], line 106\u001b[0m, in \u001b[0;36mtrain_masks\u001b[1;34m(mask_model, train_loader, keep_best, depth, width, masks, cx, mt, ms)\u001b[0m\n\u001b[0;32m    104\u001b[0m children \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m couple \u001b[38;5;129;01min\u001b[39;00m couples:\n\u001b[1;32m--> 106\u001b[0m     off1, off2 \u001b[38;5;241m=\u001b[39m \u001b[43mcx\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcouple\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# generate 2 children\u001b[39;00m\n\u001b[0;32m    107\u001b[0m     children\u001b[38;5;241m.\u001b[39mappend(off1)\n\u001b[0;32m    108\u001b[0m     children\u001b[38;5;241m.\u001b[39mappend(off2)\n",
            "Cell \u001b[1;32mIn[14], line 169\u001b[0m, in \u001b[0;36mrun.<locals>.<lambda>\u001b[1;34m(a)\u001b[0m\n\u001b[0;32m    165\u001b[0m logging\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMasks Shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmasks\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    167\u001b[0m \u001b[38;5;66;03m# TODO: replace these with whatever functions you want\u001b[39;00m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;66;03m# takes a couple (tensor of size 2) and generates a child (singular tensor)\u001b[39;00m\n\u001b[1;32m--> 169\u001b[0m cx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m a : \u001b[43msbx_crossover\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;66;03m# Takes a mask and mutates it\u001b[39;00m\n\u001b[0;32m    172\u001b[0m mt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m a : basic_mutatation(a)\n",
            "Cell \u001b[1;32mIn[13], line 20\u001b[0m, in \u001b[0;36msbx_crossover\u001b[1;34m(couple, probability, eta)\u001b[0m\n\u001b[0;32m     17\u001b[0m         beta \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m u))) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m (eta \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m     19\u001b[0m     offspring1_tensor\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m (((\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m beta) \u001b[38;5;241m*\u001b[39m couple[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)[i]) \u001b[38;5;241m+\u001b[39m ((\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta) \u001b[38;5;241m*\u001b[39m couple[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)[i]))\n\u001b[1;32m---> 20\u001b[0m     offspring2_tensor\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m (((\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta) \u001b[38;5;241m*\u001b[39m couple[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)[i]) \u001b[38;5;241m+\u001b[39m ((\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m beta) \u001b[38;5;241m*\u001b[39m couple[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)[i]))\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     22\u001b[0m     offspring1_tensor\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)[i] \u001b[38;5;241m=\u001b[39m couple[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)[i]\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "## HYPERPARAMETERS\n",
        "# number of masks\n",
        "# number of top parents to keep\n",
        "# the crossover operation\n",
        "# the mutation operation\n",
        "# the masking operation\n",
        "# number of epochs\n",
        "# length should be 2x the trained version, and width polynomial in width to get SLT\n",
        "\n",
        "EPOCHS=5000\n",
        "LEARNING_RATE=0.001\n",
        "TRAIN=False\n",
        "NUM_MASKS=14\n",
        "assert NUM_MASKS % 2 == 0 # must be able to create groups of 2 parents\n",
        "KEEP_BEST=4\n",
        "assert KEEP_BEST % 2 == 0 # must be able to create groups of 2 parents from the best masks\n",
        "NUM_ELEMENTS_TO_KEEP = 400\n",
        "MAX_THREADS = 2\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_size, num_hidden_layers, hidden_size, output_size):\n",
        "        super(MLP, self).__init__()\n",
        "        assert num_hidden_layers > 0\n",
        "        self.input = nn.Linear(input_size, hidden_size)\n",
        "        self.layers = nn.ModuleList()\n",
        "        for i in range(num_hidden_layers):\n",
        "            self.layers.append(nn.Linear(hidden_size, hidden_size))\n",
        "        self.output = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x, mask = None):\n",
        "        \"\"\"\n",
        "        This allows for both a trainable MLP model and a non-training mask based model\n",
        "        \"\"\"\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.input(x)\n",
        "\n",
        "        if (mask != None):\n",
        "            for i, layer in enumerate(self.layers):\n",
        "                x = layer(x) * mask[i]\n",
        "                x = torch.relu(x)\n",
        "        else:\n",
        "            for layer in self.layers:\n",
        "                x = layer(x)\n",
        "                x = torch.relu(x)\n",
        "\n",
        "        x = self.output(x)\n",
        "        return torch.softmax(x, dim=1)\n",
        "\n",
        "\n",
        "def random_masks(num, depth, width):\n",
        "    if num <= 0:\n",
        "        return None\n",
        "    return torch.rand(num, depth, width, device=DEVICE)\n",
        "\n",
        "def convert_to_binary_mask(float_mask_2d: torch.tensor, num_elements_to_keep: int = NUM_ELEMENTS_TO_KEEP):\n",
        "    _, top_indices_2d = float_mask_2d.topk(num_elements_to_keep, dim=1)\n",
        "    # Create new mask for of 0's in the same shape as the float mask\n",
        "    binary_mask_2d = torch.zeros_like(float_mask_2d)\n",
        "    # Place 1's at the position of the top k elements in each layer\n",
        "    binary_mask_2d.scatter_(1, top_indices_2d, 1)\n",
        "    return binary_mask_2d\n",
        "\n",
        "def basic_mutatation(mask):\n",
        "    # Create a randomly initialized tensor with values between 0.1 and 0\n",
        "    random_tensor = 0.15 * torch.rand_like(mask)\n",
        "    # If the random value is less than 0.5, subtract the random tensor from the mask, otherwise add it\n",
        "    if random.random() < 0.5:\n",
        "        return mask - random_tensor\n",
        "    return mask + random_tensor\n",
        "\n",
        "def eval_accuracy_of_mask(index_mask_tuple, train_loader, mask_model, ms):\n",
        "    index, mask = index_mask_tuple\n",
        "    binary_mask = ms(mask) # mask the model\n",
        "    accuracy = evaluate(mask_model, train_loader, binary_mask)\n",
        "    return index, accuracy\n",
        "\n",
        "def train_masks(mask_model, train_loader, keep_best, depth, width, masks, cx, mt, ms):\n",
        "\n",
        "    if (keep_best + keep_best * 2 > NUM_MASKS):\n",
        "        raise ValueError(\"Too many masks specified to keep\")\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        accuracies = [None] * len(masks)\n",
        "\n",
        "        # Evaluate the current generation of masks in seperate threads\n",
        "        indexed_masks = list(enumerate(masks))\n",
        "        # Use functools.partial to create a new function with frozen extra arguments, in this case the train_loader, mask_model, and ms\n",
        "        partial_process_mask = partial(eval_accuracy_of_mask, train_loader=train_loader, mask_model=mask_model, ms=ms)\n",
        "        with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_THREADS) as executor:\n",
        "            # Use executor.map to process the (index, mask) tuples concurrently\n",
        "            results = list(executor.map(partial_process_mask, indexed_masks))\n",
        "        \n",
        "        for index, accuracy in results:\n",
        "            accuracies[index] = accuracy \n",
        "\n",
        "        logging.info(f\"Accuracies: {accuracies}\")\n",
        "\n",
        "        # Find the indicies of the best masks that will survive the generation\n",
        "        best_mask_indexes = np.argpartition(accuracies, -keep_best)[-keep_best:]\n",
        "        best_masks = masks[best_mask_indexes]\n",
        "\n",
        "        # Crossover\n",
        "        couples = best_masks.chunk(int(keep_best / 2)) # chunk into groups of 2 parents\n",
        "        children = []\n",
        "        for couple in couples:\n",
        "            off1, off2 = cx(couple) # generate 2 children\n",
        "            children.append(off1)\n",
        "            children.append(off2)\n",
        "        #logging.debug(f\"Children: {children}\")\n",
        "\n",
        "        # Mutation\n",
        "        new_masks = [mt(child) for child in children] # mutate the children\n",
        "        #logging.debug(f\"New masks: {new_masks}\")\n",
        "        \n",
        "        # Keep the 'keep-best' number of original masks, add the new generation of masks, and fill the rest with random masks if needed\n",
        "        filler_masks = random_masks((NUM_MASKS - len(new_masks) - keep_best), depth, width)\n",
        "        masks = torch.cat((best_masks, torch.stack(new_masks, dim=0), filler_masks), dim=0)    \n",
        "        \n",
        "\n",
        "def run(SHOW_PLOTS):\n",
        "    num_hidden_layers = 2\n",
        "    hidden_size = 500\n",
        "\n",
        "    # TODO: the total number of neurons in the target network (train_model), should\n",
        "    # be equal to the number of unmasked neurons in the masked model\n",
        "    # e.g. 1*2 = 2 hidden neurons in target network, 3*8*sparsity == 2 in mask network\n",
        "    # with 2l and polynomial width\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    if TRAIN:\n",
        "        logging.info('Training...')\n",
        "        train_accuracies = []\n",
        "        start_time = timeit.default_timer()\n",
        "        train_model = MLP(input_size=784, num_hidden_layers=1, hidden_size=2, output_size=10)\n",
        "        train_model.to(DEVICE)\n",
        "\n",
        "        optimizer = torch.optim.Adam(train_model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "        for epoch in range(EPOCHS):\n",
        "            model = train(train_model, train_loader, optimizer, criterion)\n",
        "            accuracy = evaluate(model, validate_loader)\n",
        "            train_accuracies.append(accuracy)\n",
        "            print(f'Epoch {epoch + 1}/{EPOCHS}, Accuracy: {accuracy}')\n",
        "\n",
        "        end_time = timeit.default_timer()\n",
        "        print(f'Total training time: {end_time - start_time}')\n",
        "\n",
        "        if SHOW_PLOTS:\n",
        "            fig = px.line(x=range(1, EPOCHS + 1), y=train_accuracies)\n",
        "            fig.show()\n",
        "\n",
        "        logging.info('Testing...')\n",
        "        test_accuracy = evaluate(train_model, test_loader)\n",
        "        print(f'Test Accuracy: {test_accuracy}')\n",
        "\n",
        "        # save model in case we want to use it again, and accuracy for a stop condition\n",
        "        torch.save(train_model.state_dict(), './train_model.pt')\n",
        "        with open('accuracy.json', 'w', encoding='utf-8') as f:\n",
        "            json.dump(test_accuracy)\n",
        "\n",
        "    mask_model = MLP(input_size=784, num_hidden_layers=num_hidden_layers, hidden_size=hidden_size, output_size=10)\n",
        "    mask_model.to(DEVICE)\n",
        "\n",
        "    masks = random_masks(NUM_MASKS, num_hidden_layers, hidden_size)\n",
        "    logging.debug(f\"Masks Shape: {masks.shape}\")\n",
        "\n",
        "    # TODO: replace these with whatever functions you want\n",
        "    # takes a couple (tensor of size 2) and generates a child (singular tensor)\n",
        "    cx = lambda a : sbx_crossover(a)\n",
        "\n",
        "    # Takes a mask and mutates it\n",
        "    mt = lambda a : basic_mutatation(a)\n",
        "\n",
        "    # Convert mask of floats to a binary mask\n",
        "    ms = lambda a: convert_to_binary_mask(a, NUM_ELEMENTS_TO_KEEP)\n",
        "\n",
        "    train_masks(mask_model, subset_data_loader, KEEP_BEST, num_hidden_layers, hidden_size, masks, cx, mt, ms)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run(SHOW_PLOTS=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
