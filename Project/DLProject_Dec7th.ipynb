{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vRjDB0seyHnj",
        "outputId": "8f3b1198-34d3-4df9-de0a-3367d471f858"
      },
      "outputs": [],
      "source": [
        "import torch \n",
        "import torchvision \n",
        "import plotly\n",
        "import logging\n",
        "import json\n",
        "import os\n",
        "import timeit\n",
        "import numpy as np\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, Subset, TensorDataset\n",
        "import random\n",
        "import heapq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {},
      "outputs": [],
      "source": [
        "BATCH_SIZE=64\n",
        "\n",
        "# Make it repeatable, set seeds\n",
        "random.seed(0)\n",
        "torch.manual_seed(0)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {},
      "outputs": [],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "# load MNIST dataset\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "train_dataset, validate_dataset = torch.utils.data.random_split(train_dataset, [50000, 10000])\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "validate_loader = DataLoader(dataset=validate_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {
        "id": "VPPueoaSFWmr"
      },
      "outputs": [],
      "source": [
        "def train(model, data_loader, optimizer, criterion, mask = None):\n",
        "    results = []\n",
        "    model.train()\n",
        "    for data in data_loader:\n",
        "        inputs, labels = data\n",
        "        optimizer.zero_grad() # zero the parameter gradients\n",
        "        predictions = model(inputs) # forward pass\n",
        "        loss = criterion(predictions, labels) # calculate loss\n",
        "        loss.backward() # backward pass\n",
        "        optimizer.step() # update parameters\n",
        "\n",
        "    return model\n",
        "\n",
        "def evaluate(model, data_loader, mask = None):\n",
        "      model.eval()\n",
        "      with torch.inference_mode():\n",
        "          correct = 0\n",
        "          total = 0\n",
        "          for inputs, labels in data_loader:\n",
        "              inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
        "              outputs = model(inputs, mask)\n",
        "              _, predicted = torch.max(outputs.data, 1)\n",
        "              total += labels.size(0)\n",
        "              correct += (predicted == labels).sum().item()\n",
        "\n",
        "      return correct / total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {
        "id": "KXDeZNPSzGpP"
      },
      "outputs": [],
      "source": [
        "DEBUG = bool(os.environ.get('DEBUG', 'True') == 'True')\n",
        "log_level = os.environ.get('LOG_LEVEL', 'DEBUG' if DEBUG else 'INFO')\n",
        "logging.basicConfig(level=logging.getLevelName(log_level), format=\"%(message)s\", force=True)\n",
        "\n",
        "# a random subset of MNIST for quicker accuracy determination\n",
        "subset_size = 10000\n",
        "# Create a random subset of the MNIST dataset and move it to the GPU\n",
        "subset_indices = torch.randperm(len(train_dataset))[:subset_size]\n",
        "subset_dataset = Subset(train_dataset, subset_indices)\n",
        "# Create DataLoader for the subset\n",
        "subset_data_loader = DataLoader(subset_dataset, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "id": "eWOFMw5uIALJ",
        "outputId": "0b7ac7d2-2c6e-41a4-f76f-faa26e1d26d0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Masks Shape: torch.Size([26, 3, 100])\n",
            "Accuracies: [0.1049, 0.0991, 0.105, 0.0926, 0.1083, 0.1049, 0.0991, 0.0999, 0.1049, 0.1049, 0.0921, 0.1393, 0.0991, 0.1055, 0.1049, 0.0991, 0.1049, 0.0991, 0.1049, 0.1049, 0.1049, 0.1049, 0.1247, 0.1049, 0.1049, 0.1049]\n",
            "Accuracies: [0.1083, 0.1247, 0.1393, 0.1011, 0.0991, 0.1049, 0.0991, 0.1049, 0.1049, 0.0991, 0.1049, 0.0991, 0.1049, 0.1049, 0.1049, 0.0968, 0.1107, 0.1049, 0.1049, 0.1049, 0.1049, 0.0991, 0.1049, 0.1049, 0.0991, 0.1096]\n",
            "Accuracies: [0.1107, 0.1393, 0.1247, 0.1049, 0.1182, 0.1049, 0.0991, 0.1049, 0.0991, 0.1049, 0.0911, 0.1048, 0.1049, 0.1049, 0.1049, 0.1019, 0.0991, 0.1049, 0.1049, 0.0737, 0.1049, 0.0991, 0.1049, 0.1, 0.0991, 0.1048]\n"
          ]
        }
      ],
      "source": [
        "## HYPERPARAMETERS\n",
        "# number of masks\n",
        "# number of top parents to keep\n",
        "# the crossover operation\n",
        "# the mutation operation\n",
        "# the masking operation\n",
        "# number of epochs\n",
        "# length should be 2x the trained version, and width polynomial in width to get SLT\n",
        "\n",
        "EPOCHS=3\n",
        "LEARNING_RATE=0.001\n",
        "TRAIN=False\n",
        "NUM_MASKS=26\n",
        "assert NUM_MASKS % 2 == 0 # must be able to create groups of 2 parents\n",
        "NUM_ELEMENTS_TO_KEEP = 20\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_size, num_hidden_layers, hidden_size, output_size):\n",
        "        super(MLP, self).__init__()\n",
        "        assert num_hidden_layers > 0\n",
        "        self.input = nn.Linear(input_size, hidden_size)\n",
        "        self.layers = nn.ModuleList()\n",
        "        for i in range(num_hidden_layers):\n",
        "            self.layers.append(nn.Linear(hidden_size, hidden_size))\n",
        "        self.output = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x, mask = None):\n",
        "        \"\"\"\n",
        "        This allows for both a trainable MLP model and a non-training mask based model\n",
        "        \"\"\"\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.input(x)\n",
        "\n",
        "        if (mask != None):\n",
        "            for i, layer in enumerate(self.layers):\n",
        "                x = layer(x) * mask[i]\n",
        "                x = torch.relu(x)\n",
        "        else:\n",
        "            for layer in self.layers:\n",
        "                x = layer(x)\n",
        "                x = torch.relu(x)\n",
        "\n",
        "        x = self.output(x)\n",
        "        return torch.softmax(x, dim=1)\n",
        "\n",
        "\n",
        "def random_masks(num, depth, width):\n",
        "    if num <= 0:\n",
        "        return torch.tensor([])\n",
        "    return torch.rand(num, depth, width, device=DEVICE)\n",
        "\n",
        "def convert_to_binary_mask(float_mask_2d: torch.tensor, num_elements_to_keep: int = NUM_ELEMENTS_TO_KEEP):\n",
        "    _, top_indices_2d = float_mask_2d.topk(num_elements_to_keep, dim=1)\n",
        "    # Create new mask for of 0's in the same shape as the float mask\n",
        "    binary_mask_2d = torch.zeros_like(float_mask_2d)\n",
        "    # Place 1's at the position of the top k elements in each layer\n",
        "    binary_mask_2d.scatter_(1, top_indices_2d, 1)\n",
        "    return binary_mask_2d\n",
        "\n",
        "def basic_crossover(couple):\n",
        "    return couple[0] * couple[1]\n",
        "\n",
        "def basic_mutatation(mask):\n",
        "    # Create a randomly initialized tensor with values between 0.1 and 0\n",
        "    random_tensor = 0.1 * torch.rand_like(mask)\n",
        "    # If the random value is less than 0.5, subtract the random tensor from the mask, otherwise add it\n",
        "    if random.random() < 0.5:\n",
        "        return mask - random_tensor\n",
        "    return mask + random_tensor\n",
        "\n",
        "def eval_accuracy_of_mask(mask_model, mask, ms):\n",
        "    binary_mask = ms(mask) # mask the model\n",
        "    accuracy = evaluate(mask_model, train_loader, binary_mask)\n",
        "    return accuracy\n",
        "\n",
        "def train_masks(mask_model, train_loader, keep_best, depth, width, masks, cx, mt, ms):\n",
        "    for epoch in range(EPOCHS):\n",
        "        accuracies = []\n",
        "        # Evaluate the current generation of masks\n",
        "        for mask in masks:\n",
        "            binary_mask = ms(mask) # mask the model\n",
        "            accuracies.append(evaluate(mask_model, train_loader, binary_mask))\n",
        "        logging.info(f\"Accuracies: {accuracies}\")\n",
        "\n",
        "        # Find the indicies of the best masks that will survive the generation\n",
        "        best_mask_indexes = np.argpartition(accuracies, -keep_best)[-keep_best:]\n",
        "\n",
        "        # Crossover and mutation\n",
        "        couples = masks.chunk(int(NUM_MASKS / 2)) # chunk into groups of 2 parents\n",
        "        children = [cx(couple) for couple in couples] # generate 1 child\n",
        "        #logging.debug(f\"Children: {children}\")\n",
        "        new_masks = [mt(child) for child in children] # mutate the children\n",
        "        #logging.debug(f\"New masks: {new_masks}\")\n",
        "        \n",
        "        if (len(best_mask_indexes) + len(new_masks) > NUM_MASKS):\n",
        "            raise ValueError(\"Too many masks specified to keep\")\n",
        "        \n",
        "        # Keep the 'keep-best' number of original masks, add the new generation of masks, and fill the rest with random masks\n",
        "        filler_masks = random_masks((NUM_MASKS - len(new_masks) - keep_best), depth, width)\n",
        "        masks = torch.cat((masks[best_mask_indexes], torch.stack(new_masks, dim=0), filler_masks), dim=0)\n",
        "\n",
        "def run(SHOW_PLOTS):\n",
        "    num_hidden_layers = 3\n",
        "    hidden_size = 100\n",
        "\n",
        "    # TODO: the total number of neurons in the target network (train_model), should\n",
        "    # be equal to the number of unmasked neurons in the masked model\n",
        "    # e.g. 1*2 = 2 hidden neurons in target network, 3*8*sparsity == 2 in mask network\n",
        "    # with 2l and polynomial width\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    if TRAIN:\n",
        "        logging.info('Training...')\n",
        "        train_accuracies = []\n",
        "        start_time = timeit.default_timer()\n",
        "        train_model = MLP(input_size=784, num_hidden_layers=1, hidden_size=2, output_size=10)\n",
        "        train_model.to(DEVICE)\n",
        "\n",
        "        optimizer = torch.optim.Adam(train_model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "        for epoch in range(EPOCHS):\n",
        "            model = train(train_model, train_loader, optimizer, criterion)\n",
        "            accuracy = evaluate(model, validate_loader)\n",
        "            train_accuracies.append(accuracy)\n",
        "            print(f'Epoch {epoch + 1}/{EPOCHS}, Accuracy: {accuracy}')\n",
        "\n",
        "        end_time = timeit.default_timer()\n",
        "        print(f'Total training time: {end_time - start_time}')\n",
        "\n",
        "        if SHOW_PLOTS:\n",
        "            fig = px.line(x=range(1, EPOCHS + 1), y=train_accuracies)\n",
        "            fig.show()\n",
        "\n",
        "\n",
        "        logging.info('Testing...')\n",
        "        test_accuracy = evaluate(train_model, test_loader)\n",
        "        print(f'Test Accuracy: {test_accuracy}')\n",
        "\n",
        "        # save model in case we want to use it again, and accuracy for a stop condition\n",
        "        torch.save(train_model.state_dict(), './train_model.pt')\n",
        "        with open('accuracy.json', 'w', encoding='utf-8') as f:\n",
        "            json.dump(test_accuracy)\n",
        "\n",
        "    mask_model = MLP(input_size=784, num_hidden_layers=num_hidden_layers, hidden_size=hidden_size, output_size=10)\n",
        "    mask_model.to(DEVICE)\n",
        "\n",
        "    masks = random_masks(NUM_MASKS, num_hidden_layers, hidden_size)\n",
        "    logging.debug(f\"Masks Shape: {masks.shape}\")\n",
        "\n",
        "    # TODO: replace these with whatever functions you want\n",
        "    # takes a couple (tensor of size 2) and generates a child (singular tensor)\n",
        "    cx = lambda a : basic_crossover(a)\n",
        "\n",
        "    # Takes a mask and mutates it\n",
        "    mt = lambda a : basic_mutatation(a)\n",
        "\n",
        "    # Convert mask of floats to a binary mask\n",
        "    ms = lambda a: convert_to_binary_mask(a, NUM_ELEMENTS_TO_KEEP)\n",
        "\n",
        "    train_masks(mask_model, subset_data_loader, 10, num_hidden_layers, hidden_size, masks, cx, mt, ms)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run(SHOW_PLOTS=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
