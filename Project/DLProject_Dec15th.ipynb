{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vRjDB0seyHnj",
        "outputId": "8f3b1198-34d3-4df9-de0a-3367d471f858"
      },
      "outputs": [],
      "source": [
        "import torch \n",
        "import torchvision \n",
        "import plotly\n",
        "import logging\n",
        "import json\n",
        "import os\n",
        "import timeit\n",
        "import numpy as np\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, Subset, TensorDataset\n",
        "import random\n",
        "import concurrent.futures\n",
        "from functools import partial\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [],
      "source": [
        "BATCH_SIZE=64\n",
        "\n",
        "# Make it repeatable, set seeds\n",
        "random.seed(0)\n",
        "torch.manual_seed(0)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "# load MNIST dataset\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "train_dataset, validate_dataset = torch.utils.data.random_split(train_dataset, [50000, 10000])\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "validate_loader = DataLoader(dataset=validate_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# # Create a random subset of the MNIST dataset and pin the memory for faster transfer to GPU\n",
        "subset_size = 500\n",
        "subset_indices = torch.randperm(len(train_dataset))[:subset_size]\n",
        "subset_dataset = Subset(train_dataset, subset_indices)\n",
        "subset_data_loader = DataLoader(subset_dataset, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "VPPueoaSFWmr"
      },
      "outputs": [],
      "source": [
        "def train(model, data_loader, optimizer, criterion, mask = None):\n",
        "    results = []\n",
        "    model.train()\n",
        "    for data in data_loader:\n",
        "        inputs, labels = data\n",
        "        optimizer.zero_grad() # zero the parameter gradients\n",
        "        predictions = model(inputs) # forward pass\n",
        "        loss = criterion(predictions, labels) # calculate loss\n",
        "        loss.backward() # backward pass\n",
        "        optimizer.step() # update parameters\n",
        "\n",
        "    return model\n",
        "\n",
        "def evaluate(model, data_loader, mask = None):\n",
        "      model.eval()\n",
        "      with torch.inference_mode():\n",
        "          correct = 0\n",
        "          total = 0\n",
        "          for inputs, labels in data_loader:\n",
        "              inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
        "              outputs = model(inputs, mask)\n",
        "              _, predicted = torch.max(outputs.data, 1)\n",
        "              total += labels.size(0)\n",
        "              correct += (predicted == labels).sum().item()\n",
        "\n",
        "      return correct / total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "KXDeZNPSzGpP"
      },
      "outputs": [],
      "source": [
        "DEBUG = bool(os.environ.get('DEBUG', 'True') == 'True')\n",
        "log_level = os.environ.get('LOG_LEVEL', 'DEBUG' if DEBUG else 'INFO')\n",
        "logging.basicConfig(level=logging.getLevelName(log_level), format=\"%(message)s\", force=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Crossover methods\n",
        "def basic_crossover(couple):\n",
        "    return couple[0] * couple[1]\n",
        "\n",
        "def calculate_beta(q, eta):\n",
        "    beta = torch.where(q <= 0.5, \n",
        "                       (2 * q) ** (1 / (eta + 1)), \n",
        "                       (1 / (2 * (1 - q))) ** (1 / (eta + 1)))\n",
        "    return beta\n",
        "\n",
        "def sbx_crossover(couple, probability=0.9, eta=1): # Takes like 1 minute per crossover - very slow.\n",
        "    # Init empty offspring\n",
        "    offspring1, offspring2 = {}, {}\n",
        "    offspring1[\"hidden\"], offspring2[\"hidden\"] = [], []\n",
        "    \n",
        "    # Crossover for each layer, add to the offspring\n",
        "    offspring1[\"input\"], offspring2[\"input\"] = sbx_crossover_per_layer(couple[0][\"input\"], couple[1][\"input\"], probability, eta)\n",
        "    for i in range(min(len(couple[0][\"hidden\"]), len(couple[1][\"hidden\"]))):\n",
        "        off1_hidden, off2_hidden = sbx_crossover_per_layer(couple[0][\"hidden\"][i], couple[1][\"hidden\"][i], probability, eta)\n",
        "        offspring1[\"hidden\"].append(off1_hidden)\n",
        "        offspring2[\"hidden\"].append(off2_hidden)\n",
        "    offspring1[\"output\"], offspring2[\"output\"] = sbx_crossover_per_layer(couple[0][\"output\"], couple[1][\"output\"], probability, eta)\n",
        "    return offspring1, offspring2\n",
        "\n",
        "def sbx_crossover_per_layer(offspring1_layer_tensor, offspring2_layer_tensor, probability=0.9, eta=1):\n",
        "\n",
        "    # Generate random numbers and calculate beta for the entire tensor\n",
        "    rand = torch.rand(offspring1_layer_tensor.size(), device=offspring1_layer_tensor.device)\n",
        "    beta = calculate_beta(rand, eta)\n",
        "\n",
        "    # Apply SBX crossover using vectorized operations\n",
        "    beta_1 = 0.5 * ((1 + beta) * offspring1_layer_tensor + (1 - beta) * offspring2_layer_tensor)\n",
        "    beta_2 = 0.5 * ((1 - beta) * offspring1_layer_tensor + (1 + beta) * offspring2_layer_tensor)\n",
        "\n",
        "    return beta_1, beta_2\n",
        "\n",
        "def uninform_crossover(couple, probability=0.5):\n",
        "    offspring1, offspring2 = {}, {}\n",
        "    offspring1[\"hidden\"], offspring2[\"hidden\"] = [], []\n",
        "\n",
        "    offspring1[\"input\"], offspring2[\"input\"] = uniform_crossover_per_layer(couple[0][\"input\"], couple[1][\"input\"], probability)\n",
        "    for i in range(min(len(couple[0][\"hidden\"]), len(couple[1][\"hidden\"]))):\n",
        "        off1_hidden, off2_hidden = uniform_crossover_per_layer(couple[0][\"hidden\"][i], couple[1][\"hidden\"][i], probability)\n",
        "        offspring1[\"hidden\"].append(off1_hidden)\n",
        "        offspring2[\"hidden\"].append(off2_hidden)\n",
        "    offspring1[\"output\"], offspring2[\"output\"] = uniform_crossover_per_layer(couple[0][\"output\"], couple[1][\"output\"], probability)\n",
        "    return offspring1, offspring2\n",
        "    \n",
        "def uniform_crossover_per_layer(offspring1_layer_tensor, offspring2_layer_tensor, probability=0.5):\n",
        "    # Initialize the offspring tensors\n",
        "    # Reshape the tensors for vectorized operations\n",
        "    flat_tensor1 = offspring1_layer_tensor.view(-1)\n",
        "    flat_tensor2 = offspring2_layer_tensor.view(-1)\n",
        "\n",
        "    # Generate a random mask\n",
        "    mask = torch.rand_like(flat_tensor1) < probability\n",
        "\n",
        "    # Create offspring tensors using vectorized operations\n",
        "    offspring1_tensor = torch.where(mask, flat_tensor2, flat_tensor1)\n",
        "    offspring2_tensor = torch.where(mask, flat_tensor1, flat_tensor2)\n",
        "\n",
        "    # Reshape back to original shape\n",
        "    return offspring1_tensor.view_as(offspring1_layer_tensor), offspring2_tensor.view_as(offspring2_layer_tensor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {},
      "outputs": [],
      "source": [
        "FLIP_BIT_PROBABILITY = 0.1\n",
        "RANDOM_RESETTING_PROB = 0.15\n",
        "CREEP_MUTATION_PROB = 0.1\n",
        "GAUSS_MUTATION_PROB = 0.2\n",
        "\n",
        "def random_mutation(mask):\n",
        "    for key in mask:\n",
        "        if isinstance(mask[key], list):\n",
        "            mask[key] = [mutate_layer(layer) for layer in mask[key]]\n",
        "        else:\n",
        "            mask[key] = mutate_layer(mask[key])\n",
        "    return mask\n",
        "\n",
        "def mutate_layer(layer):\n",
        "    mutation_choice = random.choices(\n",
        "        ['flip_bit', 'random_reset', 'creep', 'none', 'guass'],\n",
        "        weights=[FLIP_BIT_PROBABILITY, RANDOM_RESETTING_PROB, CREEP_MUTATION_PROB, 0.45, GAUSS_MUTATION_PROB])[0]\n",
        "\n",
        "    if mutation_choice == 'flip_bit':\n",
        "        return flip_bit_mutation_layer(layer)\n",
        "    elif mutation_choice == 'random_reset':\n",
        "        return random_resetting_mutation_layer(layer)\n",
        "    elif mutation_choice == 'creep':\n",
        "        return creep_mutation_layer(layer)\n",
        "    elif mutation_choice == 'guass':\n",
        "        return gaussian_mutation_layer(layer)\n",
        "    else:  # 'none'\n",
        "        return layer\n",
        "\n",
        "def gaussian_mutation_layer(mask, mean=0.0, stddev=0.1):\n",
        "    # Adds Gaussian noise to the elements of the mask\n",
        "    noise = torch.normal(mean, stddev, size=mask.size(), device=mask.device)\n",
        "    return mask + noise\n",
        "\n",
        "def flip_bit_mutation_layer(mask, mutation_rate=0.01):\n",
        "    # Randomly flip some of the elements in the mask\n",
        "    flip_mask = torch.rand_like(mask) < mutation_rate\n",
        "    return torch.where(flip_mask, 1 - mask, mask)\n",
        "\n",
        "def random_resetting_mutation_layer(mask, mutation_rate=0.01):\n",
        "    # Randomly reset a certain percentage of the elements in the mask to new values.\n",
        "    random_values = torch.rand_like(mask)\n",
        "    mutation_mask = torch.rand_like(mask) < mutation_rate\n",
        "    return torch.where(mutation_mask, random_values, mask)\n",
        "\n",
        "def creep_mutation_layer(mask, creep_rate=0.05, max_creep=0.1):\n",
        "    # Increments or decrements the values of the mask by a small random value.\n",
        "    creep = (2 * torch.rand_like(mask) - 1) * max_creep  # Values between -max_creep and +max_creep\n",
        "    mutation_mask = torch.rand_like(mask) < creep_rate\n",
        "    return torch.where(mutation_mask, mask + creep, mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "id": "eWOFMw5uIALJ",
        "outputId": "0b7ac7d2-2c6e-41a4-f76f-faa26e1d26d0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Initial mask shape, Input: torch.Size([128, 784]), Hidden: torch.Size([128, 128]), Output: torch.Size([10, 128])\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/2000, Best Accuracy: 0.118. All Accuracies: [0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.108, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.118, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.106, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098]. Accuracy eval time: 10.639198541641235, Crossover time: 0.011903762817382812, Mutation Time: 0.012896060943603516, Overall time: 10.663998365402222\n",
            "Epoch 2/2000, Best Accuracy: 0.13. All Accuracies: [0.098, 0.098, 0.098, 0.098, 0.098, 0.118, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.118, 0.106, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.108, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.108, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.11, 0.098, 0.098, 0.098, 0.126, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.104, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.13, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098]. Accuracy eval time: 17.395214080810547, Crossover time: 0.013888120651245117, Mutation Time: 0.011904001235961914, Overall time: 17.421006202697754\n",
            "Epoch 3/2000, Best Accuracy: 0.13. All Accuracies: [0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.11, 0.098, 0.098, 0.098, 0.098, 0.126, 0.098, 0.098, 0.108, 0.098, 0.108, 0.118, 0.098, 0.106, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.13, 0.098, 0.098, 0.104, 0.098, 0.098, 0.098, 0.098, 0.098, 0.118, 0.098, 0.098, 0.118, 0.098, 0.098, 0.098, 0.098, 0.098, 0.106, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.11, 0.098, 0.098, 0.098, 0.098, 0.098, 0.094, 0.098, 0.098, 0.098, 0.098, 0.106, 0.098, 0.098, 0.098, 0.098, 0.09, 0.118, 0.098, 0.096, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.094, 0.098, 0.098, 0.088, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.102, 0.106, 0.098, 0.098, 0.098, 0.076, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098]. Accuracy eval time: 17.948254108428955, Crossover time: 0.012399911880493164, Mutation Time: 0.013391971588134766, Overall time: 17.975037813186646\n",
            "Epoch 4/2000, Best Accuracy: 0.172. All Accuracies: [0.098, 0.118, 0.098, 0.098, 0.098, 0.118, 0.118, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.096, 0.098, 0.13, 0.098, 0.098, 0.098, 0.098, 0.094, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.126, 0.118, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.106, 0.108, 0.098, 0.098, 0.11, 0.098, 0.098, 0.098, 0.126, 0.098, 0.098, 0.118, 0.108, 0.098, 0.098, 0.098, 0.118, 0.098, 0.076, 0.098, 0.098, 0.11, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.124, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.1, 0.106, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.104, 0.11, 0.098, 0.098, 0.098, 0.098, 0.11, 0.172, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098]. Accuracy eval time: 17.691325902938843, Crossover time: 0.011904001235961914, Mutation Time: 0.013888359069824219, Overall time: 17.71811032295227\n",
            "Epoch 5/2000, Best Accuracy: 0.172. All Accuracies: [0.098, 0.126, 0.126, 0.118, 0.098, 0.098, 0.098, 0.13, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.106, 0.13, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.124, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.11, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.118, 0.118, 0.098, 0.098, 0.098, 0.118, 0.098, 0.108, 0.098, 0.098, 0.098, 0.172, 0.098, 0.098, 0.098, 0.126, 0.1, 0.118, 0.098, 0.098, 0.076, 0.098, 0.098, 0.098, 0.098, 0.118, 0.098, 0.098, 0.098, 0.108, 0.098, 0.098, 0.098, 0.118, 0.126, 0.118, 0.098, 0.098, 0.094, 0.098, 0.116, 0.09, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.094, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.116, 0.12, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.088, 0.098, 0.098, 0.098, 0.098, 0.106, 0.072, 0.098, 0.098, 0.086, 0.098, 0.098, 0.098, 0.106, 0.098, 0.098, 0.098, 0.098, 0.072, 0.098, 0.098, 0.098, 0.098, 0.106, 0.132, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098]. Accuracy eval time: 17.62436556816101, Crossover time: 0.011904001235961914, Mutation Time: 0.013391733169555664, Overall time: 17.65164566040039\n",
            "Epoch 6/2000, Best Accuracy: 0.172. All Accuracies: [0.098, 0.098, 0.098, 0.118, 0.132, 0.098, 0.13, 0.098, 0.126, 0.172, 0.132, 0.098, 0.098, 0.098, 0.098, 0.098, 0.12, 0.126, 0.098, 0.098, 0.118, 0.124, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.12, 0.098, 0.118, 0.098, 0.118, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.126, 0.172, 0.098, 0.098, 0.126, 0.098, 0.098, 0.098, 0.098, 0.126, 0.098, 0.098, 0.098, 0.13, 0.098, 0.126, 0.098, 0.098, 0.098, 0.094, 0.126, 0.11, 0.098, 0.098, 0.098, 0.098, 0.088, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.13, 0.098, 0.126, 0.098, 0.098, 0.098, 0.098, 0.118, 0.098, 0.116, 0.098, 0.098, 0.098, 0.106, 0.088, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.116, 0.098, 0.098, 0.14, 0.116, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.104, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.13, 0.098, 0.098, 0.094, 0.098, 0.098, 0.098, 0.098, 0.098, 0.096, 0.132, 0.098, 0.098, 0.098, 0.098, 0.116, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.106, 0.098, 0.098, 0.116, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098]. Accuracy eval time: 17.59807777404785, Crossover time: 0.010911941528320312, Mutation Time: 0.011904001235961914, Overall time: 17.622382164001465\n",
            "Epoch 7/2000, Best Accuracy: 0.172. All Accuracies: [0.098, 0.098, 0.098, 0.14, 0.098, 0.13, 0.116, 0.098, 0.126, 0.098, 0.098, 0.13, 0.126, 0.13, 0.098, 0.098, 0.098, 0.132, 0.13, 0.098, 0.098, 0.098, 0.098, 0.172, 0.126, 0.098, 0.098, 0.118, 0.124, 0.098, 0.098, 0.104, 0.098, 0.118, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.132, 0.098, 0.132, 0.098, 0.172, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.11, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.118, 0.098, 0.098, 0.098, 0.14, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.13, 0.098, 0.132, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.106, 0.098, 0.064, 0.098, 0.098, 0.106, 0.098, 0.106, 0.104, 0.098, 0.098, 0.116, 0.098, 0.098, 0.098, 0.104, 0.106, 0.098, 0.132, 0.098, 0.098, 0.098, 0.168, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.096, 0.136, 0.098, 0.098, 0.098, 0.098, 0.096, 0.082, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.092, 0.126, 0.098, 0.098, 0.098, 0.074, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.104, 0.098, 0.098, 0.098, 0.128, 0.098, 0.096, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098]. Accuracy eval time: 17.495405912399292, Crossover time: 0.011408090591430664, Mutation Time: 0.012399911880493164, Overall time: 17.520205974578857\n",
            "Epoch 8/2000, Best Accuracy: 0.172. All Accuracies: [0.098, 0.126, 0.13, 0.098, 0.172, 0.098, 0.096, 0.098, 0.098, 0.132, 0.098, 0.132, 0.104, 0.098, 0.098, 0.098, 0.172, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.13, 0.136, 0.168, 0.098, 0.172, 0.098, 0.106, 0.098, 0.082, 0.098, 0.132, 0.104, 0.098, 0.098, 0.098, 0.098, 0.098, 0.132, 0.118, 0.136, 0.098, 0.118, 0.098, 0.098, 0.098, 0.126, 0.098, 0.098, 0.098, 0.11, 0.098, 0.132, 0.098, 0.098, 0.104, 0.098, 0.118, 0.14, 0.132, 0.098, 0.098, 0.098, 0.098, 0.124, 0.14, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.108, 0.096, 0.098, 0.098, 0.098, 0.118, 0.11, 0.098, 0.098, 0.098, 0.108, 0.096, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.128, 0.118, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.088, 0.098, 0.116, 0.098, 0.098, 0.098, 0.098, 0.098, 0.086, 0.098, 0.098, 0.09, 0.098, 0.096, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098, 0.098]. Accuracy eval time: 17.938334941864014, Crossover time: 0.010911226272583008, Mutation Time: 0.012399911880493164, Overall time: 17.963134050369263\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[80], line 226\u001b[0m\n\u001b[0;32m    221\u001b[0m     ms \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m a: convert_to_binary_mask(a, ELEMENTS_KEEP_INPUT_HIDDEN, ELEMENTS_KEEP_HIDDEN_HIDDEN, ELEMENTS_KEEP_HIDDEN_OUTPUT)\n\u001b[0;32m    223\u001b[0m     train_masks(mask_model, subset_data_loader, KEEP_BEST, NUM_HIDDEN_LAYERS, HIDDEN_SIZE, masks, cx, mt, ms)\n\u001b[1;32m--> 226\u001b[0m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mSHOW_PLOTS\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[1;32mIn[80], line 223\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(SHOW_PLOTS)\u001b[0m\n\u001b[0;32m    220\u001b[0m \u001b[38;5;66;03m# Convert mask of floats to a binary mask\u001b[39;00m\n\u001b[0;32m    221\u001b[0m ms \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m a: convert_to_binary_mask(a, ELEMENTS_KEEP_INPUT_HIDDEN, ELEMENTS_KEEP_HIDDEN_HIDDEN, ELEMENTS_KEEP_HIDDEN_OUTPUT)\n\u001b[1;32m--> 223\u001b[0m \u001b[43mtrain_masks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubset_data_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mKEEP_BEST\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNUM_HIDDEN_LAYERS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mHIDDEN_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmasks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mms\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[1;32mIn[80], line 126\u001b[0m, in \u001b[0;36mtrain_masks\u001b[1;34m(mask_model, train_loader, keep_best, depth, width, masks, cx, mt, ms)\u001b[0m\n\u001b[0;32m    123\u001b[0m partial_process_mask \u001b[38;5;241m=\u001b[39m partial(eval_accuracy_of_mask, train_loader\u001b[38;5;241m=\u001b[39mtrain_loader, mask_model\u001b[38;5;241m=\u001b[39mmask_model, ms\u001b[38;5;241m=\u001b[39mms)\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m concurrent\u001b[38;5;241m.\u001b[39mfutures\u001b[38;5;241m.\u001b[39mThreadPoolExecutor(max_workers\u001b[38;5;241m=\u001b[39mMAX_THREADS_MASK) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[0;32m    125\u001b[0m     \u001b[38;5;66;03m# Use executor.map to process the (index, mask) tuples concurrently\u001b[39;00m\n\u001b[1;32m--> 126\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(executor\u001b[38;5;241m.\u001b[39mmap(partial_process_mask, indexed_masks))\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index, accuracy \u001b[38;5;129;01min\u001b[39;00m results:\n\u001b[0;32m    129\u001b[0m     accuracies[index] \u001b[38;5;241m=\u001b[39m accuracy \n",
            "File \u001b[1;32mc:\\Users\\Nathan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\concurrent\\futures\\_base.py:619\u001b[0m, in \u001b[0;36mExecutor.map.<locals>.result_iterator\u001b[1;34m()\u001b[0m\n\u001b[0;32m    616\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m fs:\n\u001b[0;32m    617\u001b[0m     \u001b[38;5;66;03m# Careful not to keep a reference to the popped future\u001b[39;00m\n\u001b[0;32m    618\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 619\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[43m_result_or_cancel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    620\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    621\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m _result_or_cancel(fs\u001b[38;5;241m.\u001b[39mpop(), end_time \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic())\n",
            "File \u001b[1;32mc:\\Users\\Nathan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\concurrent\\futures\\_base.py:317\u001b[0m, in \u001b[0;36m_result_or_cancel\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    316\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 317\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfut\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    319\u001b[0m         fut\u001b[38;5;241m.\u001b[39mcancel()\n",
            "File \u001b[1;32mc:\\Users\\Nathan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\concurrent\\futures\\_base.py:451\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m    449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[1;32m--> 451\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_condition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[0;32m    454\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n",
            "File \u001b[1;32mc:\\Users\\Nathan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\threading.py:327\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[0;32m    326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 327\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    328\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    329\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "## HYPERPARAMETERS\n",
        "# number of masks\n",
        "# number of top parents to keep\n",
        "# the crossover operation\n",
        "# the mutation operation\n",
        "# the masking operation\n",
        "# number of epochs\n",
        "# length should be 2x the trained version, and width polynomial in width to get SLT\n",
        "\n",
        "EPOCHS=2000\n",
        "LEARNING_RATE=0.001\n",
        "TRAIN=False\n",
        "NUM_MASKS=100\n",
        "assert NUM_MASKS % 2 == 0 # must be able to create groups of 2 parents\n",
        "KEEP_BEST=10\n",
        "NUM_CROSSOVER = 40\n",
        "assert NUM_CROSSOVER % 2 == 0 # must be able to create couples for crossover. They'll be randomly grouped, it'll create 2 children per couple\n",
        "NUM_HIDDEN_LAYERS = 3\n",
        "HIDDEN_SIZE = 128\n",
        "ELEMENTS_KEEP_INPUT_HIDDEN = 781*HIDDEN_SIZE*0.6 # 80% of the weights are kept\n",
        "ELEMENTS_KEEP_HIDDEN_HIDDEN = HIDDEN_SIZE*HIDDEN_SIZE*0.6 # 80% of the weights are kept\n",
        "ELEMENTS_KEEP_HIDDEN_OUTPUT = HIDDEN_SIZE*10*0.6 # 80% of the weights are kept\n",
        "# Masking appears to be very optimized already\n",
        "MAX_THREADS_MASK = 2\n",
        "MAX_THREADS_CROSS = 4\n",
        "# Mutation is adding noise to the mask, lots of threads\n",
        "MAX_THREADS_MUTATE = 10\n",
        "TRACK_PERF = True\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_size, num_hidden_layers, hidden_size, output_size):\n",
        "        super(MLP, self).__init__()\n",
        "        assert num_hidden_layers > 0\n",
        "        self.input = nn.Linear(input_size, hidden_size)\n",
        "        self.layers = nn.ModuleList()\n",
        "        for i in range(num_hidden_layers):\n",
        "            self.layers.append(nn.Linear(hidden_size, hidden_size))\n",
        "        self.output = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x, mask = None):\n",
        "        # This allows for a forward that applied the masks to a model's parameters but doesn't train the masks\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "\n",
        "        if mask is not None:\n",
        "            x = torch.nn.functional.linear(x, self.input.weight * mask['input'], self.input.bias)\n",
        "\n",
        "            for layer, mask_layer in zip(self.layers, mask['hidden']):\n",
        "                x = torch.nn.functional.linear(x, layer.weight * mask_layer, layer.bias)\n",
        "                x = torch.relu(x)\n",
        "\n",
        "            x = torch.nn.functional.linear(x, self.output.weight * mask['output'], self.output.bias)\n",
        "        else:\n",
        "            x = self.input(x)\n",
        "            for layer in self.layers:\n",
        "                x = layer(x)\n",
        "                x = torch.relu(x)\n",
        "            x = self.output(x)\n",
        "            \n",
        "        return torch.softmax(x, dim=1)\n",
        "\n",
        "def random_masks(num, depth, width):\n",
        "    if num <= 0:\n",
        "        return None\n",
        "    masks = []\n",
        "\n",
        "    for _ in range(num):\n",
        "        mask = {}\n",
        "        mask['input'] = torch.rand(width, 784, device=DEVICE)\n",
        "        mask['hidden'] = [torch.rand(width, width, device=DEVICE) for _ in range(depth)]\n",
        "        mask['output'] = torch.rand(10, width, device=DEVICE)\n",
        "        masks.append(mask)\n",
        "    return masks\n",
        "\n",
        "def convert_to_binary_mask(float_masks: dict[str, torch.tensor], elements_keep_input_hidden: int = ELEMENTS_KEEP_INPUT_HIDDEN, \n",
        "                           elements_keep_hidden_hidden: int = ELEMENTS_KEEP_HIDDEN_HIDDEN, elements_keep_hidden_output: int = ELEMENTS_KEEP_HIDDEN_OUTPUT):\n",
        "    \n",
        "    # Convert the masks for each layer to binary masks\n",
        "    float_masks['input'] = convert_layer(float_masks['input'], elements_keep_input_hidden)\n",
        "    for i in range(len(float_masks['hidden'])):\n",
        "        float_masks['hidden'][i] = convert_layer(float_masks['hidden'][i], elements_keep_hidden_hidden)\n",
        "    float_masks['output'] = convert_layer(float_masks['output'], elements_keep_hidden_output)\n",
        "    return float_masks\n",
        "\n",
        "def convert_layer(float_mask: torch.tensor, elements_keep: int):\n",
        "\n",
        "    flattened_tensor = float_mask.view(-1)\n",
        "    # Find the indices of the top k elements\n",
        "    _, top_indices = torch.topk(flattened_tensor, int(elements_keep))\n",
        "    zero_tensor = torch.zeros_like(flattened_tensor)\n",
        "    zero_tensor[top_indices] = 1\n",
        "    # Reshape back to the original shape\n",
        "    return zero_tensor.reshape(float_mask.shape)\n",
        "    \n",
        "\n",
        "def eval_accuracy_of_mask(index_mask_tuple, train_loader, mask_model, ms):\n",
        "    index, mask = index_mask_tuple\n",
        "    binary_mask = ms(mask) # mask the model\n",
        "    accuracy = evaluate(mask_model, train_loader, binary_mask)\n",
        "    return index, accuracy\n",
        "\n",
        "def parallel_crossover(couple, cx):\n",
        "    off1, off2 = cx(couple)  # generate 2 children\n",
        "    return off1, off2\n",
        "\n",
        "def parallel_mutation(child, mt):\n",
        "    return mt(child)\n",
        "\n",
        "def train_masks(mask_model, train_loader, keep_best, depth, width, masks, cx, mt, ms):\n",
        "\n",
        "    if (keep_best + 2 * NUM_CROSSOVER > NUM_MASKS):\n",
        "        raise ValueError(\"Too many masks specified to keep\")\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        overall_start = time.time()\n",
        "        accuracies = [None] * len(masks)\n",
        "\n",
        "        # Evaluate the current generation of masks in seperate threads\n",
        "        indexed_masks = list(enumerate(masks))\n",
        "        start_accur_eval = time.time()\n",
        "        # Use functools.partial to create a new function with frozen extra arguments, in this case the train_loader, mask_model, and ms\n",
        "        partial_process_mask = partial(eval_accuracy_of_mask, train_loader=train_loader, mask_model=mask_model, ms=ms)\n",
        "        with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_THREADS_MASK) as executor:\n",
        "            # Use executor.map to process the (index, mask) tuples concurrently\n",
        "            results = list(executor.map(partial_process_mask, indexed_masks))\n",
        "        \n",
        "        for index, accuracy in results:\n",
        "            accuracies[index] = accuracy \n",
        "        end__accur_eval = time.time()\n",
        "\n",
        "        # Find the indicies of the best masks that will survive the generation\n",
        "        best_mask_indexes = np.argpartition(accuracies, -keep_best)[-keep_best:]\n",
        "        best_masks = [masks[i] for i in best_mask_indexes]\n",
        "\n",
        "        start_cross_evaluate = time.time()\n",
        "        # Parallelize the crossover\n",
        "        partial_process_crossover = partial(parallel_crossover, cx=cx)\n",
        "\n",
        "        # Add the best masks to the selected masks for crossover. \n",
        "        # Then randomly select other masks to perform crossover on. Make sure to shuffle the list first\n",
        "        selected_masks = best_masks\n",
        "        selected_masks += random.sample(masks, NUM_CROSSOVER*2 - keep_best)\n",
        "        random.shuffle(selected_masks)\n",
        "        couples = [(selected_masks[i], selected_masks[i + 1]) for i in range(0, len(selected_masks) - 1, 2)]\n",
        "\n",
        "        with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_THREADS_CROSS) as executor:\n",
        "            results = executor.map(partial_process_crossover, couples)\n",
        "        # Flatten the list of tuples to a single list of children\n",
        "        children = [child for couple in results for child in couple]\n",
        "        end_cross_evaluate = time.time()\n",
        "\n",
        "        start_mutate = time.time()\n",
        "        # Parallelize the Mutation\n",
        "        partial_process_mutation = partial(parallel_mutation, mt=mt)\n",
        "        with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_THREADS_MUTATE) as executor:\n",
        "            new_masks = list(executor.map(partial_process_mutation, children))\n",
        "        \n",
        "        end_mutate = time.time()\n",
        "        overall_end = time.time()\n",
        "        print(f'Epoch {epoch + 1}/{EPOCHS}, Best Accuracy: {max(accuracies)}. All Accuracies: {accuracies}. Accuracy eval time: {end__accur_eval - start_accur_eval}, Crossover time: {end_cross_evaluate - start_cross_evaluate}, Mutation Time: {end_mutate - start_mutate}, Overall time: {overall_end - overall_start}')\n",
        "\n",
        "        # Keep the 'keep-best' number of original masks, add the new generation of masks, and fill the rest with random masks if needed\n",
        "        filler_masks = random_masks((NUM_MASKS - len(new_masks) - keep_best), depth, width)\n",
        "        masks = best_masks + new_masks + filler_masks\n",
        "        \n",
        "\n",
        "def run(SHOW_PLOTS):\n",
        "\n",
        "    # TODO: the total number of neurons in the target network (train_model), should\n",
        "    # be equal to the number of unmasked neurons in the masked model\n",
        "    # e.g. 1*2 = 2 hidden neurons in target network, 3*8*sparsity == 2 in mask network\n",
        "    # with 2l and polynomial width\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    if TRAIN:\n",
        "        logging.info('Training...')\n",
        "        train_accuracies = []\n",
        "        start_time = timeit.default_timer()\n",
        "        train_model = MLP(input_size=784, num_hidden_layers=1, hidden_size=2, output_size=10)\n",
        "        train_model.to(DEVICE)\n",
        "\n",
        "        optimizer = torch.optim.Adam(train_model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "        for epoch in range(EPOCHS):\n",
        "            model = train(train_model, train_loader, optimizer, criterion)\n",
        "            accuracy = evaluate(model, validate_loader)\n",
        "            train_accuracies.append(accuracy)\n",
        "            print(f'Epoch {epoch + 1}/{EPOCHS}, Accuracy: {accuracy}')\n",
        "\n",
        "        end_time = timeit.default_timer()\n",
        "        print(f'Total training time: {end_time - start_time}')\n",
        "\n",
        "        if SHOW_PLOTS:\n",
        "            fig = px.line(x=range(1, EPOCHS + 1), y=train_accuracies)\n",
        "            fig.show()\n",
        "\n",
        "        logging.info('Testing...')\n",
        "        test_accuracy = evaluate(train_model, test_loader)\n",
        "        print(f'Test Accuracy: {test_accuracy}')\n",
        "\n",
        "        # save model in case we want to use it again, and accuracy for a stop condition\n",
        "        torch.save(train_model.state_dict(), './train_model.pt')\n",
        "        with open('accuracy.json', 'w', encoding='utf-8') as f:\n",
        "            json.dump(test_accuracy)\n",
        "\n",
        "    mask_model = MLP(input_size=784, num_hidden_layers=NUM_HIDDEN_LAYERS, hidden_size=HIDDEN_SIZE, output_size=10)\n",
        "    mask_model.to(DEVICE)\n",
        "\n",
        "    masks = random_masks(NUM_MASKS, NUM_HIDDEN_LAYERS, HIDDEN_SIZE)\n",
        "    logging.info(f\"Initial mask shape, Input: {masks[0]['input'].shape}, Hidden: {masks[0]['hidden'][0].shape}, Output: {masks[0]['output'].shape}\")\n",
        "\n",
        "    # TODO: replace these with whatever functions you want\n",
        "    # Takes a couple of masks and returns 2 offspring\n",
        "    cx = lambda a : uninform_crossover(a)\n",
        "\n",
        "    # Takes a mask and mutates it\n",
        "    mt = lambda a : random_mutation(a)\n",
        "\n",
        "    # Convert mask of floats to a binary mask\n",
        "    ms = lambda a: convert_to_binary_mask(a, ELEMENTS_KEEP_INPUT_HIDDEN, ELEMENTS_KEEP_HIDDEN_HIDDEN, ELEMENTS_KEEP_HIDDEN_OUTPUT)\n",
        "\n",
        "    train_masks(mask_model, subset_data_loader, KEEP_BEST, NUM_HIDDEN_LAYERS, HIDDEN_SIZE, masks, cx, mt, ms)\n",
        "\n",
        "\n",
        "run(SHOW_PLOTS=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
