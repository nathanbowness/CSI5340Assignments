{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3\n",
    "## Download Dependencies - Dataset, Pre-Trained Models\n",
    "* Download a pre-trained word embedding vector from https://nlp.stanford.edu/projects/glove to help with the assignment and classification\n",
    "* Download the IMDB movie review dataset from https://ai.stanford.edu/~amaas/data/sentiment/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "import zipfile\n",
    "import tarfile\n",
    "\n",
    "def download_and_extract_if_not_exist(url, extract_path, zip_file_name, zip_file_contents_file_name):\n",
    "\n",
    "    # If any of the files already exist, skip the download - it's already been done and extracted\n",
    "    path = os.path.join(extract_path, zip_file_contents_file_name)\n",
    "    if os.path.exists(path):\n",
    "        print(\"Files already exist. Skipping download.\")\n",
    "        return\n",
    "\n",
    "    # Check if the zip file already exists, if it does, skip the download to save time\n",
    "    zip_file_path = os.path.join(extract_path, zip_file_name)\n",
    "    if not os.path.exists(zip_file_path):\n",
    "        print(\"Downloading zip file...\")\n",
    "        if not os.path.exists(extract_path):\n",
    "            os.makedirs(extract_path)\n",
    "        # Download the zip file\n",
    "        zip_file_path, _ = urllib.request.urlretrieve(url, zip_file_path)\n",
    "    else:\n",
    "        print(\"Zip file already exists. Skipping download.\")\n",
    "\n",
    "    # Extract the contents of the zip file to the given directory based on the file extension\n",
    "    if zip_file_path.endswith(\".zip\"):\n",
    "        with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(extract_path)\n",
    "    elif zip_file_path.endswith(\".tar.gz\") or zip_file_path.endswith(\".tgz\"):\n",
    "        with tarfile.open(zip_file_path, 'r:gz') as tar_ref:\n",
    "            tar_ref.extractall(extract_path)\n",
    "\n",
    "    # Close and remove the zip file to free up space\n",
    "    os.remove(zip_file_path)\n",
    "    print(f\"Files extracted to: {extract_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Pre-Trained Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already exist. Skipping download.\n"
     ]
    }
   ],
   "source": [
    "# Download the word embeddings into the following folder: \n",
    "folder_to_download = \"./models\"\n",
    "\n",
    "# 6 Billion token model\n",
    "embedding_url = \"https://nlp.stanford.edu/data/glove.6B.zip\"\n",
    "file_name_to_download = \"glove.6B.zip\"\n",
    "\n",
    "# 42 Billion token model\n",
    "# embedding_url = \"https://nlp.stanford.edu/data/wordvecs/glove.42B.300d.zip\"\n",
    "# file_name_to_download = \"glove.42B.300d.zip\"\n",
    "\n",
    "# Download and extract the embedding file, then set a variable with the path to the embedding file\n",
    "download_and_extract_if_not_exist(embedding_url, folder_to_download, file_name_to_download, \"glove.6B.50d.txt\")\n",
    "word_embedding_file_path = os.path.join(folder_to_download, \"glove.6B.50d.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Movie Review Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already exist. Skipping download.\n"
     ]
    }
   ],
   "source": [
    "# Download the dataset into the following folder: \n",
    "folder_to_download = \"./dataset\"\n",
    "\n",
    "# 6 Billion token model\n",
    "dataset_url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
    "file_name_to_download = \"aclImdb_v1.tar.gz\"\n",
    "\n",
    "# Download and extract, then set the path to the dataset\n",
    "download_and_extract_if_not_exist(dataset_url, folder_to_download, file_name_to_download, \"aclImdb/imdb.vocab\")\n",
    "dataset_folder= os.path.join(folder_to_download, \"aclImdb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the Dataset for training\n",
    "From the downloaded files, create a dataset for training using torch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_folder= os.path.join(folder_to_download, \"aclImdb\")\n",
    "def read_imdb_data_from_folder(imdb_reviews_folder):\n",
    "    texts = []\n",
    "    labels = []\n",
    "    review_folders = [\"pos\", \"neg\"] # there are 2 folders in each dataset folder: pos and neg\n",
    "    for connotation_dir in review_folders:\n",
    "        review_dir = os.path.join(imdb_reviews_folder, connotation_dir)\n",
    "        for filename in os.listdir(review_dir):\n",
    "            with open(os.path.join(review_dir, filename), \"r\", encoding=\"utf8\") as f:\n",
    "                texts.append(f.read())\n",
    "            labels.append(0 if connotation_dir == \"neg\" else 1)\n",
    "    return texts, labels\n",
    "\n",
    "def get_negative_positive_from_label(label):\n",
    "    return \"negative\" if label == 0 else \"positive\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the IMDB Movie Review dataset from the downloaded folder\n",
    "test_texts, test_labels = read_imdb_data_from_folder(os.path.join(dataset_folder, \"test\"))\n",
    "train_texts, train_labels = read_imdb_data_from_folder(os.path.join(dataset_folder, \"train\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review #0\n",
      "Label: 0\n",
      "Sentiment: negative\n",
      "Text: Christ, oh Christ... One watches stunned, incredulous, and possibly deranged, as this tawdry exercise in mirthless smut unfolds with all the wit and dexterity of a palsied Galapagos tortoise. Can such things be? Does this movie actually exist, or was I the unwitting guinea pig of some shadowy international drugs company, sipping my coffee unaware that it had been spiked with a dangerous hallucinogen? I've seen a lot of films, and a lot of bad films, but nothing prepared me for this; by the end of it I was a gibbering, snivelling wreck, tearing at the carpet with my teeth like a dog, clawing at the walls, howling till my lungs were sore. I pleaded desperately, frenziedly for mercy (to whom this appeal was made, I don't know), and longed with burning desire for the soothing balm of Ozu Yasujiro. Sweet Weeping Jesus, the memories... sometimes they come back to me. When I'm at my most vulnerable, when I'm least able to handle them. I shudder, I break down in tears, I bite my fingernails till my hands are slathered with blood, but I can't quite banish the awful flashbacks from my mind. I'm haunted. I'm damaged. I'm a shell of a man.<br /><br />The other user comments here suggest that I am not alone in having undergone this terrifying experience, which can only mean one of two things: a) the film does, in fact, exist, or b) I am but one victim among legions of an international conspiracy of truly sinister proportions. What is quite mind-boggling is that some people seem to have enjoyed their ordeal, or at least have not been left traumatised by it. Perhaps they're part of the operation. God damn them, the maniacs! God damn them all to Hell!!!!!!\n",
      "\n",
      "Review #1\n",
      "Label: 0\n",
      "Sentiment: negative\n",
      "Text: This movie was like \"The Disney Channel after Dark.\" Take out the \"aren't we naughty\" language and themes and you are left with dialogue and plot devices that insult the intelligence of anyone who doesn't describe \"Saved by the Bell\" as quality television. The dialogue so laughably cliched and knowingly dirty, one might think the screenplay was the product of locking Aaron Spelling and Joe Eszterhas in a room with orders to produce an amalgam of every bad script each had ever had a hand in creating. If that was Roger Kumble's intention, mission accomplished.\n",
      "\n",
      "Review #2\n",
      "Label: 0\n",
      "Sentiment: negative\n",
      "Text: I was willing to go with the original _Cruel Intentions._ It went along with the plot and stayed true to the characters of one of my favorite stories. However, this movie was, in my opinion, a crummy rehash with essentially the same story line and no clear character choices. I didn't honestly care what happened to any of them. The strongest part of the original story (Les Liasons, not CI) is the characters and their interactions, not the events themselves. I wasn't clear until I read the IMDB that this movie was meant to be a prequel, especially since the title includes a number \"2,\" I expected a sequel, but then determined that it must be a companion piece. Over all, I must say that this movie read, to me at least, like a soft porn version of Les Liasons. I was not impressed.\n",
      "\n",
      "Review #3\n",
      "Label: 0\n",
      "Sentiment: negative\n",
      "Text: Alright normally i am not as harsh on sequels especially if the first film is done well and was ultimately a good movie. As for 1999 i feel that one of the top five films was Cruel Intentions. It had everything a great movie should have except for an original story, being adapted from a novel it was still damn good. On to Cruel Intentions 2 which was supposed to actually just be the opener for a series based on the film called manchester prep. Which must not have happened. Actually after seeing this trifle of a film i can understand. Before the thing started i was like at least the writer and director Roger Kumble did this one also. Well 1 minute into this movie i was disappointed. It starts off with a rehash of the opening of the original with a different twist sebastian instead of putting the shrinks daughter's naked picture on the net he puts the schools principals wife in the school directory naked. This would have been alright if the lady was not like 50. And basically the rest of the movie is a wannabe carbon copy of the original. Which i understand the if there is nothing wrong with it leave it the way it was. But you can not do that with a movie. This actually being a prequel i gave it a chance just to see how they turned out like they did in part 1. But with Sebastian being more or less just a prankster and Kathryn being a herself and turning sebastian into the sexual predator he was in the real story, this movie had no foundation to it. Whoever did the casting on this thing was way off. They could have at least tried to get people who looked like the original cast but no, they just hired a bunch of not even really good looking actors. I am using this term although i dont know why. They for sure didnt do any in this movie.<br /><br />All this movie is a bunch of one liners that dont even match the wit that the original had, well some of them did but that was just because they were from part 1. Another bad point was in part one you could understand the need for them to act out for attention because there was no involvement from teir parents this one had them in it and they were poorly used, as if to show why the kids are like this. It didnt work though. The best thing though about the original was that the cast had chemistry they took you into this world. The on screen tension that was there made the film what it was. This thing Really ruins the experience of the first one stay way from this.\n",
      "\n",
      "Review #4\n",
      "Label: 0\n",
      "Sentiment: negative\n",
      "Text: <br /><br />When I first started watching this movie last night on Cinemax, I was shocked that it had been made. Cruel Intentions, in my opinion, was one of the best teen-oriented films made in years. This prequel had certain things incongruent with the original. (Sebastian's father married into wealth, then why does he have a rich Aunt on long island?)<br /><br />Then I found out today that it was not really intended to be a new movie, but rather a television series, Manchester Prep. After hearing that, it made sense to me that it wasn't the same as the movie, just as Buffy the Vampire Slayer is different in TV form. <br /><br />I think that Roger Kumble most likely added the ending that this movie had, AFTER the series wasn't picked up by Fox. It just seems like something that would happen too fast (Sebastian becoming the male version of Katharyn) and I just don't know where they would go with the next episode, since it wouldn't be leading to the 1999 Film (Which the newer ending is directed right towards.)<br /><br />One thing I didn't like was that it suggested Sebastian and his father had married into the wealth, which isn't typically looked good upon in this area of new york, and sine Katharyn's mother was just an adult version of her, it didn't seem like something a woman in her position would do, marry a man not of her social class. <br /><br />As a prequel this is fairly lame. But I would have been interested to see where this had gone as a series.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(f\"Review #{i}\")\n",
    "    print(f\"Label: {train_labels[i+24000]}\")\n",
    "    print(f\"Sentiment: {get_negative_positive_from_label(train_labels[i+24000])}\")\n",
    "    print(f\"Text: {train_texts[i+24000]}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spacy\n",
    "\n",
    "# # Tokenize the sentences into words\n",
    "# train_sentence_example = train_texts[i+24000]\n",
    "\n",
    "# # Load the English Spacy Tokenizer Model\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "# doc = nlp.tokenizer(train_sentence_example)\n",
    "\n",
    "# tokenized_test_text_docs = [nlp.tokenizer(text) for text in test_texts]\n",
    "# tokenized_train_text_docs = [nlp.tokenizer(text) for text in train_texts]\n",
    "\n",
    "# print_doc = tokenized_test_text_docs[24001]\n",
    "# print(print_doc)\n",
    "# for token in print_doc:\n",
    "#     print(token.text)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "state_dimensions = [20, 50, 100, 200, 500]\n",
    "embedding_dim = 50  # You can choose the dimension of the word embeddings -- Note: this needs to match the model I pick from Glove!!\n",
    "max_len = 200  # You can adjust this based on the average length of reviews\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "num_epochs = 5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize the dataset and pad the sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import torchtext\n",
    "\n",
    "\n",
    "# Tokenize and pad sequences\n",
    "tokenizer = torchtext.data.utils.get_tokenizer('basic_english')\n",
    "train_texts_tokenized = [tokenizer(review) for review in train_texts]\n",
    "test_texts_tokenized = [tokenizer(review) for review in test_texts]\n",
    "\n",
    "# Build vocabulary\n",
    "vocab = torchtext.vocab.build_vocab_from_iterator(train_texts_tokenized + test_texts_tokenized) # Note: This probably shouldn't include the test_texts, but deal with this later\n",
    "\n",
    "train_sequences = [torch.tensor([vocab[token] for token in review], dtype=torch.long) for review in train_texts_tokenized]\n",
    "test_sequences = [torch.tensor([vocab[token] for token in review], dtype=torch.long) for review in test_texts_tokenized] # Error thrown here since the test tokens may not exist in the vocabulary\n",
    "\n",
    "train_padded = nn.utils.rnn.pad_sequence(train_sequences, batch_first=True, padding_value=0)\n",
    "test_padded = nn.utils.rnn.pad_sequence(test_sequences, batch_first=True, padding_value=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start the model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download GloVe word embeddings\n",
    "# You can download from https://nlp.stanford.edu/projects/glove\n",
    "# I'll assume you've downloaded the 100-dimensional GloVe embeddings\n",
    "\n",
    "glove_file = word_embedding_file_path\n",
    "embedding_index = {}\n",
    "with open(glove_file, encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embedding_index[word] = coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embedding matrix\n",
    "embedding_matrix = np.zeros((len(vocab), embedding_dim))\n",
    "for word, i in vocab.get_stoi().items():\n",
    "    embedding_vector = embedding_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training loop\n",
    "results = []\n",
    "# Convert padded to torch.float32\n",
    "train_padded = train_padded.float()\n",
    "test_padded = test_padded.float()\n",
    "\n",
    "\n",
    "for state_dim in state_dimensions:\n",
    "    for model_type in [\"RNN\", \"LSTM\"]:\n",
    "        # Build model\n",
    "        class TextClassifier(nn.Module):\n",
    "            def __init__(self, embedding_dim, state_dim):\n",
    "                super(TextClassifier, self).__init__()\n",
    "                self.embedding = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix), freeze=True)\n",
    "                if model_type == \"RNN\":\n",
    "                    self.rnn = nn.RNN(embedding_dim, state_dim, batch_first=True)\n",
    "                elif model_type == \"LSTM\":\n",
    "                    self.rnn = nn.LSTM(embedding_dim, state_dim, batch_first=True)\n",
    "                self.fc = nn.Linear(state_dim, 1)\n",
    "                self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "            def forward(self, x):\n",
    "                x = x.long()\n",
    "                x = self.embedding(x)\n",
    "                x = x.float()\n",
    "                output, _ = self.rnn(x)\n",
    "                if model_type == \"RNN\":\n",
    "                    output = output[:, -1, :]\n",
    "                elif model_type == \"LSTM\":\n",
    "                    output = output[:, -1, :]\n",
    "                output = self.fc(output)\n",
    "                output = self.sigmoid(output)\n",
    "                return output\n",
    "\n",
    "        model = TextClassifier(embedding_dim, state_dim)\n",
    "\n",
    "        # Loss and optimizer\n",
    "        criterion = nn.BCELoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "        # Convert to DataLoader\n",
    "        train_dataset = TensorDataset(train_padded, torch.tensor(train_labels, dtype=torch.float32))\n",
    "        test_dataset = TensorDataset(test_padded, torch.tensor(test_labels, dtype=torch.float32))\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        # Training loop\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            for inputs, labels in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs.squeeze(), labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        # Evaluation\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                outputs = model(inputs)\n",
    "                predicted = (outputs.squeeze() > 0.5).float()\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        accuracy = correct / total\n",
    "        results.append((model_type, state_dim, accuracy))\n",
    "\n",
    "# Display results\n",
    "print(\"Results:\")\n",
    "print(\"| Model | State Dimension | Accuracy |\")\n",
    "print(\"|-------|------------------|----------|\")\n",
    "for result in results:\n",
    "    print(f\"| {result[0]} | {result[1]} | {result[2]:.4f} |\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
