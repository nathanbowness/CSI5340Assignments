{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "R2BmAK-BoDej"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import urllib.request\n",
        "import zipfile\n",
        "import tarfile\n",
        "\n",
        "maximum_words = 500\n",
        "\n",
        "def download_and_extract_if_not_exist(url, extract_path, zip_file_name, zip_file_contents_file_name):\n",
        "\n",
        "    # If any of the files already exist, skip the download - it's already been done and extracted\n",
        "    path = os.path.join(extract_path, zip_file_contents_file_name)\n",
        "    if os.path.exists(path):\n",
        "        print(\"Files already exist. Skipping download.\")\n",
        "        return\n",
        "\n",
        "    # Check if the zip file already exists, if it does, skip the download to save time\n",
        "    zip_file_path = os.path.join(extract_path, zip_file_name)\n",
        "    if not os.path.exists(zip_file_path):\n",
        "        print(\"Downloading zip file...\")\n",
        "        if not os.path.exists(extract_path):\n",
        "            os.makedirs(extract_path)\n",
        "        # Download the zip file\n",
        "        zip_file_path, _ = urllib.request.urlretrieve(url, zip_file_path)\n",
        "    else:\n",
        "        print(\"Zip file already exists. Skipping download.\")\n",
        "\n",
        "    # Extract the contents of the zip file to the given directory based on the file extension\n",
        "    if zip_file_path.endswith(\".zip\"):\n",
        "        with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "            zip_ref.extractall(extract_path)\n",
        "    elif zip_file_path.endswith(\".tar.gz\") or zip_file_path.endswith(\".tgz\"):\n",
        "        with tarfile.open(zip_file_path, 'r:gz') as tar_ref:\n",
        "            tar_ref.extractall(extract_path)\n",
        "\n",
        "    # Close and remove the zip file to free up space\n",
        "    os.remove(zip_file_path)\n",
        "    print(f\"Files extracted to: {extract_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iJjUcRSHWNkb",
        "outputId": "ba7b1bc4-7c40-48ee-ea7c-f940c6b2f867"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already exist. Skipping download.\n"
          ]
        }
      ],
      "source": [
        "# Download the word embeddings into the following folder:\n",
        "folder_to_download = \"./models\"\n",
        "\n",
        "# 6 Billion token model\n",
        "embedding_url = \"https://nlp.stanford.edu/data/glove.6B.zip\"\n",
        "file_name_to_download = \"glove.6B.zip\"\n",
        "\n",
        "# 42 Billion token model\n",
        "# embedding_url = \"https://nlp.stanford.edu/data/wordvecs/glove.42B.300d.zip\"\n",
        "# file_name_to_download = \"glove.42B.300d.zip\"\n",
        "\n",
        "# Download and extract the embedding file, then set a variable with the path to the embedding file\n",
        "download_and_extract_if_not_exist(embedding_url, folder_to_download, file_name_to_download, \"glove.6B.50d.txt\")\n",
        "word_embedding_file_path = os.path.join(folder_to_download, \"glove.6B.50d.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1lO3QuMLWPox",
        "outputId": "47965ecc-f265-43c4-b1f1-6e88f9452ef2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already exist. Skipping download.\n"
          ]
        }
      ],
      "source": [
        "# Download the dataset into the following folder:\n",
        "folder_to_download = \"./dataset\"\n",
        "\n",
        "# 6 Billion token model\n",
        "dataset_url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
        "file_name_to_download = \"aclImdb_v1.tar.gz\"\n",
        "\n",
        "# Download and extract, then set the path to the dataset\n",
        "download_and_extract_if_not_exist(dataset_url, folder_to_download, file_name_to_download, \"aclImdb/imdb.vocab\")\n",
        "dataset_folder= os.path.join(folder_to_download, \"aclImdb\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "rRJnSMuKWK_H"
      },
      "outputs": [],
      "source": [
        "dataset_folder= os.path.join(folder_to_download, \"aclImdb\")\n",
        "def read_imdb_data_from_folder(imdb_reviews_folder):\n",
        "    texts = []\n",
        "    labels = []\n",
        "    review_folders = [\"pos\", \"neg\"] # there are 2 folders in each dataset folder: pos and neg\n",
        "    for connotation_dir in review_folders:\n",
        "        review_dir = os.path.join(imdb_reviews_folder, connotation_dir)\n",
        "        for filename in os.listdir(review_dir):\n",
        "            with open(os.path.join(review_dir, filename), \"r\", encoding=\"utf8\") as f:\n",
        "                texts.append(f.read())\n",
        "            labels.append(0 if connotation_dir == \"neg\" else 1)\n",
        "    return texts, labels\n",
        "\n",
        "def get_negative_positive_from_label(label):\n",
        "    return \"negative\" if label == 0 else \"positive\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "D6DPi9ScWTFr"
      },
      "outputs": [],
      "source": [
        "# Load the IMDB Movie Review dataset from the downloaded folder\n",
        "test_texts, test_labels = read_imdb_data_from_folder(os.path.join(dataset_folder, \"test\"))\n",
        "train_texts, train_labels = read_imdb_data_from_folder(os.path.join(dataset_folder, \"train\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zi5wSaN7WVNM",
        "outputId": "4bba8559-2556-4c9e-c7f7-3b814b6adf1c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Review #0\n",
            "Label: 0\n",
            "Sentiment: negative\n",
            "Text: This wilfully bizarre adaptation of Borges short story is typical Cox. His strong visual sense is, as usual, undone by the appalling half baked acting of most of the cast. The film is definitely in the surreal tradition of Bunuel's Mexican period, and looks at times like a poor man's take on Lars Von Trier's Elements of Crime. Cox's apparent preference for single takes, jump cuts, and ambient sound recording all work against the film's effectiveness. Worth a look but ultimately disappointing.\n",
            "\n",
            "Review #1\n",
            "Label: 0\n",
            "Sentiment: negative\n",
            "Text: Lets first start this review with the fact that I SIGNED UP JUST TO WRITE THE REVIEW AND WARN PEOPLE TO SAVE Their MONEY!!<br /><br />This was one of the worst pieces of trash i have seen since The Hulk. The storyline was the most predictable garbage you could possibly come up with. If you are expecting 24 but on the big screen, flush that expectation down the toilet immediately along with the money you would use for a ticket.. You may get more enjoyment that way. The acting was terrible, the plot was completely unrealistic, (along with the so called \"twist\" in the end. I must say this.. The ending did surprise me. I am not referring to the plot twist that surprised me, but instead the effortless manner that they put together what could be considered the ONLY scene of somewhat decent action in the entire movie. They rushed the ending so quickly that I didn't even realize that it was over until I saw the credits rolling and at that point i considered burning the reel of film if I could just figure out how to get into that screening room.<br /><br />Casting was awful for a few reasons. First of all, they must have accidentally switched the character assignments, because Michael Douglas played the roll that CLEARLY Keifer Sutherland should have been playing. While Douglas was sneaking around agents, tapping phone lines, hacking into systems and taking out people who are chasing after him, Sutherland plays the less capable agent who is always in a bad mood even when things aren't going that badly for him. He plays a very bland agent, nothing like his Jack Bauer type roll us 24 fans love.<br /><br />I can just about promise you that this movie will disappoint in all areas. It can be best compared to a remake of the \"The Fugitive\" / \"In The Line of Fire\" but written by people with mental disabilities\n",
            "\n",
            "Review #2\n",
            "Label: 0\n",
            "Sentiment: negative\n",
            "Text: I used to be an avid viewer until I personally spent long cold hours helping build a home for the White Family, only to be sickened to see the house a year later. All of the beautiful rock landscaping has been removed, the gorgeous rock sidewalk and front fountain have been removed, all the pine trees and pecan trees in the front have been cut down, sprinkler system has been ripped out. It now looks like a disaster area. They don't even live there any more... they live \"in town\" and come out only for the weekend. It sickens me to think of all the hours that the great people of Oklahoma donated to these people and to see the result. The story that we all saw on TV wasn't completely the truth... don't believe every thing you see and hear.\n",
            "\n",
            "Review #3\n",
            "Label: 0\n",
            "Sentiment: negative\n",
            "Text: I bought this movie for 1 euro, not knowing what it was all about. I thought \"hmmm, a movie named mutilation man must be if not very funny at least filled with gore\". It wasn't funny alright. It was disturbing. Very disturbing. And I don't mind disturbing movies but this one just didn't mean anything, except that child abuse is not a good thing to do. hmmm... The quality of the images were terrible. The acting...there was no acting. Just some fcked-up fcker mutilating himself for over 90 minutes. This is probably material for sicko's jurking off on extreme gore.<br /><br />Don't watch this. It's not worth your time. Its just awful. I wish i never bought this.<br /><br />They should mutilate the guy who made this\n",
            "\n",
            "Review #4\n",
            "Label: 0\n",
            "Sentiment: negative\n",
            "Text: This movie should not be watched as it was meant to be a flop. Ram Gopal Verma first wanted to make this a remake of classic bollywood movie \"Sholay\", but after having problems with the original makers decided to go ahead with the project and... i guess leave all the good parts of the movie (acting, script, songs, music, comedy, action etc) out and shoot the movie just because he already happen to hire the crew. Waste of money, waste of time. After making movies like Rangeela, Satya, and Company he pulled a Coppola (Godfather) on us; What were you thinking RGV? Anyways, the story is, though hard to follow, is almost like the Old sholay. Ajay Devgan playing Heero (Beeru, sholay) and Ajay, new kid on the block playing Ajay (Jay,sholay). Both \"bad yet funny\" friends help a cop capture a bad guy first. Later in the movie, now Retired cop hires them as personal security and safeguarding from the hands of a very most wanted Bubban played by Amitabh Bachan. In case you haven't been watching Bollywood movies, the Good guys win in the end. There I just saved you 3 precious hours of your life!\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for i in range(5):\n",
        "    print(f\"Review #{i}\")\n",
        "    print(f\"Label: {train_labels[i+24000]}\")\n",
        "    print(f\"Sentiment: {get_negative_positive_from_label(train_labels[i+24000])}\")\n",
        "    print(f\"Text: {train_texts[i+24000]}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "6cz36QJEoH-Y"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "\n",
        "# Tokenize the sentences into words\n",
        "train_sentence_example = train_texts[i+24000]\n",
        "\n",
        "# Load the English Spacy Tokenizer Model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp.tokenizer(train_sentence_example)\n",
        "\n",
        "tokenized_test_text_docs = [nlp.tokenizer(text) for text in test_texts]\n",
        "tokenized_train_text_docs = [nlp.tokenizer(text) for text in train_texts]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LQYURkqCZ4DI",
        "outputId": "19c999e4-8e57-4725-f466-553f1ccee775"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "272.46188\n"
          ]
        }
      ],
      "source": [
        "# Calculate the total length and count of nested arrays\n",
        "total_length = 0\n",
        "count = 0\n",
        "\n",
        "for nested_array in tokenized_train_text_docs:\n",
        "    total_length += len(nested_array)\n",
        "    count += 1\n",
        "\n",
        "# Calculate the average length\n",
        "average_length = total_length / count\n",
        "print(average_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2mqLyaWGbQ9K",
        "outputId": "75a84118-fc16-4c51-c8fa-4e16ecf7f96c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hmmm! is it worst film ever? well sort of, for some of the cast its a shame to see them in such a film but hey if it pays the bills why not, as for the film well. OK cg effects not to bad for such a cheap film ,music is just about OK again for a cheap film, end credits are OK lol<br /><br />BAD to many to list but, cast, acting, sets, script, ending..what the hell ,Drac..........worst Drac EVER!, many more but can not be bothered to put them all down.<br /><br />Idea was OK but needed ten times the budget and more thought and much better lighting and style and change all bad points, i do say however to see this film so you to can say\"What the FU%$ was that all about\"as the credits run.Also its kind of a must see just to see how bad it is.\n",
            "\n",
            "Hmmm\n",
            "\n",
            "!\n",
            "\n",
            "is\n",
            "\n",
            "it\n",
            "\n",
            "worst\n",
            "\n",
            "film\n",
            "\n",
            "ever\n",
            "\n",
            "?\n",
            "\n",
            "well\n",
            "\n",
            "sort\n",
            "\n",
            "of\n",
            "\n",
            ",\n",
            "\n",
            "for\n",
            "\n",
            "some\n",
            "\n",
            "of\n",
            "\n",
            "the\n",
            "\n",
            "cast\n",
            "\n",
            "its\n",
            "\n",
            "a\n",
            "\n",
            "shame\n",
            "\n",
            "to\n",
            "\n",
            "see\n",
            "\n",
            "them\n",
            "\n",
            "in\n",
            "\n",
            "such\n",
            "\n",
            "a\n",
            "\n",
            "film\n",
            "\n",
            "but\n",
            "\n",
            "hey\n",
            "\n",
            "if\n",
            "\n",
            "it\n",
            "\n",
            "pays\n",
            "\n",
            "the\n",
            "\n",
            "bills\n",
            "\n",
            "why\n",
            "\n",
            "not\n",
            "\n",
            ",\n",
            "\n",
            "as\n",
            "\n",
            "for\n",
            "\n",
            "the\n",
            "\n",
            "film\n",
            "\n",
            "well\n",
            "\n",
            ".\n",
            "\n",
            "OK\n",
            "\n",
            "cg\n",
            "\n",
            "effects\n",
            "\n",
            "not\n",
            "\n",
            "to\n",
            "\n",
            "bad\n",
            "\n",
            "for\n",
            "\n",
            "such\n",
            "\n",
            "a\n",
            "\n",
            "cheap\n",
            "\n",
            "film\n",
            "\n",
            ",\n",
            "\n",
            "music\n",
            "\n",
            "is\n",
            "\n",
            "just\n",
            "\n",
            "about\n",
            "\n",
            "OK\n",
            "\n",
            "again\n",
            "\n",
            "for\n",
            "\n",
            "a\n",
            "\n",
            "cheap\n",
            "\n",
            "film\n",
            "\n",
            ",\n",
            "\n",
            "end\n",
            "\n",
            "credits\n",
            "\n",
            "are\n",
            "\n",
            "OK\n",
            "\n",
            "lol\n",
            "\n",
            "<\n",
            "\n",
            "br\n",
            "\n",
            "/><br\n",
            "\n",
            "/>BAD\n",
            "\n",
            "to\n",
            "\n",
            "many\n",
            "\n",
            "to\n",
            "\n",
            "list\n",
            "\n",
            "but\n",
            "\n",
            ",\n",
            "\n",
            "cast\n",
            "\n",
            ",\n",
            "\n",
            "acting\n",
            "\n",
            ",\n",
            "\n",
            "sets\n",
            "\n",
            ",\n",
            "\n",
            "script\n",
            "\n",
            ",\n",
            "\n",
            "ending\n",
            "\n",
            "..\n",
            "\n",
            "what\n",
            "\n",
            "the\n",
            "\n",
            "hell\n",
            "\n",
            ",\n",
            "\n",
            "Drac\n",
            "\n",
            "..........\n",
            "\n",
            "worst\n",
            "\n",
            "Drac\n",
            "\n",
            "EVER\n",
            "\n",
            "!\n",
            "\n",
            ",\n",
            "\n",
            "many\n",
            "\n",
            "more\n",
            "\n",
            "but\n",
            "\n",
            "can\n",
            "\n",
            "not\n",
            "\n",
            "be\n",
            "\n",
            "bothered\n",
            "\n",
            "to\n",
            "\n",
            "put\n",
            "\n",
            "them\n",
            "\n",
            "all\n",
            "\n",
            "down.<br\n",
            "\n",
            "/><br\n",
            "\n",
            "/>Idea\n",
            "\n",
            "was\n",
            "\n",
            "OK\n",
            "\n",
            "but\n",
            "\n",
            "needed\n",
            "\n",
            "ten\n",
            "\n",
            "times\n",
            "\n",
            "the\n",
            "\n",
            "budget\n",
            "\n",
            "and\n",
            "\n",
            "more\n",
            "\n",
            "thought\n",
            "\n",
            "and\n",
            "\n",
            "much\n",
            "\n",
            "better\n",
            "\n",
            "lighting\n",
            "\n",
            "and\n",
            "\n",
            "style\n",
            "\n",
            "and\n",
            "\n",
            "change\n",
            "\n",
            "all\n",
            "\n",
            "bad\n",
            "\n",
            "points\n",
            "\n",
            ",\n",
            "\n",
            "i\n",
            "\n",
            "do\n",
            "\n",
            "say\n",
            "\n",
            "however\n",
            "\n",
            "to\n",
            "\n",
            "see\n",
            "\n",
            "this\n",
            "\n",
            "film\n",
            "\n",
            "so\n",
            "\n",
            "you\n",
            "\n",
            "to\n",
            "\n",
            "can\n",
            "\n",
            "say\"What\n",
            "\n",
            "the\n",
            "\n",
            "FU%$\n",
            "\n",
            "was\n",
            "\n",
            "that\n",
            "\n",
            "all\n",
            "\n",
            "about\"as\n",
            "\n",
            "the\n",
            "\n",
            "credits\n",
            "\n",
            "run\n",
            "\n",
            ".\n",
            "\n",
            "Also\n",
            "\n",
            "its\n",
            "\n",
            "kind\n",
            "\n",
            "of\n",
            "\n",
            "a\n",
            "\n",
            "must\n",
            "\n",
            "see\n",
            "\n",
            "just\n",
            "\n",
            "to\n",
            "\n",
            "see\n",
            "\n",
            "how\n",
            "\n",
            "bad\n",
            "\n",
            "it\n",
            "\n",
            "is\n",
            "\n",
            ".\n"
          ]
        }
      ],
      "source": [
        "print_doc = tokenized_test_text_docs[24001]\n",
        "print(print_doc)\n",
        "for token in print_doc:\n",
        "    print(\"\")\n",
        "    print(token)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "wGhjt9ymcm_f"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "state_dimensions = [100] #[20, 50, 100, 200, 500]\n",
        "embedding_dim = 50  # You can choose the dimension of the word embeddings -- Note: this needs to match the model I pick from Glove!!\n",
        "max_len = 100  # You can adjust this based on the average length of reviews\n",
        "\n",
        "### ** state dimension, and the max length of the reviews need to be the same you dummy....."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vkEiaQhNUmf6",
        "outputId": "e6447925-fded-4fb7-a9ca-a3eb15ff04a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "hwRxmLarQuhr"
      },
      "outputs": [],
      "source": [
        "# If the texts are longer than 500 words, shorten them, so they have a max of 500 words.\n",
        "def truncate_nested_arrays(array, max_length):\n",
        "    new_array = [None] * len(array)\n",
        "    for i in range(len(array)):\n",
        "        new_array[i] = array[i][:max_length]\n",
        "    return new_array\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "O4YNgFvCcMyI"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import numpy as np\n",
        "import torchtext\n",
        "\n",
        "# Tokenize and pad sequences\n",
        "tokenizer = torchtext.data.utils.get_tokenizer('basic_english')\n",
        "train_texts_tokenized = [tokenizer(review) for review in train_texts]\n",
        "test_texts_tokenized = [tokenizer(review) for review in test_texts]\n",
        "\n",
        "# test_index_to_print = 2\n",
        "# print(train_texts_tokenized[test_index_to_print])\n",
        "# print(len(train_texts_tokenized[test_index_to_print]))\n",
        "\n",
        "train_texts_tokenized = truncate_nested_arrays(train_texts_tokenized, max_len)\n",
        "test_texts_tokenized = truncate_nested_arrays(test_texts_tokenized, max_len)\n",
        "\n",
        "# print(train_texts_tokenized[test_index_to_print])\n",
        "# print(len(train_texts_tokenized[test_index_to_print]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "wzW3HQYEV68P"
      },
      "outputs": [],
      "source": [
        "# Build vocabulary\n",
        "# import os\n",
        "# os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
        "\n",
        "vocab = torchtext.vocab.build_vocab_from_iterator(train_texts_tokenized + test_texts_tokenized) # Note: This probably shouldn't include the test_texts, but deal with this later\n",
        "\n",
        "train_sequences = [torch.tensor([vocab[token] for token in review], dtype=torch.long, device=device)for review in train_texts_tokenized]\n",
        "test_sequences = [torch.tensor([vocab[token]for token in review], dtype=torch.long, device=device) for review in test_texts_tokenized] # Error thrown here since the test tokens may not exist in the vocabulary\n",
        "\n",
        "train_padded = nn.utils.rnn.pad_sequence(train_sequences, batch_first=True, padding_value=0)\n",
        "test_padded = nn.utils.rnn.pad_sequence(test_sequences, batch_first=True, padding_value=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Im_5zhWlCet2",
        "outputId": "2312dd2c-ffdf-4525-c631-a0d6be70507f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "8\n",
            "212\n",
            "['i', 'found', 'this', 'film', 'to', 'be', 'a', 'fascinating', 'study', 'of', 'a', 'family', 'in', 'crisis', '.', 'when', 'leo', ',', 'the', 'oldest', 'announces', 'that', 'he', 'is', 'hiv+', 'the', 'reactions', 'of', 'the', 'family', 'members', 'alone', 'and', 'with', 'each', 'other', 'was', 'touching', 'and', 'yet', 'strange', '.', 'i', 'have', 'never', 'seen', 'a', 'family', 'that', 'was', 'as', 'physically', 'demonstrative', 'as', 'this', 'one', 'nor', 'one', 'as', 'likely', 'to', 'shout', 'at', 'each', 'other', '.', 'i', 'didn', \"'\", 't', 'understand', 'why', 'the', 'family', 'felt', 'that', 'youngest', 'couldn', \"'\", 't', 'deal', 'with', 'the', 'news', 'but', 'once', 'past', 'that', 'difficult', 'i', 'found', 'this', 'a', 'thoroughly', 'moving', 'film', '.']\n",
            "100\n",
            "100\n"
          ]
        }
      ],
      "source": [
        "print(vocab[\"i\"])\n",
        "print(vocab[\"found\"])\n",
        "\n",
        "print(train_texts_tokenized[0])\n",
        "\n",
        "print(len(train_padded[4]))\n",
        "\n",
        "print(len(test_padded[5]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "5KQicpEXcvyq"
      },
      "outputs": [],
      "source": [
        "# Download GloVe word embeddings\n",
        "# You can download from https://nlp.stanford.edu/projects/glove\n",
        "# I'll assume you've downloaded the 100-dimensional GloVe embeddings\n",
        "\n",
        "glove_file = word_embedding_file_path\n",
        "embedding_index = {}\n",
        "with open(glove_file, encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embedding_index[word] = coefs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I-8KC9UNDqRs",
        "outputId": "3e310009-8b3f-4ff7-8d29-eeae3ea18beb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ 0.0034133 -0.14757    1.2649    -0.21882    0.31136    0.094664\n",
            "  0.21661   -0.56447    0.22246    0.090778  -0.93579   -0.76006\n",
            " -0.066726  -0.16403   -0.016907  -1.2052    -0.31426    0.25058\n",
            " -0.78974    0.33977   -0.086043   0.10269    0.71572   -0.26135\n",
            " -0.37595    0.23748    0.62505   -0.43565   -0.39561   -0.23105\n",
            "  1.3102     0.22167    0.64756    0.86903    0.85008   -0.32513\n",
            "  0.84591   -0.12774    0.11723    0.41916   -0.54855   -0.45053\n",
            "  1.1328     0.30328   -1.0642     0.97721   -0.75989    0.18138\n",
            "  0.22958   -0.31125  ]\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "print(embedding_index.get(\"123\"))\n",
        "print(embedding_index.get(\"volptuous\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "cYMoQfjycz9R"
      },
      "outputs": [],
      "source": [
        "# Create embedding matrix\n",
        "embedding_matrix = np.zeros((len(vocab), embedding_dim))\n",
        "for word, i in vocab.get_stoi().items():\n",
        "    # print(word, i)\n",
        "    embedding_vector = embedding_index.get(word)\n",
        "    # print(embedding_vector)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "\n",
        "embedding_matrix_tensor = torch.tensor(embedding_matrix, device=device)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "USXa8-8EmgbR"
      },
      "outputs": [],
      "source": [
        "# Build model\n",
        "class LSTMTextClassifier(nn.Module):\n",
        "    def __init__(self, embedding_dim, state_dim):\n",
        "        super(LSTMTextClassifier, self).__init__()\n",
        "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix_tensor, freeze=True)\n",
        "        self.rnn = nn.LSTM(input_size=embedding_dim, hidden_size=state_dim, batch_first=True, device=device)\n",
        "        self.fc = nn.Linear(state_dim, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.long()\n",
        "        x = self.embedding(x)\n",
        "        x = x.float()\n",
        "        output, _ = self.rnn(x)\n",
        "        output = output[:, -1, :]\n",
        "        output = self.fc(output)\n",
        "        output = self.sigmoid(output)\n",
        "        return output\n",
        "\n",
        "\n",
        "print_things = False\n",
        "\n",
        "# Build model\n",
        "class RNNTextClassifier(nn.Module):\n",
        "    def __init__(self, embedding_dim, state_dim):\n",
        "        super(RNNTextClassifier, self).__init__()\n",
        "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix_tensor, freeze=True)\n",
        "        self.rnn = nn.RNN(embedding_dim, state_dim, batch_first=True, device=device)\n",
        "        self.fc = nn.Linear(state_dim, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Input tensor containing a sequence of tokens\n",
        "        Return: The output of the current RNN, a list out tensors (a list of binary classifications)\n",
        "        \"\"\"\n",
        "        x = x.long()\n",
        "        if print_things: print(f\"Passed in: {x}\")\n",
        "        vector_embedding = self.embedding(x).float()\n",
        "        if print_things: print(f\"Embedding lookup float: {vector_embedding}\")\n",
        "        rnn_output, _ = self.rnn(vector_embedding)\n",
        "\n",
        "        if print_things: print(f\"Embedding lookup float: {rnn_output}\")\n",
        "        # Option 1: Pooling (e.g., max pooling)\n",
        "        max_pooling_output, _ = torch.max(rnn_output, 1)\n",
        "        output = self.fc(max_pooling_output)\n",
        "        output = self.sigmoid(output)\n",
        "        if print_things: print(f\"Model output: {output}\")\n",
        "        return output\n",
        "\n",
        "def model_training_loop(model, criterion, optimizer, training_data_loader):\n",
        "\n",
        "    training_loop_print = False\n",
        "\n",
        "    loss = None\n",
        "    # Training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        for batch in training_data_loader:\n",
        "            inputs, labels = batch[0], batch[1]\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            # if training_loop_print: print(f\"Training Inputs: {inputs}\")\n",
        "            if training_loop_print: print(f\"Training Label: {labels}\")\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            if training_loop_print: print(f\"Training Output: {outputs}\")\n",
        "\n",
        "\n",
        "            labels = labels.unsqueeze(1)\n",
        "            loss = criterion(outputs, labels)\n",
        "            if training_loop_print: print(f\"Training Loss: {loss}\")\n",
        "            if training_loop_print: print(f\"----\")\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        print(loss)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A3ngI_U3c29N",
        "outputId": "d2782042-1a8c-47f3-969f-53c076271246"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(0.5056, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "tensor(0.4136, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "tensor(0.4308, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "tensor(0.4395, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "tensor(0.2927, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "Results:\n",
            "| Model | State Dimension | Accuracy |\n",
            "|-------|------------------|----------|\n",
            "| RNN | 100 | 77.7520% |\n"
          ]
        }
      ],
      "source": [
        "batch_size_training = 64\n",
        "batch_size_testing = 64\n",
        "learning_rate = 0.001\n",
        "num_epochs = 5\n",
        "\n",
        "# Model training loop\n",
        "results = []\n",
        "# Convert padded to torch.float32\n",
        "train_padded = train_padded.float()\n",
        "test_padded = test_padded.float()\n",
        "\n",
        "train_label_tensor = torch.tensor(train_labels, dtype=torch.float32, device=device)\n",
        "test_label_tensor = torch.tensor(test_labels, dtype=torch.float32, device=device)\n",
        "\n",
        "# Convert to tensor dataset\n",
        "train_dataset = TensorDataset(train_padded, train_label_tensor)\n",
        "test_dataset = TensorDataset(test_padded, test_label_tensor)\n",
        "\n",
        "# Converty the tensor datasets into a dataloader\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size_training, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size_testing, shuffle=True)\n",
        "\n",
        "\n",
        "for state_dim in state_dimensions:\n",
        "    for model_type in [\"RNN\"]: #, \"LSTM\"]:\n",
        "\n",
        "        model = None\n",
        "        if model_type == \"RNN\":\n",
        "          model = RNNTextClassifier(embedding_dim, state_dim).to(device)\n",
        "        elif model_type == \"LSTM\":\n",
        "          model = LSTMTextClassifier(embedding_dim, state_dim).to(device)\n",
        "\n",
        "        # Loss and optimizer\n",
        "        # criterion = nn.BCELoss() # Cross entropy preffered for classification tasks\n",
        "        criterion = nn.BCELoss() # Different criterion commonly used for classification tasks\n",
        "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "        model_training_loop(model, criterion, optimizer, train_loader)\n",
        "\n",
        "        # model_using_iterator(model, criterion, optimizer, train_loader)\n",
        "\n",
        "        # Evaluation\n",
        "        model.eval()\n",
        "\n",
        "        print_test_info = False\n",
        "\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in test_loader:\n",
        "                # if print_test_info: print(f\"Input: {inputs}\")\n",
        "                if print_test_info: print(f\"Input Labels: {labels}\")\n",
        "                outputs = model(inputs)\n",
        "                if print_test_info: print(f\"Model Output: {outputs}\")\n",
        "                predicted = (outputs.squeeze() > 0.5).float()\n",
        "                if print_test_info: print(f\"Predicted: {predicted}\")\n",
        "                if print_test_info: print(\"---\")\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "        accuracy = correct / total * 100\n",
        "        results.append((model_type, state_dim, accuracy))\n",
        "\n",
        "# Display results\n",
        "print(\"Results:\")\n",
        "print(\"| Model | State Dimension | Accuracy |\")\n",
        "print(\"|-------|------------------|----------|\")\n",
        "for result in results:\n",
        "    print(f\"| {result[0]} | {result[1]} | {result[2]:.4f}% |\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "1AF7e-zcoZP1"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
